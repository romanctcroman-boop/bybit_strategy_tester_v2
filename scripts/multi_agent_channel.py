#!/usr/bin/env python3
"""
Multi-Agent Communication Channel: DeepSeek ‚Üî Perplexity
–ë—ã—Å—Ç—Ä—ã–π –∫–∞–Ω–∞–ª —Å–≤—è–∑–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã AI –∞–≥–µ–Ω—Ç–æ–≤
"""

import sys
import json
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

from dotenv import load_dotenv

# Add parent directory to path for imports
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(PROJECT_ROOT))

# Load environment variables so KeyManager can read MASTER_ENCRYPTION_KEY, etc.
load_dotenv(PROJECT_ROOT / ".env")

# API Keys —á–µ—Ä–µ–∑ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π KeyManager (–∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ)
from backend.security.key_manager import get_decrypted_key


def load_tz_context(
    directory: Path,
    keywords: Optional[List[str]] = None,
    max_files: int = 6,
    max_chars_per_file: int = 3500,
    max_total_chars: int = 15000,
) -> Tuple[str, List[Path]]:
    """Load relevant TZ documents and return aggregated context + file list."""

    if keywords is None:
        keywords = [
            "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ",
            "–∑–∞–¥–∞–Ω–∏–µ",
            "tz",
            "spec",
            "orchestrator",
            "—Ç–∑",
        ]

    if not directory.exists():
        raise FileNotFoundError(f"üìÅ –ö–∞—Ç–∞–ª–æ–≥ —Å –¢–ó –Ω–µ –Ω–∞–π–¥–µ–Ω: {directory}")

    candidates: List[Path] = []

    for pattern in ("*.md", "*.txt"):
        for path in sorted(directory.glob(pattern)):
            lower_name = path.name.lower()
            if any(keyword in lower_name for keyword in keywords):
                candidates.append(path)

    # Fallback: –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–µ md-—Ñ–∞–π–ª—ã, –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à—ë–ª
    if not candidates:
        candidates = sorted(directory.glob("*.md"))[:max_files]

    if not candidates:
        raise ValueError("–í –∫–∞—Ç–∞–ª–æ–≥–µ –Ω–µ—Ç Markdown —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¢–ó")

    aggregated_parts: List[str] = []
    used_files: List[Path] = []
    total_chars = 0

    for path in candidates:
        if len(used_files) >= max_files:
            break

        try:
            content = path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            content = path.read_text(encoding="utf-8", errors="ignore")

        snippet = content.strip()
        if not snippet:
            continue

        snippet = snippet[:max_chars_per_file]
        aggregated_parts.append(f"## {path.name}\n\n{snippet}")
        used_files.append(path)
        total_chars += len(snippet)

        if total_chars >= max_total_chars:
            break

    if not aggregated_parts:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¢–ó –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞")

    aggregated_text = "\n\n---\n\n".join(aggregated_parts)
    if len(aggregated_text) > max_total_chars:
        aggregated_text = aggregated_text[:max_total_chars]

    return aggregated_text, used_files


def load_latest_corrected_spec(tz_corrected_dir: Path, max_chars: int = 18000) -> Tuple[Optional[str], Optional[Path]]:
    """Load the most recent corrected TZ file if available."""

    if not tz_corrected_dir.exists():
        return None, None

    candidates = sorted(tz_corrected_dir.glob("corrected_tz_*.md"))
    if not candidates:
        return None, None

    latest_path = candidates[-1]
    content = latest_path.read_text(encoding="utf-8", errors="ignore")
    return content[:max_chars], latest_path


def summarize_results(results: List[Dict], max_chars: int = 12000) -> str:
    """Create a compact summary of collaboration results for downstream prompts."""

    chunks: List[str] = []
    total = 0

    for result in results:
        if not result.get("success"):
            continue

        agent = result.get("agent", "Unknown")
        timestamp = result.get("timestamp", "")
        content = result.get("content", "")
        entry = f"[{agent} @ {timestamp}]\n{content.strip()}"
        truncated = entry[: max_chars - total]
        if not truncated:
            break
        chunks.append(truncated)
        total += len(truncated)
        if total >= max_chars:
            break

    return "\n\n".join(chunks)

PERPLEXITY_API_KEY = get_decrypted_key("PERPLEXITY_API_KEY")
DEEPSEEK_API_KEY = get_decrypted_key("DEEPSEEK_API_KEY")

class MultiAgentChannel:
    """–ö–∞–Ω–∞–ª —Å–≤—è–∑–∏ –º–µ–∂–¥—É DeepSeek –∏ Perplexity"""
    
    def __init__(self):
        self.conversation_history = []
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def deepseek_call(self, prompt: str, context: Optional[str] = None) -> Dict:
        """–í—ã–∑–æ–≤ DeepSeek —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        messages = [
            {
                "role": "system",
                "content": "–¢—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ –∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –†–∞–±–æ—Ç–∞–µ—à—å –≤ –∫–æ–º–∞–Ω–¥–µ —Å Perplexity AI."
            }
        ]
        
        if context:
            prompt = (
                f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n---\n\n"
                f"–ó–ê–î–ê–ù–ò–ï:\n{prompt}"
            )
        
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        payload = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.2,
            "max_tokens": 4000
        }
        
        try:
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers={
                    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=60
            )
        except requests.RequestException as exc:
            return {
                "success": False,
                "error": f"DeepSeek request failed: {exc}",
                "agent": "DeepSeek"
            }
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            return {
                "success": True,
                "content": content,
                "agent": "DeepSeek",
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "success": False,
                "error": f"DeepSeek error {response.status_code}: {response.text}",
                "agent": "DeepSeek"
            }
    
    def perplexity_call(self, prompt: str, context: Optional[str] = None) -> Dict:
        """–í—ã–∑–æ–≤ Perplexity —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        messages = [
            {
                "role": "system",
                "content": "–¢—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑—É –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏. –†–∞–±–æ—Ç–∞–µ—à—å –≤ –∫–æ–º–∞–Ω–¥–µ —Å DeepSeek."
            }
        ]
        
        if context:
            prompt = (
                f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n---\n\n"
                f"–ó–ê–î–ê–ù–ò–ï:\n{prompt}"
            )
        
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        payload = {
            "model": "sonar-pro",
            "messages": messages,
            "temperature": 0.2,
            "max_tokens": 4000
        }
        
        try:
            response = requests.post(
                "https://api.perplexity.ai/chat/completions",
                headers={
                    "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=60
            )
        except requests.RequestException as exc:
            return {
                "success": False,
                "error": f"Perplexity request failed: {exc}",
                "agent": "Perplexity"
            }
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            citations = result.get('citations', [])
            return {
                "success": True,
                "content": content,
                "citations": citations,
                "agent": "Perplexity",
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "success": False,
                "error": f"Perplexity error {response.status_code}: {response.text}",
                "agent": "Perplexity"
            }
    
    def collaborative_analysis(
        self,
        topic: str,
        deepseek_task: str,
        perplexity_task: str,
        iterations: int = 2,
        shared_context: Optional[str] = None,
    ) -> List[Dict]:
        """
        –°–æ–≤–º–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –æ–±–º–µ–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Args:
            topic: –¢–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞
            deepseek_task: –ó–∞–¥–∞—á–∞ –¥–ª—è DeepSeek
            perplexity_task: –ó–∞–¥–∞—á–∞ –¥–ª—è Perplexity
            iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±–º–µ–Ω–∞
        """
        results = []
        
        print("=" * 80)
        print(f"COLLABORATIVE ANALYSIS: {topic}")
        print("=" * 80)
        print()
        
        # –ò—Ç–µ—Ä–∞—Ü–∏—è 1: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        print("üîÑ ITERATION 1: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        print()
        
        print("üì§ DeepSeek: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        deepseek_result = self.deepseek_call(deepseek_task, context=shared_context)
        results.append(deepseek_result)
        
        if deepseek_result["success"]:
            print(f"‚úÖ DeepSeek –≥–æ—Ç–æ–≤ ({len(deepseek_result['content'])} chars)")
        else:
            print(f"‚ùå DeepSeek failed: {deepseek_result.get('error')}")
            return results
        
        print()
        print("üì§ Perplexity: –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        perplexity_result = self.perplexity_call(perplexity_task, context=shared_context)
        results.append(perplexity_result)
        
        if perplexity_result["success"]:
            print(f"‚úÖ Perplexity –≥–æ—Ç–æ–≤ ({len(perplexity_result['content'])} chars)")
            print(f"üìö Citations: {len(perplexity_result.get('citations', []))}")
        else:
            print(f"‚ùå Perplexity failed: {perplexity_result.get('error')}")
            return results
        
        # –ò—Ç–µ—Ä–∞—Ü–∏—è 2+: –û–±–º–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        for i in range(2, iterations + 1):
            print()
            print(f"üîÑ ITERATION {i}: –û–±–º–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è")
            print()
            
            # DeepSeek –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—ã–≤–æ–¥—ã Perplexity
            deepseek_followup = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Perplexity –∏ –¥–∞–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è:

–ó–ê–î–ê–ß–ê: {deepseek_task}

–ß—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å/–∏–∑–º–µ–Ω–∏—Ç—å –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º –ø–ª–∞–Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏?"""
            
            print("üì§ DeepSeek: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ Perplexity...")
            deepseek_result = self.deepseek_call(
                deepseek_followup,
                context=perplexity_result["content"][:2000]  # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤
            )
            results.append(deepseek_result)
            
            if deepseek_result["success"]:
                print(f"‚úÖ DeepSeek –≥–æ—Ç–æ–≤ ({len(deepseek_result['content'])} chars)")
            else:
                print(f"‚ùå DeepSeek failed: {deepseek_result.get('error')}")
                break
            
            print()
            
            # Perplexity –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ DeepSeek
            perplexity_followup = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ DeepSeek –¥–∞–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:

–ó–ê–î–ê–ß–ê: {perplexity_task}

–ö–∞–∫ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é? –ö–∞–∫–∏–µ —Ä–∏—Å–∫–∏?"""
            
            print("üì§ Perplexity: –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
            perplexity_result = self.perplexity_call(
                perplexity_followup,
                context=deepseek_result["content"][:2000]
            )
            results.append(perplexity_result)
            
            if perplexity_result["success"]:
                print(f"‚úÖ Perplexity –≥–æ—Ç–æ–≤ ({len(perplexity_result['content'])} chars)")
            else:
                print(f"‚ùå Perplexity failed: {perplexity_result.get('error')}")
                break
        
        return results
    
    def save_session(self, results: List[Dict], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã"""
        report = {
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat(),
            "results": results
        }
        
        output_path = Path(filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞—ë–º —Ç–∞–∫–∂–µ markdown –≤–µ—Ä—Å–∏—é
        md_path = output_path.with_suffix('.md')
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# Multi-Agent Collaboration Session\n\n")
            f.write(f"**Session ID:** {self.session_id}\n")
            f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            for i, result in enumerate(results, 1):
                if not result.get("success"):
                    continue
                    
                agent = result.get("agent", "Unknown")
                content = result.get("content", "")
                timestamp = result.get("timestamp", "")
                
                f.write(f"## {i}. {agent} ({timestamp})\n\n")
                f.write(content)
                f.write("\n\n")
                
                if "citations" in result and result["citations"]:
                    f.write("### Citations\n\n")
                    for j, citation in enumerate(result["citations"], 1):
                        f.write(f"{j}. {citation}\n")
                    f.write("\n")
                
                f.write("---\n\n")
        
        return md_path

    def generate_corrected_spec(
        self,
        tz_context: str,
        collaboration_summary: str,
        focus_notes: Optional[str] = None,
    ) -> Dict:
        """–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ –¢–ó –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

        guidance = focus_notes or (
            "–°–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–π—Å—è –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –ø—Ä–æ–µ–∫—Ç–µ, "
            "—É—á—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è Knowledge Base, Sandbox, MCP –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞, –∏ AI-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π."
        )

        prompt = f"""–¢—ã –≥–ª–∞–≤–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å. –ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–µ—Ä—Å–∏–π –¢–ó –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ 
—Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ DeepSeek ‚Üî Perplexity –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –µ–¥–∏–Ω–æ–µ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- —É—á–∏—Ç—ã–≤–∞–π, —á—Ç–æ –ø—Ä–æ–µ–∫—Ç —É–∂–µ –ø—Ä–æ–¥–≤–∏–Ω—É–ª—Å—è –¥–∞–ª—å—à–µ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã—Ö –∏–¥–µ–π;
- —è–≤–Ω–æ —Ñ–∏–∫—Å–∏—Ä—É–π —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è (multi-agent –æ—Ä–∫–µ—Å—Ç—Ä, sandbox, knowledge base, MCP);
- —Å–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É: 1) –û–±–∑–æ—Ä, 2) –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, 3) –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, 4) –ù–µ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, 5) –ü–ª–∞–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, 6) –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞;
- –ø–æ–º–µ—Ç—å –±–ª–æ–∫–∏ —Å—Ç–∞—Ç—É—Å–∞–º–∏ (–ì–û–¢–û–í–û, –í –ü–†–û–¶–ï–°–°–ï, –ü–õ–ê–ù) –∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï (—Å–Ω–∞—á–∞–ª–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¢–ó, –∑–∞—Ç–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫–∞):

<–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¢–ó>
{tz_context}

<–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤>
{collaboration_summary}

{guidance}

–°–§–û–†–ú–ò–†–£–ô –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ –¢–ó –≤ Markdown —Å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ–º, —Å–ø–∏—Å–∫–∞–º–∏ –∑–∞–¥–∞—á, –∏ —á—ë—Ç–∫–∏–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏.
"""

        return self.deepseek_call(prompt)

    def save_corrected_spec(self, content: str, output_dir: Path) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¢–ó –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é."""

        output_dir.mkdir(parents=True, exist_ok=True)
        spec_path = output_dir / f"corrected_tz_{self.session_id}.md"
        with open(spec_path, "w", encoding="utf-8") as f:
            f.write(content)
        return spec_path

    def comparative_review(
        self,
        previous_spec: Optional[str],
        new_spec: Optional[str],
        previous_session_summary: str,
    ) -> List[Dict]:
        """Run a comparative review loop so agents can self-evaluate progress."""

        if not previous_spec or not new_spec:
            return []

        comparison_results: List[Dict] = []

        perplexity_prompt = f"""–°—Ä–∞–≤–Ω–∏ –¥–≤–µ –≤–µ—Ä—Å–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è.

<–ü—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è>
{previous_spec[:8000]}

<–ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è>
{new_spec[:8000]}

–¢—Ä–µ–±—É–µ—Ç—Å—è:
1. –í—ã—è–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∏ —á—Ç–æ –≤—Å—ë –µ—â—ë —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏.
2. –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, KPI, –ø–ª–∞–Ω–æ–≤ –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º.
3. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫/—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Ç–≤–µ—Ä–¥—è—Ç —É–ª—É—á—à–µ–Ω–∏—è (self-learning signals).
"""

        print()
        print("üìä Perplexity: –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–µ—Ä—Å–∏–π –¢–ó...")
        perplexity_analysis = self.perplexity_call(perplexity_prompt)
        comparison_results.append(perplexity_analysis)

        deepseek_prompt = f"""–¢—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä. –ù–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—á—ë—Ç–∞ Perplexity –∏ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å–µ—Å—Å–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –ø–ª–∞–Ω —Å–∞–º–æ—É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è.

<–í—ã–≤–æ–¥—ã Perplexity>
{perplexity_analysis.get('content', '')[:6000]}

<–†–µ–∑—é–º–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å–µ—Å—Å–∏–∏>
{previous_session_summary[:6000]}

–¢—Ä–µ–±—É–µ—Ç—Å—è:
1. –ö—Ä–∞—Ç–∫–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–µ–ª—å—Ç—ã (—á—Ç–æ —É–ª—É—á—à–∏–ª–æ—Å—å, —á—Ç–æ –Ω–µ—Ç).
2. –°–æ—Å—Ç–∞–≤–∏—Ç—å roadmap –∏–∑ 5-7 –∑–∞–¥–∞—á self-improvement (–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º—ã—Ö –∞–≥–µ–Ω—Ç–∞–º–∏).
3. –£–∫–∞–∑–∞—Ç—å, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ/–º–æ–¥—É–ª–∏ –Ω—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –≤ —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–≥–æ–Ω.
"""

        if perplexity_analysis.get("success"):
            print("üß† DeepSeek: –ü–ª–∞–Ω —Å–∞–º–æ—É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è...")
            deepseek_follow = self.deepseek_call(deepseek_prompt)
            comparison_results.append(deepseek_follow)

        return comparison_results


def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞ —Å–≤—è–∑–∏"""
    
    print("=" * 80)
    print("MULTI-AGENT COMMUNICATION CHANNEL TEST")
    print("=" * 80)
    print()
    tz_directory = PROJECT_ROOT / "ai_audit_results"
    tz_context, tz_files = load_tz_context(tz_directory)
    tz_corrected_dir = tz_directory / "tz_corrected"
    previous_spec_text, previous_spec_path = load_latest_corrected_spec(tz_corrected_dir)
    print("üìÑ –ó–∞–≥—Ä—É–∑–∏–ª–∏ –¢–ó —Ñ–∞–π–ª—ã:")
    for path in tz_files:
        print(f"   ‚Ä¢ {path.name}")
    print()

    shared_context = "–°–≤–æ–¥–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –¢–ó (—É–∫–æ—Ä–æ—á–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏):\n" + tz_context
    if previous_spec_text and previous_spec_path:
        shared_context += (
            "\n\n<–ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –¢–ó>\n"
            f"–§–∞–π–ª: {previous_spec_path.name}\n\n{previous_spec_text[:8000]}"
        )

    channel = MultiAgentChannel()

    deepseek_task = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –≤–µ—Ä—Å–∏–∏ –¢–ó –∏ —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞.
1. –û–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–∏–µ –º–æ–¥—É–ª–∏ —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –∫–∞–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¢–ó –∫—Ä–∏—Ç–∏—á–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å.
2. –£–∫–∞–∂–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–æ–ª–≥–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (MCP ‚Üî Sandbox ‚Üî Knowledge Base ‚Üî Unified Agent Interface).
3. –°—Ñ–æ—Ä–º–∏—Ä—É–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –≤ –¢–ó (—á—Ç–æ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å, –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å, —É–¥–∞–ª–∏—Ç—å)."""

    perplexity_task = """–°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¢–ó.
1. –°—Ä–∞–≤–Ω–∏ –≤–µ—Ä—Å–∏–∏ –¢–ó –∏ –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –∑—Ä–µ–ª—ã–µ –±–ª–æ–∫–∏.
2. –û–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –Ω–∞—á–∞–ª—å–Ω—ã–µ –∏–¥–µ–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, multi-agent orchestration, caching, sandbox, kb).
3. –°–æ—Å—Ç–∞–≤—å –ø–ª–∞–Ω –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¢–ó: –∫–∞–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã —Ä–∞—Å—à–∏—Ä–∏—Ç—å, –∫–∞–∫–∏–µ KPI –∏ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–∏—Ç—å."""

    results = channel.collaborative_analysis(
        topic="TZ Modernization",
        deepseek_task=deepseek_task,
        perplexity_task=perplexity_task,
        iterations=2,
        shared_context=shared_context,
    )

    collaboration_summary = summarize_results(results)

    print()
    print("=" * 80)
    print("–§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –û–ë–ù–û–í–õ–Å–ù–ù–û–ì–û –¢–ó")
    print("=" * 80)
    print()

    spec_result = channel.generate_corrected_spec(tz_context, collaboration_summary)
    results.append(spec_result)

    tz_output_dir = tz_directory / "tz_corrected"
    if spec_result.get("success"):
        spec_path = channel.save_corrected_spec(spec_result["content"], tz_output_dir)
        print(f"‚úÖ –û–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ –¢–ó —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {spec_path}")
    else:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ –¢–ó: {spec_result.get('error')}")

    # Comparative self-improvement stage
    new_spec_text = spec_result.get("content") if spec_result.get("success") else None
    comparison_stage = channel.comparative_review(
        previous_spec=previous_spec_text,
        new_spec=new_spec_text,
        previous_session_summary=collaboration_summary,
    )
    results.extend(comparison_stage)

    print()
    print("=" * 80)
    print("–°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 80)
    print()

    md_path = channel.save_session(results, "multi_agent_session.json")
    print(f"‚úÖ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: multi_agent_session.json")
    print(f"‚úÖ Markdown —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {md_path}")
    print()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    success_count = sum(1 for r in results if r.get("success"))
    total_count = len(results)

    print("=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_count}")
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö: {success_count}")
    print(f"–ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {total_count - success_count}")
    if total_count:
        print(f"Success Rate: {success_count / total_count * 100:.1f}%")
    print("=" * 80)


if __name__ == "__main__":
    main()
