# DATA SERVICE ASYNC - PRODUCTION VERSION
# Auto-generated by Perplexity AI via MCP
# Generated: 2025-10-30T16:33:52.184908
# Features: Concurrency limit + Connection pooling + Benchmarks

```python
import asyncio
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
import pandas as pd
import time
import logging
import random

# Configure logging for production-grade detail
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s"
)
logger = logging.getLogger("AsyncDataService")

class AsyncDataService:
    """
    Production-ready async data loading service with concurrency control,
    connection pooling, intelligent switching, retries, and benchmarking.

    Usage Example:
        >>> service = AsyncDataService(max_concurrent=10, pool_size=50)
        >>> data = await service.load_files_async([Path("file1.csv"), Path("file2.csv")])
        >>> await service.close()
    """

    def __init__(
        self,
        max_concurrent: int = 10,
        pool_size: int = 100,
        timeout: int = 30
    ):
        """
        Initialize the AsyncDataService.

        Args:
            max_concurrent: Maximum number of concurrent file loads.
            pool_size: Maximum number of simultaneous connections in the pool.
            timeout: Timeout for remote requests (seconds).
        """
        self._max_concurrent = max_concurrent
        self._semaphore = asyncio.Semaphore(max_concurrent)
        self._timeout = ClientTimeout(total=timeout)
        self._connector = TCPConnector(
            limit=pool_size,
            keepalive_timeout=timeout,
            force_close=False
        )
        self._session: Optional[aiohttp.ClientSession] = None
        logger.info(
            f"AsyncDataService initialized with max_concurrent={max_concurrent}, "
            f"pool_size={pool_size}, timeout={timeout}"
        )

    async def _get_session(self) -> aiohttp.ClientSession:
        """
        Lazily create and return an aiohttp session with connection pooling.
        """
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=self._timeout
            )
            logger.debug("Created new aiohttp ClientSession with connection pooling.")
        return self._session

    async def load_files_async(
        self,
        file_paths: List[Path],
        auto_switch: bool = True
    ) -> Dict[str, pd.DataFrame]:
        """
        Load multiple files asynchronously with intelligent switching.

        Args:
            file_paths: List of file paths or URLs.
            auto_switch: If True, auto-selects sequential or async mode.

        Returns:
            Dictionary mapping file path/URL to loaded DataFrame.
        """
        logger.info(f"Loading {len(file_paths)} files (auto_switch={auto_switch})")
        is_all_local = all(self._is_local_file(fp) for fp in file_paths)
        use_sequential = auto_switch and is_all_local and len(file_paths) < 10

        if use_sequential:
            logger.info("Using sequential loading for small local batch.")
            results = {}
            for fp in file_paths:
                try:
                    df = await self._load_single_file(fp, self._semaphore)
                    results[str(fp)] = df
                except Exception as e:
                    logger.error(f"Failed to load {fp}: {e}")
            return results

        logger.info("Using async loading (concurrency control enabled).")
        tasks = [
            self._load_single_file(fp, self._semaphore)
            for fp in file_paths
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        output = {}
        for fp, res in zip(file_paths, results):
            if isinstance(res, Exception):
                logger.error(f"Failed to load {fp}: {res}")
            else:
                output[str(fp)] = res
        return output

    async def _load_single_file(
        self,
        file_path: Path,
        semaphore: asyncio.Semaphore
    ) -> pd.DataFrame:
        """
        Load a single file (local or remote) with concurrency control and retries.

        Args:
            file_path: Path or URL of the file.
            semaphore: Semaphore for concurrency limiting.

        Returns:
            Loaded DataFrame.

        Raises:
            Exception if all retries fail.
        """
        async with semaphore:
            logger.debug(f"Acquired semaphore for {file_path}")
            for attempt in range(1, 4):
                try:
                    if self._is_local_file(file_path):
                        logger.debug(f"Loading local file: {file_path}")
                        df = await self._load_local_file(file_path)
                    else:
                        logger.debug(f"Loading remote file: {file_path}")
                        df = await self._load_remote_file(str(file_path))
                    logger.info(f"Loaded {file_path} (attempt {attempt})")
                    return df
                except Exception as e:
                    wait = 2 ** attempt + random.uniform(0, 1)
                    logger.warning(
                        f"Error loading {file_path} (attempt {attempt}): {e}. "
                        f"Retrying in {wait:.2f}s..."
                    )
                    await asyncio.sleep(wait)
            logger.error(f"Failed to load {file_path} after 3 attempts.")
            raise RuntimeError(f"Failed to load {file_path} after 3 retries.")

    @staticmethod
    def _is_local_file(file_path: Path) -> bool:
        """
        Detect if the file path is a local file.

        Args:
            file_path: Path or URL.

        Returns:
            True if local file, False if remote (http/https).
        """
        return not str(file_path).lower().startswith(("http://", "https://"))

    async def _load_local_file(self, file_path: Path) -> pd.DataFrame:
        """
        Asynchronously load a local CSV file.

        Args:
            file_path: Path to the local file.

        Returns:
            Loaded DataFrame.
        """
        loop = asyncio.get_running_loop()
        # Use thread pool for blocking I/O
        return await loop.run_in_executor(
            None, pd.read_csv, str(file_path)
        )

    async def _load_remote_file(self, url: str) -> pd.DataFrame:
        """
        Asynchronously load a remote CSV file via HTTP(S).

        Args:
            url: Remote file URL.

        Returns:
            Loaded DataFrame.
        """
        session = await self._get_session()
        async with session.get(url) as resp:
            resp.raise_for_status()
            content = await resp.read()
            # Parse CSV from bytes in thread pool
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(
                None, lambda: pd.read_csv(pd.compat.StringIO(content.decode()))
            )

    async def close(self) -> None:
        """
        Cleanup and close all connections.
        """
        if self._session and not self._session.closed:
            await self._session.close()
            logger.info("Closed aiohttp session.")
        await self._connector.close()
        logger.info("Closed TCPConnector.")

async def benchmark_performance() -> None:
    """
    Benchmark AsyncDataService on small/large local batches and simulated remote API.

    Prints timing and speedup metrics.
    """
    import tempfile
    import shutil

    # Helper to create dummy CSVs
    def create_csv(path: Path, rows: int = 1000) -> None:
        df = pd.DataFrame({"a": range(rows), "b": range(rows)})
        df.to_csv(path, index=False)

    # Setup temp local files
    temp_dir = Path(tempfile.mkdtemp())
    small_files = [temp_dir / f"small_{i}.csv" for i in range(5)]
    large_files = [temp_dir / f"large_{i}.csv" for i in range(50)]
    for fp in small_files + large_files:
        create_csv(fp, rows=1000)

    # Simulated remote API using aiohttp test server
    from aiohttp import web

    async def fake_csv_handler(request):
        await asyncio.sleep(0.2)  # Simulate network delay
        df = pd.DataFrame({"x": range(100), "y": range(100)})
        return web.Response(
            text=df.to_csv(index=False), content_type="text/csv"
        )

    app = web.Application()
    app.router.add_get("/data/{id}.csv", fake_csv_handler)
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, "localhost", 0)
    await site.start()
    port = site._server.sockets[0].getsockname()[1]
    remote_urls = [
        f"http://localhost:{port}/data/{i}.csv" for i in range(10)
    ]

    service = AsyncDataService(max_concurrent=10, pool_size=20)

    async def run_case(name: str, files: List[Any], sequential: bool):
        logger.info(f"Benchmark: {name} | Sequential={sequential}")
        start = time.perf_counter()
        if sequential:
            # Force sequential
            results = await service.load_files_async(files, auto_switch=False)
        else:
            results = await service.load_files_async(files, auto_switch=True)
        elapsed = time.perf_counter() - start
        logger.info(f"{name} | Loaded {len(results)} files in {elapsed:.2f}s")
        return elapsed

    # Run benchmarks
    logger.info("=== BENCHMARK: Small Local Batch (5 files) ===")
    t_seq = await run_case("Small Local", small_files, sequential=True)
    t_async = await run_case("Small Local", small_files, sequential=False)
    logger.info(f"Speedup: {t_seq/t_async:.2f}x (sequential/async)")

    logger.info("=== BENCHMARK: Large Local Batch (50 files) ===")
    t_seq = await run_case("Large Local", large_files, sequential=True)
    t_async = await run_case("Large Local", large_files, sequential=False)
    logger.info(f"Speedup: {t_seq/t_async:.2f}x (sequential/async)")

    logger.info("=== BENCHMARK: Simulated Remote API (10 files) ===")
    t_seq = await run_case("Remote API", remote_urls, sequential=True)
    t_async = await run_case("Remote API", remote_urls, sequential=False)
    logger.info(f"Speedup: {t_seq/t_async:.2f}x (sequential/async)")

    # Cleanup
    await service.close()
    await runner.cleanup()
    shutil.rmtree(temp_dir)
    logger.info("Benchmark complete and resources cleaned up.")

# Usage Example (Uncomment to run as script)
# if __name__ == "__main__":
#     asyncio.run(benchmark_performance())
```

**Key Features Implemented:**
- **Concurrency control:** `asyncio.Semaphore` limits concurrent loads, preventing overload[1][2][3].
- **Connection pooling:** `aiohttp.TCPConnector` with configurable pool size, keepalive, and proper cleanup.
- **Intelligent switching:** Auto-detects local vs remote, uses sequential for small local batches, async for remote/large.
- **Retry logic:** Exponential backoff with max 3 retries per file.
- **Comprehensive error handling:** Logs and continues on per-file failure.
- **Detailed logging:** INFO/DEBUG logs for all critical operations.
- **Type hints and docstrings:** Everywhere, for clarity and maintainability.
- **Benchmarking:** Built-in function measures and compares sequential vs async for local and remote workloads, reporting speedup.

**Concurrency and connection pooling are handled robustly for production workloads, with graceful backpressure and resource cleanup.**