"""
Comprehensive Test Suite for New AI Agent Features

Tests all newly implemented components:
- MCP Integration
- Real LLM Connections
- Shared Memory
- ML Anomaly Detection
- Agent Communication Protocol
- Prometheus/Grafana Integration
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


# ============================================================
# Test Results Tracking
# ============================================================
test_results = {
    "passed": 0,
    "failed": 0,
    "skipped": 0,
    "details": {},
}


def mark_test(category: str, test_name: str, passed: bool, error: str = None):
    """Mark test result"""
    if category not in test_results["details"]:
        test_results["details"][category] = []

    test_results["details"][category].append(
        {
            "name": test_name,
            "passed": passed,
            "error": error,
        }
    )

    if passed:
        test_results["passed"] += 1
    else:
        test_results["failed"] += 1


# ============================================================
# MCP Tests
# ============================================================
async def test_mcp():
    """Test MCP Protocol"""
    category = "MCP"

    try:
        from backend.agents.mcp.protocol import (
            MCPMessage,
            MCPPrompt,
            MCPResource,
            MCPServer,
        )

        # Test MCPMessage
        msg = MCPMessage.request("tools/list", {"cursor": None})
        assert msg.id is not None
        assert msg.method == "tools/list"
        mark_test(category, "message_creation", True)

        # Test MCPServer
        server = MCPServer(name="test-server")

        @server.tool("test_tool")
        async def test_tool(value: int) -> dict:
            """Test tool"""
            return {"result": value * 2}

        assert "test_tool" in server.tools
        mark_test(category, "tool_registration", True)

        # Test resource
        server.add_resource(
            MCPResource(
                uri="test://resource",
                name="Test Resource",
                description="A test resource",
            )
        )
        assert "test://resource" in server.resources
        mark_test(category, "resource_management", True)

        # Test prompt
        server.add_prompt(
            MCPPrompt(
                name="test_prompt",
                description="A test prompt",
            )
        )
        assert "test_prompt" in server.prompts
        mark_test(category, "prompt_management", True)

    except Exception as e:
        mark_test(category, "mcp_integration", False, str(e))


async def test_tool_registry():
    """Test Tool Registry"""
    category = "MCP"

    try:
        from backend.agents.mcp.tool_registry import ToolRegistry

        registry = ToolRegistry()

        @registry.register(category="trading")
        async def calculate_rsi(prices: list, period: int = 14):
            """Calculate RSI"""
            return {"rsi": 50.0}

        tools = registry.list_tools(category="trading")
        assert len(tools) >= 1
        mark_test(category, "tool_registry", True)

        # Execute tool
        result = await registry.execute("calculate_rsi", prices=[100, 101, 102])
        assert result.success
        mark_test(category, "tool_execution", True)

    except Exception as e:
        mark_test(category, "tool_registry", False, str(e))


async def test_resource_manager():
    """Test Resource Manager"""
    category = "MCP"

    try:
        from backend.agents.mcp.resource_manager import MemoryResourceProvider, ResourceManager

        manager = ResourceManager()
        memory_provider = MemoryResourceProvider()

        # Add resource
        memory_provider.add_resource(
            uri="memory://test",
            name="Test",
            content="Test content",
        )

        manager.add_provider(memory_provider)

        # List resources
        resources = await manager.list_resources()
        assert len(resources) >= 1
        mark_test(category, "resource_listing", True)

        # Read resource
        content = await manager.read_resource("memory://test")
        assert content.text == "Test content"
        mark_test(category, "resource_reading", True)

    except Exception as e:
        mark_test(category, "resource_manager", False, str(e))


async def test_context_manager():
    """Test Context Manager"""
    category = "MCP"

    try:
        from backend.agents.mcp.context_manager import ContextManager, ContextScope

        manager = ContextManager()

        # Create context
        ctx = manager.create_context(ContextScope.REQUEST)
        ctx.set("user_id", "user_123")
        assert ctx.get("user_id") == "user_123"
        mark_test(category, "context_creation", True)

        # Context inheritance
        child = ctx.create_child(ContextScope.TASK)
        assert child.get("user_id") == "user_123"
        mark_test(category, "context_inheritance", True)

        # Context scope
        with manager.use_context(ctx):
            current = manager.get_current()
            assert current.id == ctx.id
        mark_test(category, "context_scope", True)

    except Exception as e:
        mark_test(category, "context_manager", False, str(e))


# ============================================================
# LLM Connection Tests
# ============================================================
async def test_llm_connections():
    """Test LLM Connections (mock)"""
    category = "LLM"

    try:
        from backend.agents.llm.connections import (
            LLMConfig,
            LLMMessage,
            LLMProvider,
            LLMResponse,
            RateLimiter,
        )

        # Test LLMMessage
        msg = LLMMessage(role="user", content="Hello")
        assert msg.to_dict()["role"] == "user"
        mark_test(category, "message_creation", True)

        # Test LLMResponse
        response = LLMResponse(
            content="Hello!",
            model="test-model",
            provider=LLMProvider.DEEPSEEK,
            prompt_tokens=10,
            completion_tokens=5,
        )
        assert response.total_tokens == 15  # Should be calculated or set
        mark_test(category, "response_structure", True)

        # Test RateLimiter (deprecated â€” backward compat)
        import warnings

        with warnings.catch_warnings():
            warnings.simplefilter("ignore", DeprecationWarning)
            limiter = RateLimiter(rate_limit_rpm=60)
        acquired = await limiter.acquire()
        assert acquired
        mark_test(category, "rate_limiter", True)

        # Test LLMConfig
        config = LLMConfig(
            provider=LLMProvider.DEEPSEEK,
            api_key="test-key",
            model="deepseek-chat",
        )
        assert config.temperature == 0.7
        mark_test(category, "config_defaults", True)

    except Exception as e:
        mark_test(category, "llm_connections", False, str(e))


# ============================================================
# Shared Memory Tests
# ============================================================
import pytest


@pytest.mark.skip(reason="SharedMemory asyncio.Lock incompatible with pytest-asyncio on Windows/Python 3.13")
async def test_shared_memory():
    """Test Shared Memory"""
    category = "SharedMemory"

    try:
        from backend.agents.memory.shared_memory import SharedMemory

        memory = SharedMemory()

        # Basic set/get
        await memory.set("agent_1", "key1", "value1")
        value = await memory.get("key1")
        assert value == "value1"
        mark_test(category, "basic_operations", True)

        # Increment
        await memory.set("agent_1", "counter", 0)
        new_value = await memory.increment("agent_1", "counter", 5)
        assert new_value == 5.0
        mark_test(category, "atomic_increment", True)

        # Versioning
        value, version = await memory.get_with_version("counter")
        assert version > 0
        mark_test(category, "versioning", True)

        # Compare and swap
        success = await memory.compare_and_swap("agent_1", "counter", 5.0, 10.0)
        assert success
        mark_test(category, "compare_and_swap", True)

        # Locking
        locked = await memory.acquire_lock("agent_1", "resource")
        assert locked
        released = await memory.release_lock("agent_1", "resource")
        assert released
        mark_test(category, "locking", True)

        # Transaction
        async with memory.transaction("agent_1") as tx:
            tx.set("tx_key1", "tx_value1")
            tx.set("tx_key2", "tx_value2")

        assert await memory.get("tx_key1") == "tx_value1"
        mark_test(category, "transactions", True)

    except Exception as e:
        mark_test(category, "shared_memory", False, str(e))


# ============================================================
# ML Anomaly Detection Tests
# ============================================================
@pytest.mark.skip(reason="MLAnomalyDetector asyncio.to_thread hangs on Windows/Python 3.13")
async def test_ml_anomaly():
    """Test ML Anomaly Detection"""
    category = "MLAnomaly"

    try:
        import numpy as np

        from backend.agents.monitoring.ml_anomaly import (
            EnsembleDetector,
            IQRDetector,
            MLAnomalyDetector,
            MovingAverageDetector,
            ZScoreDetector,
        )

        # Generate test data
        np.random.seed(42)
        normal_data = np.random.normal(100, 10, 100)

        # Z-Score detector
        zscore = ZScoreDetector(threshold=3.0)
        zscore.fit(normal_data)
        scores = zscore.score(np.array([100, 150, 200]))
        assert len(scores) == 3
        mark_test(category, "zscore_detector", True)

        # IQR detector
        iqr = IQRDetector(multiplier=1.5)
        iqr.fit(normal_data)
        detections = iqr.detect(np.array([100, 150, 200]))
        assert len(detections) == 3
        mark_test(category, "iqr_detector", True)

        # Moving Average detector
        ma = MovingAverageDetector(window_size=10)
        ma.fit(normal_data)
        detections = ma.detect(normal_data)
        assert len(detections) == len(normal_data)
        mark_test(category, "moving_average_detector", True)

        # Ensemble detector
        ensemble = EnsembleDetector()
        ensemble.fit(normal_data)
        detections = ensemble.detect(np.array([100, 150, 200]))
        assert len(detections) == 3
        mark_test(category, "ensemble_detector", True)

        # ML Anomaly Detector
        detector = MLAnomalyDetector()
        await detector.train("test_metric", list(normal_data))

        # Detect anomalies
        anomalies = await detector.detect(
            "test_metric",
            [100, 150, 200, 100, 100],
        )
        mark_test(category, "ml_anomaly_detector", True)

    except Exception as e:
        mark_test(category, "ml_anomaly", False, str(e))


# ============================================================
# Communication Protocol Tests
# ============================================================
async def test_communication():
    """Test Agent Communication Protocol"""
    category = "Communication"

    try:
        from backend.agents.communication.protocol import (
            AgentInfo,
            Message,
            MessageBroker,
        )

        # Test Message
        msg = Message(
            sender_id="agent_1",
            receiver_id="agent_2",
            topic="test",
            payload={"data": "test"},
        )
        assert msg.id is not None
        assert not msg.is_expired()
        mark_test(category, "message_creation", True)

        # Test AgentInfo
        info = AgentInfo(
            agent_id="test_agent",
            agent_type="trading",
            capabilities=["analyze", "recommend"],
        )
        assert "analyze" in info.capabilities
        mark_test(category, "agent_info", True)

        # Test MessageBroker (without starting background loop)
        broker = MessageBroker()

        # Register agent
        broker.register_agent(info)
        agents = broker.list_agents()
        assert len(agents) >= 1
        mark_test(category, "agent_registration", True)

        # Subscribe to topic
        received = []

        def handler(msg):
            received.append(msg)

        broker.subscribe("test_topic", handler)

        # Publish message (synchronous delivery via subscribe)
        await broker.publish(
            Message(
                sender_id="test_agent",
                topic="test_topic",
                payload={"test": True},
            )
        )

        assert len(received) >= 1
        mark_test(category, "pub_sub", True)

        # Test stats
        stats = broker.get_stats()
        assert stats["messages_sent"] > 0
        mark_test(category, "broker_stats", True)

    except Exception as e:
        mark_test(category, "communication", False, str(e))


# ============================================================
# Prometheus/Grafana Tests
# ============================================================
async def test_prometheus_grafana():
    """Test Prometheus/Grafana Integration"""
    category = "PrometheusGrafana"

    try:
        from backend.agents.monitoring.prometheus_grafana import (
            GrafanaConfig,
            GrafanaDashboard,
            PrometheusConfig,
            create_ai_agent_dashboard,
        )

        # Test PrometheusConfig
        config = PrometheusConfig(
            endpoint="/metrics",
            port=9090,
        )
        assert config.namespace == "ai_agent"
        mark_test(category, "prometheus_config", True)

        # Test GrafanaConfig
        grafana_config = GrafanaConfig(
            url="http://localhost:3000",
        )
        assert grafana_config.org_id == 1
        mark_test(category, "grafana_config", True)

        # Test GrafanaDashboard
        dashboard = GrafanaDashboard(
            uid="test-dashboard",
            title="Test Dashboard",
            panels=[{"id": 1, "type": "stat", "title": "Test"}],
        )
        data = dashboard.to_dict()
        assert data["uid"] == "test-dashboard"
        mark_test(category, "dashboard_creation", True)

        # Test pre-built dashboard
        ai_dashboard = create_ai_agent_dashboard()
        assert len(ai_dashboard.panels) > 0
        mark_test(category, "ai_dashboard_template", True)

    except Exception as e:
        mark_test(category, "prometheus_grafana", False, str(e))


# ============================================================
# Dashboard API Tests
# ============================================================
async def test_dashboard_api():
    """Test Dashboard API"""
    category = "DashboardAPI"

    try:
        from backend.agents.dashboard.api import AlertCreate, DashboardWidget, MetricQuery

        # Test MetricQuery
        query = MetricQuery(
            metric_name="agent_requests_total",
            time_from="now-1h",
        )
        assert query.aggregation == "avg"
        mark_test(category, "metric_query_model", True)

        # Test AlertCreate
        alert = AlertCreate(
            name="high_latency",
            metric_name="latency_ms",
            condition="gt",
            threshold=5000,
        )
        assert alert.severity == "warning"
        mark_test(category, "alert_create_model", True)

        # Test DashboardWidget
        widget = DashboardWidget(
            id="w1",
            type="stat",
            title="Requests",
            metric_name="requests_total",
            position={"x": 0, "y": 0, "w": 3, "h": 2},
        )
        assert widget.type == "stat"
        mark_test(category, "widget_model", True)

    except Exception as e:
        mark_test(category, "dashboard_api", False, str(e))


# ============================================================
# Main Test Runner
# ============================================================
async def run_all_tests():
    """Run all tests"""
    print("\n" + "=" * 70)
    print("ğŸ§ª NEW FEATURES TEST SUITE")
    print("=" * 70)

    # Test categories
    test_suites = [
        (
            "MCP Protocol",
            [
                test_mcp,
                test_tool_registry,
                test_resource_manager,
                test_context_manager,
            ],
        ),
        ("LLM Connections", [test_llm_connections]),
        ("Shared Memory", [test_shared_memory]),
        ("ML Anomaly Detection", [test_ml_anomaly]),
        ("Communication Protocol", [test_communication]),
        ("Prometheus/Grafana", [test_prometheus_grafana]),
        ("Dashboard API", [test_dashboard_api]),
    ]

    for suite_name, tests in test_suites:
        print(f"\n{'=' * 60}")
        print(f"ğŸ§ª {suite_name}")
        print("=" * 60)

        for test_fn in tests:
            try:
                await test_fn()
                print(f"  âœ… {test_fn.__name__}")
            except Exception as e:
                print(f"  âŒ {test_fn.__name__}: {e}")

    # Print summary
    print("\n" + "=" * 70)
    print("ğŸ“‹ TEST SUMMARY")
    print("=" * 70)

    for category, tests in test_results["details"].items():
        passed = sum(1 for t in tests if t["passed"])
        total = len(tests)
        status = "âœ…" if passed == total else "âš ï¸"
        print(f"\n{status} {category}: {passed}/{total} passed")

        for test in tests:
            icon = "âœ…" if test["passed"] else "âŒ"
            print(f"     {icon} {test['name']}")
            if test["error"]:
                print(f"        Error: {test['error'][:50]}...")

    print("\n" + "-" * 70)
    total = test_results["passed"] + test_results["failed"]
    percent = (test_results["passed"] / total * 100) if total > 0 else 0
    print(f"ğŸ“Š Total: {test_results['passed']}/{total} passed ({percent:.1f}%)")
    print("=" * 70)

    return test_results["failed"] == 0


if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)
