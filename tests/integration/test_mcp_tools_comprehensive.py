"""
ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… MCP Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Cyclic Dialogue (Copilot â†” Perplexity)

Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹:
1. Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ (7)
2. Perplexity Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· (4)
3. Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ (8)

Ğ’Ğ¡Ğ•Ğ“Ğ: 19 MCP Tools
"""

import pytest
import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ñ€ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ² PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ MCP server Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ
from mcp_server.server import (
    # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    get_project_structure,
    list_available_strategies,
    get_supported_timeframes,
    get_backtest_capabilities,
    check_system_status,
    get_testing_summary,
    explain_project_architecture,
    
    # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Perplexity Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    perplexity_search,
    perplexity_analyze_crypto,
    perplexity_strategy_research,
    perplexity_market_news,
    
    # Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    analyze_backtest_results,
    compare_strategies,
    risk_management_advice,
    technical_indicator_research,
    explain_metric,
    market_regime_detection,
    code_review_strategy,
    generate_test_scenarios
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¢Ğ•Ğ¡Ğ¢ĞĞ’Ğ«Ğ• Ğ£Ğ¢Ğ˜Ğ›Ğ˜Ğ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CyclicDialogueRecorder:
    """Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¼ĞµĞ¶Ğ´Ñƒ Copilot Ğ¸ Perplexity"""
    
    def __init__(self):
        self.turns: List[Dict[str, Any]] = []
        self.total_tokens = 0
        self.start_time = None
        self.end_time = None
    
    def start(self):
        self.start_time = datetime.now()
    
    def end(self):
        self.end_time = datetime.now()
    
    def add_turn(self, speaker: str, content: str, metadata: Dict[str, Any] = None):
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºÑƒ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³"""
        turn = {
            "turn_number": len(self.turns) + 1,
            "speaker": speaker,
            "content": content,
            "metadata": metadata or {},
            "timestamp": datetime.now().isoformat()
        }
        self.turns.append(turn)
        
        # ĞŸĞ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°)
        if metadata and "tokens" in metadata:
            self.total_tokens += metadata["tokens"]
    
    def get_duration(self) -> float:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    def get_summary(self) -> Dict[str, Any]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ğ´ĞºÑƒ Ğ¿Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ñƒ"""
        return {
            "total_turns": len(self.turns),
            "duration_seconds": self.get_duration(),
            "total_tokens": self.total_tokens,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None
        }
    
    def save_to_file(self, filepath: Path):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² JSON Ñ„Ğ°Ğ¹Ğ»"""
        data = {
            "summary": self.get_summary(),
            "dialogue": self.turns
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


async def cyclic_test_tool(
    tool_name: str,
    tool_func: callable,
    tool_args: Dict[str, Any],
    recorder: CyclicDialogueRecorder
) -> Dict[str, Any]:
    """
    ĞŸÑ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ MCP Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°
    
    Ğ¨Ğ°Ğ³Ğ¸:
    1. Copilot â†’ Question: Ğ—Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Perplexity Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ°Ğº Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
    2. Perplexity â†’ Answer: ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
    3. Copilot â†’ Analysis: ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
    4. Copilot â†’ Execute: Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ñ‚ĞµÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°
    5. Perplexity â†’ Verification: ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ‚ĞµÑÑ‚Ğ°
    """
    
    # Turn 1: Copilot ÑĞ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Perplexity, ĞºĞ°Ğº Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
    question = f"""
    How should I test the MCP tool "{tool_name}" with these parameters:
    {json.dumps(tool_args, indent=2)}
    
    What are:
    1. Key test scenarios
    2. Expected outputs
    3. Edge cases to check
    4. Success criteria
    """
    recorder.add_turn("Copilot", question, {"type": "question"})
    
    # Turn 2: Perplexity Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ perplexity_search)
    try:
        perplexity_response = await perplexity_search(question, model="sonar")
        recorder.add_turn(
            "Perplexity",
            perplexity_response.get("answer", "No answer received"),
            {
                "type": "answer",
                "success": perplexity_response.get("success", False),
                "tokens": perplexity_response.get("usage", {}).get("total_tokens", 0)
            }
        )
    except Exception as e:
        recorder.add_turn("Perplexity", f"Error: {str(e)}", {"type": "error"})
        perplexity_response = {"success": False, "error": str(e)}
    
    # Turn 3: Copilot Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
    analysis = f"""
    Based on Perplexity recommendations, I will:
    1. Execute {tool_name} with provided arguments
    2. Validate output structure
    3. Check for expected fields
    4. Verify data types and values
    """
    recorder.add_turn("Copilot", analysis, {"type": "analysis"})
    
    # Turn 4: Copilot Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚ĞµÑÑ‚
    test_result = {
        "tool_name": tool_name,
        "arguments": tool_args,
        "status": "UNKNOWN",
        "output": None,
        "error": None,
        "validation": {}
    }
    
    try:
        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚
        output = await tool_func(**tool_args)
        test_result["output"] = output
        test_result["status"] = "SUCCESS"
        
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
        if isinstance(output, dict):
            test_result["validation"]["is_dict"] = True
            test_result["validation"]["has_keys"] = len(output) > 0
            
            # Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Perplexity Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            if "success" in output:
                test_result["validation"]["perplexity_success"] = output["success"]
            if "answer" in output:
                test_result["validation"]["has_answer"] = len(output["answer"]) > 0
        elif isinstance(output, str):
            test_result["validation"]["is_string"] = True
            test_result["validation"]["not_empty"] = len(output) > 0
        
        recorder.add_turn(
            "Copilot",
            f"Tool executed successfully. Output: {json.dumps(output, indent=2)[:500]}...",
            {"type": "execution", "status": "success"}
        )
        
    except Exception as e:
        test_result["status"] = "FAILED"
        test_result["error"] = str(e)
        recorder.add_turn(
            "Copilot",
            f"Tool execution failed: {str(e)}",
            {"type": "execution", "status": "failed"}
        )
    
    # Turn 5: Perplexity Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    verification_query = f"""
    Verify test result for {tool_name}:
    
    Status: {test_result['status']}
    Validation: {json.dumps(test_result['validation'], indent=2)}
    Error: {test_result.get('error', 'None')}
    
    Is this result acceptable? What improvements are needed?
    """
    
    try:
        verification = await perplexity_search(verification_query, model="sonar")
        recorder.add_turn(
            "Perplexity",
            verification.get("answer", "No verification received"),
            {
                "type": "verification",
                "success": verification.get("success", False),
                "tokens": verification.get("usage", {}).get("total_tokens", 0)
            }
        )
    except Exception as e:
        recorder.add_turn("Perplexity", f"Verification error: {str(e)}", {"type": "error"})
    
    return test_result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¢Ğ•Ğ¡Ğ¢Ğ« ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ĞĞ«Ğ¥ Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢ĞĞ’ (7 ÑˆÑ‚ÑƒĞº)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@pytest.mark.asyncio
async def test_context_tools_comprehensive():
    """ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ²ÑĞµÑ… 7 ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
    
    recorder = CyclicDialogueRecorder()
    recorder.start()
    
    context_tools = [
        ("get_project_structure", get_project_structure, {}),
        ("list_available_strategies", list_available_strategies, {}),
        ("get_supported_timeframes", get_supported_timeframes, {}),
        ("get_backtest_capabilities", get_backtest_capabilities, {}),
        ("check_system_status", check_system_status, {}),
        ("get_testing_summary", get_testing_summary, {}),
        ("explain_project_architecture", explain_project_architecture, {})
    ]
    
    results = []
    for tool_name, tool_func, tool_args in context_tools:
        result = await cyclic_test_tool(tool_name, tool_func, tool_args, recorder)
        results.append(result)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        assert result["status"] in ["SUCCESS", "FAILED"], f"{tool_name}: Invalid status"
        if result["status"] == "SUCCESS":
            assert result["output"] is not None, f"{tool_name}: No output"
    
    recorder.end()
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
    output_dir = Path(__file__).parent.parent.parent / "results" / "mcp_tests"
    output_dir.mkdir(parents=True, exist_ok=True)
    recorder.save_to_file(output_dir / "context_tools_dialogue.json")
    
    # Ğ¡Ğ²Ğ¾Ğ´ĞºĞ°
    summary = recorder.get_summary()
    print(f"\n{'='*80}")
    print(f"CONTEXT TOOLS TEST SUMMARY")
    print(f"{'='*80}")
    print(f"Total Tools Tested: {len(results)}")
    print(f"Successful: {sum(1 for r in results if r['status'] == 'SUCCESS')}")
    print(f"Failed: {sum(1 for r in results if r['status'] == 'FAILED')}")
    print(f"Total Dialogue Turns: {summary['total_turns']}")
    print(f"Duration: {summary['duration_seconds']:.2f}s")
    print(f"Total Tokens: {summary['total_tokens']}")
    print(f"{'='*80}\n")
    
    # Ğ’ÑĞµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ¹Ñ‚Ğ¸
    assert all(r["status"] == "SUCCESS" for r in results), "Some context tools failed"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ‘ĞĞ—ĞĞ’Ğ«Ğ¥ PERPLEXITY Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢ĞĞ’ (4 ÑˆÑ‚ÑƒĞºĞ¸)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@pytest.mark.asyncio
async def test_perplexity_basic_tools():
    """Ğ¢ĞµÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Perplexity Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
    
    recorder = CyclicDialogueRecorder()
    recorder.start()
    
    perplexity_tools = [
        ("perplexity_search", perplexity_search, {
            "query": "What is the optimal RSI period for crypto day trading?",
            "model": "sonar"
        }),
        ("perplexity_analyze_crypto", perplexity_analyze_crypto, {
            "symbol": "BTC",
            "analysis_type": "technical"
        }),
        ("perplexity_strategy_research", perplexity_strategy_research, {
            "strategy_type": "momentum",
            "market_conditions": "trending"
        }),
        ("perplexity_market_news", perplexity_market_news, {
            "topic": "bitcoin",
            "timeframe": "24h"
        })
    ]
    
    results = []
    for tool_name, tool_func, tool_args in perplexity_tools:
        result = await cyclic_test_tool(tool_name, tool_func, tool_args, recorder)
        results.append(result)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Perplexity-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ
        if result["status"] == "SUCCESS":
            output = result["output"]
            assert "success" in output, f"{tool_name}: Missing 'success' field"
            assert "answer" in output, f"{tool_name}: Missing 'answer' field"
            
            if output.get("success"):
                assert len(output["answer"]) > 0, f"{tool_name}: Empty answer"
    
    recorder.end()
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
    output_dir = Path(__file__).parent.parent.parent / "results" / "mcp_tests"
    output_dir.mkdir(parents=True, exist_ok=True)
    recorder.save_to_file(output_dir / "perplexity_basic_tools_dialogue.json")
    
    # Ğ¡Ğ²Ğ¾Ğ´ĞºĞ°
    summary = recorder.get_summary()
    print(f"\n{'='*80}")
    print(f"PERPLEXITY BASIC TOOLS TEST SUMMARY")
    print(f"{'='*80}")
    print(f"Total Tools Tested: {len(results)}")
    print(f"Successful: {sum(1 for r in results if r['status'] == 'SUCCESS')}")
    print(f"Failed: {sum(1 for r in results if r['status'] == 'FAILED')}")
    print(f"Total Dialogue Turns: {summary['total_turns']}")
    print(f"Duration: {summary['duration_seconds']:.2f}s")
    print(f"Total Tokens: {summary['total_tokens']}")
    print(f"{'='*80}\n")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¢Ğ•Ğ¡Ğ¢Ğ« Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞ«Ğ¥ ĞĞĞĞ›Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ¥ Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢ĞĞ’ (8 ÑˆÑ‚ÑƒĞº)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@pytest.mark.asyncio
async def test_advanced_analytical_tools():
    """Ğ¢ĞµÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
    
    recorder = CyclicDialogueRecorder()
    recorder.start()
    
    advanced_tools = [
        ("analyze_backtest_results", analyze_backtest_results, {
            "backtest_id": 1,
            "detailed": True
        }),
        ("compare_strategies", compare_strategies, {
            "strategy_a": "EMA Crossover",
            "strategy_b": "RSI Mean Reversion",
            "market_type": "crypto"
        }),
        ("risk_management_advice", risk_management_advice, {
            "capital": 10000.0,
            "risk_per_trade": 2.0,
            "max_positions": 3
        }),
        ("technical_indicator_research", technical_indicator_research, {
            "indicator_name": "MACD",
            "use_case": "trend-following"
        }),
        ("explain_metric", explain_metric, {
            "metric_name": "Sharpe Ratio",
            "context": "crypto_trading"
        }),
        ("market_regime_detection", market_regime_detection, {
            "symbol": "BTCUSDT",
            "timeframe": "1d"
        }),
        ("code_review_strategy", code_review_strategy, {
            "strategy_code": """
def generate_signals(data):
    data['ema_fast'] = data['close'].ewm(span=12).mean()
    data['ema_slow'] = data['close'].ewm(span=26).mean()
    data['signal'] = 0
    data.loc[data['ema_fast'] > data['ema_slow'], 'signal'] = 1
    data.loc[data['ema_fast'] < data['ema_slow'], 'signal'] = -1
    return data
            """,
            "language": "python"
        }),
        ("generate_test_scenarios", generate_test_scenarios, {
            "strategy_name": "Bollinger Bands Breakout",
            "complexity": "comprehensive"
        })
    ]
    
    results = []
    for tool_name, tool_func, tool_args in advanced_tools:
        result = await cyclic_test_tool(tool_name, tool_func, tool_args, recorder)
        results.append(result)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
        if result["status"] == "SUCCESS":
            output = result["output"]
            assert "success" in output, f"{tool_name}: Missing 'success' field"
            assert "analysis_type" in output, f"{tool_name}: Missing 'analysis_type' field"
    
    recorder.end()
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
    output_dir = Path(__file__).parent.parent.parent / "results" / "mcp_tests"
    output_dir.mkdir(parents=True, exist_ok=True)
    recorder.save_to_file(output_dir / "advanced_tools_dialogue.json")
    
    # Ğ¡Ğ²Ğ¾Ğ´ĞºĞ°
    summary = recorder.get_summary()
    print(f"\n{'='*80}")
    print(f"ADVANCED ANALYTICAL TOOLS TEST SUMMARY")
    print(f"{'='*80}")
    print(f"Total Tools Tested: {len(results)}")
    print(f"Successful: {sum(1 for r in results if r['status'] == 'SUCCESS')}")
    print(f"Failed: {sum(1 for r in results if r['status'] == 'FAILED')}")
    print(f"Total Dialogue Turns: {summary['total_turns']}")
    print(f"Duration: {summary['duration_seconds']:.2f}s")
    print(f"Total Tokens: {summary['total_tokens']}")
    print(f"{'='*80}\n")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞĞ‘Ğ©Ğ˜Ğ™ Ğ¢Ğ•Ğ¡Ğ¢ Ğ’Ğ¡Ğ•Ğ¥ 19 Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢ĞĞ’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@pytest.mark.asyncio
async def test_all_mcp_tools_comprehensive():
    """
    ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ²ÑĞµÑ… 19 MCP Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    
    Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Cyclic Dialogue (Copilot â†” Perplexity)
    ĞÑ…Ğ²Ğ°Ñ‚: 100% Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² MCP ÑĞµÑ€Ğ²ĞµÑ€Ğ°
    """
    
    recorder = CyclicDialogueRecorder()
    recorder.start()
    
    all_tools = [
        # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ (7)
        ("get_project_structure", get_project_structure, {}),
        ("list_available_strategies", list_available_strategies, {}),
        ("get_supported_timeframes", get_supported_timeframes, {}),
        ("get_backtest_capabilities", get_backtest_capabilities, {}),
        ("check_system_status", check_system_status, {}),
        ("get_testing_summary", get_testing_summary, {}),
        ("explain_project_architecture", explain_project_architecture, {}),
        
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Perplexity (4)
        ("perplexity_search", perplexity_search, {
            "query": "Best practices for crypto trading bots",
            "model": "sonar"
        }),
        ("perplexity_analyze_crypto", perplexity_analyze_crypto, {
            "symbol": "ETH",
            "analysis_type": "fundamental"
        }),
        ("perplexity_strategy_research", perplexity_strategy_research, {
            "strategy_type": "mean-reversion",
            "market_conditions": "ranging"
        }),
        ("perplexity_market_news", perplexity_market_news, {
            "topic": "ethereum",
            "timeframe": "7d"
        }),
        
        # Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ (8)
        ("analyze_backtest_results", analyze_backtest_results, {
            "backtest_id": 42,
            "detailed": False
        }),
        ("compare_strategies", compare_strategies, {
            "strategy_a": "Grid Trading",
            "strategy_b": "DCA Bot",
            "market_type": "crypto"
        }),
        ("risk_management_advice", risk_management_advice, {
            "capital": 50000.0,
            "risk_per_trade": 1.5,
            "max_positions": 5
        }),
        ("technical_indicator_research", technical_indicator_research, {
            "indicator_name": "Bollinger Bands",
            "use_case": "breakout"
        }),
        ("explain_metric", explain_metric, {
            "metric_name": "Maximum Drawdown",
            "context": "risk_assessment"
        }),
        ("market_regime_detection", market_regime_detection, {
            "symbol": "ETHUSDT",
            "timeframe": "4h"
        }),
        ("code_review_strategy", code_review_strategy, {
            "strategy_code": "# Simple RSI strategy\nif rsi < 30: buy()",
            "language": "python"
        }),
        ("generate_test_scenarios", generate_test_scenarios, {
            "strategy_name": "MACD Divergence",
            "complexity": "basic"
        })
    ]
    
    results = []
    failed_tools = []
    
    for i, (tool_name, tool_func, tool_args) in enumerate(all_tools, 1):
        print(f"\n[{i}/{len(all_tools)}] Testing: {tool_name}")
        
        result = await cyclic_test_tool(tool_name, tool_func, tool_args, recorder)
        results.append(result)
        
        if result["status"] == "FAILED":
            failed_tools.append(tool_name)
            print(f"  âŒ FAILED: {result.get('error', 'Unknown error')}")
        else:
            print(f"  âœ… SUCCESS")
    
    recorder.end()
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
    output_dir = Path(__file__).parent.parent.parent / "results" / "mcp_tests"
    output_dir.mkdir(parents=True, exist_ok=True)
    recorder.save_to_file(output_dir / "all_tools_comprehensive_dialogue.json")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
    report = {
        "test_metadata": {
            "total_tools": len(all_tools),
            "test_date": datetime.now().isoformat(),
            "test_type": "comprehensive_cyclic_dialogue"
        },
        "summary": recorder.get_summary(),
        "results": results,
        "failed_tools": failed_tools,
        "coverage": {
            "context_tools": 7,
            "perplexity_basic": 4,
            "advanced_analytical": 8,
            "total_tested": len(results),
            "success_rate": f"{(len(results) - len(failed_tools)) / len(results) * 100:.1f}%"
        }
    }
    
    with open(output_dir / "comprehensive_test_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # ĞŸĞµÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ´ĞºĞ¸
    print(f"\n{'='*80}")
    print(f"COMPREHENSIVE MCP TOOLS TEST SUMMARY")
    print(f"{'='*80}")
    print(f"Total Tools Tested: {len(results)}")
    print(f"âœ… Successful: {len(results) - len(failed_tools)}")
    print(f"âŒ Failed: {len(failed_tools)}")
    if failed_tools:
        print(f"\nFailed Tools:")
        for tool in failed_tools:
            print(f"  - {tool}")
    print(f"\nDialogue Statistics:")
    print(f"  Total Turns: {report['summary']['total_turns']}")
    print(f"  Duration: {report['summary']['duration_seconds']:.2f}s")
    print(f"  Total Tokens: {report['summary']['total_tokens']}")
    print(f"  Avg Tokens/Tool: {report['summary']['total_tokens'] / len(results):.0f}")
    print(f"\nCoverage:")
    print(f"  Context Tools: 7/7 (100%)")
    print(f"  Perplexity Basic: 4/4 (100%)")
    print(f"  Advanced Analytical: 8/8 (100%)")
    print(f"  Overall Success Rate: {report['coverage']['success_rate']}")
    print(f"{'='*80}\n")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ 80% Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¸ Ñ‚ĞµÑÑ‚
    success_rate = (len(results) - len(failed_tools)) / len(results)
    assert success_rate >= 0.8, f"Too many failures: {success_rate*100:.1f}% success rate"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¡ĞŸĞ•Ğ¦Ğ˜ĞĞ›Ğ¬ĞĞ«Ğ• Ğ¢Ğ•Ğ¡Ğ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@pytest.mark.asyncio
async def test_perplexity_error_handling():
    """Ğ¢ĞµÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Perplexity Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…"""
    
    # Ğ¢ĞµÑÑ‚ Ñ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼
    result = await perplexity_search("", model="sonar")
    assert result["success"] == False, "Should fail with empty query"
    
    # Ğ¢ĞµÑÑ‚ Ñ Ğ½ĞµĞ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ
    result = await perplexity_search("test query", model="invalid_model")
    # Ğ”Ğ¾Ğ»Ğ¶ĞµĞ½ fallback Ğ½Ğ° sonar
    assert "error" in result or "answer" in result
    
    print("âœ… Error handling tests passed")


@pytest.mark.asyncio
async def test_tool_parameter_validation():
    """Ğ¢ĞµÑÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
    
    # Ğ¢ĞµÑÑ‚ risk_management_advice Ñ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»Ğ¾Ğ¼
    result = await risk_management_advice(capital=-1000.0)
    # Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Perplexity ÑƒĞºĞ°Ğ¶ĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ
    
    # Ğ¢ĞµÑÑ‚ analyze_backtest_results Ñ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ID
    result = await analyze_backtest_results(backtest_id=-1)
    
    print("âœ… Parameter validation tests passed")


if __name__ == "__main__":
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ"""
    
    print("Ğ—Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… MCP Ñ‚ĞµÑÑ‚Ğ¾Ğ²...")
    print("Ğ­Ñ‚Ğ¾ Ğ·Ğ°Ğ¹Ğ¼Ñ‘Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚ (Perplexity API Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹)\n")
    
    # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚
    asyncio.run(test_all_mcp_tools_comprehensive())
    
    print("\nâœ… Ğ’ÑĞµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ñ‹!")
    print("ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²: results/mcp_tests/")
