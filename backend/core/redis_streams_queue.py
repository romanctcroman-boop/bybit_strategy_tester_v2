"""
Redis Streams Queue Manager Ğ´Ğ»Ñ MCP Server
==========================================

Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹ Ğ½Ğ° Redis Streams Ñ:
- High/Low priority Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸
- Consumer Groups Ğ´Ğ»Ñ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
- XPENDING Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ "Ğ·Ğ°ÑÑ‚Ñ€ÑĞ²ÑˆĞ¸Ñ…" Ğ·Ğ°Ğ´Ğ°Ñ‡
- Checkpointing Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- Dead Letter Queue (DLQ) Ğ´Ğ»Ñ failed tasks
- Metrics integration Ğ´Ğ»Ñ monitoring

Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Client   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚ add_task(priority, payload)
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Priority Router   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ high_priority_streamâ”‚â”€â”€â”
    â”‚ low_priority_stream â”‚â”€â”€â”¤
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Consumer Groups            â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Worker-1 â”‚ Worker-2 â”‚ Worker-3â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ XPENDING Recoveryâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Author: DeepSeek Code Agent
Date: 2025-11-02
"""

import asyncio
import json
import logging
import time
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum

import redis.asyncio as aioredis
from redis.asyncio.client import Redis
from redis.exceptions import ConnectionError, TimeoutError as RedisTimeoutError

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TASK MODELS & ENUMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TaskPriority(str, Enum):
    """Task priority levels"""
    HIGH = "high"  # Reasoning, urgent coding
    LOW = "low"  # Background jobs, batch processing


class TaskStatus(str, Enum):
    """Task lifecycle states"""
    PENDING = "pending"  # In queue, not yet picked up
    PROCESSING = "processing"  # Currently being processed by worker
    COMPLETED = "completed"  # Successfully completed
    FAILED = "failed"  # Failed after all retries
    DEAD_LETTER = "dead_letter"  # Moved to DLQ


@dataclass
class Task:
    """
    Task representation for Redis Streams
    
    Attributes:
        task_id: Unique task identifier
        type: Task type (reasoning, codegen, ml, etc.)
        priority: Task priority (high/low)
        payload: Task data (serializable dict)
        created_at: Task creation timestamp
        worker_id: Worker currently processing the task
        retry_count: Number of retry attempts
        max_retries: Maximum allowed retries
        timeout: Task timeout in seconds
    """
    task_id: str
    type: str
    priority: TaskPriority
    payload: Dict[str, Any]
    created_at: float
    worker_id: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3
    timeout: int = 120  # seconds
    
    def to_redis_dict(self) -> Dict[str, str]:
        """Convert task to Redis-compatible string dict"""
        return {
            "task_id": self.task_id,
            "type": self.type,
            "priority": self.priority.value,
            "payload": json.dumps(self.payload),
            "created_at": str(self.created_at),
            "worker_id": self.worker_id or "",
            "retry_count": str(self.retry_count),
            "max_retries": str(self.max_retries),
            "timeout": str(self.timeout)
        }
    
    @classmethod
    def from_redis_dict(cls, data: Dict[bytes, bytes]) -> 'Task':
        """Parse task from Redis stream entry"""
        return cls(
            task_id=data[b"task_id"].decode(),
            type=data[b"type"].decode(),
            priority=TaskPriority(data[b"priority"].decode()),
            payload=json.loads(data[b"payload"].decode()),
            created_at=float(data[b"created_at"].decode()),
            worker_id=data[b"worker_id"].decode() or None,
            retry_count=int(data[b"retry_count"].decode()),
            max_retries=int(data[b"max_retries"].decode()),
            timeout=int(data[b"timeout"].decode())
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REDIS STREAMS QUEUE MANAGER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RedisStreamsQueueManager:
    """
    High-performance queue manager using Redis Streams
    
    Features:
        âœ… Priority-based task routing (high/low)
        âœ… Horizontal scaling via Consumer Groups
        âœ… Automatic recovery of stalled tasks (XPENDING)
        âœ… Checkpointing for long-running workflows
        âœ… Dead Letter Queue (DLQ) for persistent failures
        âœ… Metrics tracking (queue depth, processing time, etc.)
        âœ… Graceful shutdown with task preservation
    
    Redis Streams Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Redis Pub/Sub Ğ¸Ğ»Ğ¸ Lists, Ñ‚.Ğº.:
        - Message persistence (Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğµ Ñ‚ĞµÑ€ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑÑ‚Ğ°Ñ€Ñ‚Ğµ)
        - Consumer Groups (Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
        - Message acknowledgment (XACK)
        - XPENDING Ğ´Ğ»Ñ recovery
        - Ğ’ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ ID Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ (time-ordered)
    """
    
    # Stream names
    HIGH_PRIORITY_STREAM = "mcp:tasks:high"
    LOW_PRIORITY_STREAM = "mcp:tasks:low"
    CHECKPOINT_STREAM = "mcp:checkpoints"
    DLQ_STREAM = "mcp:tasks:dlq"
    
    # Consumer group names
    CONSUMER_GROUP = "mcp_workers"
    
    def __init__(
        self,
        redis_url: str = "redis://localhost:6379",
        max_stream_len: int = 100000,
        recovery_interval: int = 60,
        consumer_name: Optional[str] = None
    ):
        """
        Initialize Redis Streams Queue Manager
        
        Args:
            redis_url: Redis connection URL
            max_stream_len: Maximum stream length (old messages trimmed)
            recovery_interval: Interval for checking stalled tasks (seconds)
            consumer_name: Unique consumer identifier (auto-generated if None)
        """
        self.redis_url = redis_url
        self.max_stream_len = max_stream_len
        self.recovery_interval = recovery_interval
        self.consumer_name = consumer_name or f"worker-{int(time.time())}"
        
        self.redis: Optional[Redis] = None
        self._recovery_task: Optional[asyncio.Task] = None
        self._running = False
        
        # Metrics
        self.metrics = {
            "tasks_added": 0,
            "tasks_processed": 0,
            "tasks_failed": 0,
            "tasks_recovered": 0
        }
    
    async def connect(self):
        """Establish Redis connection and setup streams"""
        try:
            self.redis = await aioredis.from_url(
                self.redis_url,
                encoding="utf-8",
                decode_responses=False,
                max_connections=50
            )
            
            # Test connection
            await self.redis.ping()
            logger.info(f"[Redis] Connected to {self.redis_url}")
            
            # Initialize consumer groups (ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚)
            await self._ensure_consumer_groups()
            
            logger.info(f"[Redis] Consumer groups initialized for worker: {self.consumer_name}")
            
        except Exception as e:
            logger.error(f"[Redis] Connection failed: {e}")
            raise
    
    async def _ensure_consumer_groups(self):
        """Create consumer groups if they don't exist"""
        streams = [
            self.HIGH_PRIORITY_STREAM,
            self.LOW_PRIORITY_STREAM
        ]
        
        for stream in streams:
            try:
                # Try to create consumer group
                await self.redis.xgroup_create(
                    name=stream,
                    groupname=self.CONSUMER_GROUP,
                    id="0",  # Start from beginning
                    mkstream=True  # Create stream if not exists
                )
                logger.info(f"[Redis] Created consumer group '{self.CONSUMER_GROUP}' for stream '{stream}'")
                
            except Exception as e:
                # Group likely already exists
                if "BUSYGROUP" in str(e):
                    logger.debug(f"[Redis] Consumer group already exists for stream '{stream}'")
                else:
                    logger.warning(f"[Redis] Error creating consumer group for '{stream}': {e}")
    
    async def disconnect(self):
        """Close Redis connection"""
        if self.redis:
            await self.redis.close()
            logger.info("[Redis] Disconnected")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TASK MANAGEMENT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def add_task(
        self,
        task: Task,
        maxlen: Optional[int] = None
    ) -> str:
        """
        Add task to appropriate priority stream
        
        Args:
            task: Task object
            maxlen: Override max stream length (default: self.max_stream_len)
        
        Returns:
            Redis Stream message ID (format: '1234567890-0')
        """
        if not self.redis:
            raise RuntimeError("Redis not connected. Call connect() first.")
        
        # Select stream based on priority
        stream = (
            self.HIGH_PRIORITY_STREAM
            if task.priority == TaskPriority.HIGH
            else self.LOW_PRIORITY_STREAM
        )
        
        # Add to stream with automatic trimming
        message_id = await self.redis.xadd(
            name=stream,
            fields=task.to_redis_dict(),
            maxlen=maxlen or self.max_stream_len,
            approximate=True  # ~MAXLEN Ğ´Ğ»Ñ performance
        )
        
        self.metrics["tasks_added"] += 1
        
        logger.info(
            f"[Redis] Added task {task.task_id} to {stream} "
            f"(priority: {task.priority.value}, type: {task.type})"
        )
        
        return message_id.decode()
    
    async def read_tasks(
        self,
        count: int = 10,
        block: int = 5000,
        priority: Optional[TaskPriority] = None
    ) -> List[Tuple[str, Task]]:
        """
        Read tasks from streams using consumer group
        
        Args:
            count: Maximum number of tasks to read
            block: Block timeout in milliseconds (0 = non-blocking)
            priority: Read from specific priority stream (None = read both)
        
        Returns:
            List of (message_id, Task) tuples
        """
        if not self.redis:
            raise RuntimeError("Redis not connected")
        
        # Determine streams to read from
        if priority == TaskPriority.HIGH:
            streams = {self.HIGH_PRIORITY_STREAM: ">"}
        elif priority == TaskPriority.LOW:
            streams = {self.LOW_PRIORITY_STREAM: ">"}
        else:
            # Read from both (high priority first)
            streams = {
                self.HIGH_PRIORITY_STREAM: ">",
                self.LOW_PRIORITY_STREAM: ">"
            }
        
        try:
            # XREADGROUP: Read unacknowledged messages for this consumer
            result = await self.redis.xreadgroup(
                groupname=self.CONSUMER_GROUP,
                consumername=self.consumer_name,
                streams=streams,
                count=count,
                block=block
            )
            
            tasks = []
            for stream_name, messages in result:
                for message_id, data in messages:
                    try:
                        task = Task.from_redis_dict(data)
                        task.worker_id = self.consumer_name
                        tasks.append((message_id.decode(), task))
                    except Exception as e:
                        logger.error(f"[Redis] Failed to parse task from {message_id}: {e}")
            
            if tasks:
                logger.info(f"[Redis] Read {len(tasks)} tasks for worker {self.consumer_name}")
            
            return tasks
            
        except Exception as e:
            logger.error(f"[Redis] Error reading tasks: {e}")
            return []
    
    async def acknowledge_task(self, stream: str, message_id: str):
        """
        Acknowledge task completion (removes from pending list)
        
        Args:
            stream: Stream name
            message_id: Redis Stream message ID
        """
        if not self.redis:
            return
        
        try:
            await self.redis.xack(
                stream,
                self.CONSUMER_GROUP,
                message_id
            )
            self.metrics["tasks_processed"] += 1
            logger.debug(f"[Redis] Acknowledged task {message_id} from {stream}")
            
        except Exception as e:
            logger.error(f"[Redis] Failed to acknowledge {message_id}: {e}")
    
    async def move_to_dlq(self, task: Task, error_message: str):
        """
        Move failed task to Dead Letter Queue
        
        Args:
            task: Failed task
            error_message: Failure reason
        """
        if not self.redis:
            return
        
        dlq_data = task.to_redis_dict()
        dlq_data["error"] = error_message
        dlq_data["failed_at"] = str(time.time())
        
        await self.redis.xadd(
            name=self.DLQ_STREAM,
            fields=dlq_data,
            maxlen=10000
        )
        
        self.metrics["tasks_failed"] += 1
        logger.warning(f"[Redis] Task {task.task_id} moved to DLQ: {error_message}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHECKPOINTING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def save_checkpoint(
        self,
        task_id: str,
        step: str,
        data: Dict[str, Any]
    ):
        """
        Save workflow checkpoint Ğ´Ğ»Ñ long-running tasks
        
        Args:
            task_id: Task identifier
            step: Workflow step name (e.g., "reasoning", "codegen")
            data: Checkpoint data to persist
        """
        if not self.redis:
            return
        
        checkpoint_data = {
            "task_id": task_id,
            "step": step,
            "data": json.dumps(data),
            "timestamp": str(time.time())
        }
        
        await self.redis.xadd(
            name=self.CHECKPOINT_STREAM,
            fields=checkpoint_data,
            maxlen=50000
        )
        
        logger.info(f"[Redis] Checkpoint saved: task={task_id}, step={step}")
    
    async def get_checkpoints(self, task_id: str) -> List[Dict[str, Any]]:
        """
        Retrieve all checkpoints for a task
        
        Args:
            task_id: Task identifier
        
        Returns:
            List of checkpoint data dicts
        """
        if not self.redis:
            return []
        
        # Read entire checkpoint stream and filter by task_id
        # (Ğ² production Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ secondary index Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ keys)
        result = await self.redis.xread(
            {self.CHECKPOINT_STREAM: "0-0"},
            count=1000
        )
        
        checkpoints = []
        for stream_name, messages in result:
            for message_id, data in messages:
                if data[b"task_id"].decode() == task_id:
                    checkpoints.append({
                        "step": data[b"step"].decode(),
                        "data": json.loads(data[b"data"].decode()),
                        "timestamp": float(data[b"timestamp"].decode())
                    })
        
        return checkpoints
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # XPENDING RECOVERY (Auto-recovery of stalled tasks)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def start_recovery_monitor(self):
        """
        Start background task for monitoring and recovering stalled tasks
        
        ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ XPENDING ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ recovery_interval ÑĞµĞºÑƒĞ½Ğ´ Ğ¸
        Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ "Ğ·Ğ°ÑÑ‚Ñ€ÑĞ²ÑˆĞ¸Ğµ" Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ°Ğ¼
        """
        self._running = True
        self._recovery_task = asyncio.create_task(self._recovery_loop())
        logger.info(f"[Redis] Started recovery monitor (interval: {self.recovery_interval}s)")
    
    async def stop_recovery_monitor(self):
        """Stop recovery monitor"""
        self._running = False
        if self._recovery_task:
            self._recovery_task.cancel()
            try:
                await self._recovery_task
            except asyncio.CancelledError:
                pass
        logger.info("[Redis] Stopped recovery monitor")
    
    async def _recovery_loop(self):
        """Background loop for checking stalled tasks"""
        while self._running:
            try:
                await asyncio.sleep(self.recovery_interval)
                await self._recover_stalled_tasks()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[Redis] Recovery loop error: {e}")
    
    async def _recover_stalled_tasks(self):
        """
        Check XPENDING and recover stalled tasks
        
        Tasks ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ "Ğ·Ğ°ÑÑ‚Ñ€ÑĞ²ÑˆĞµĞ¹" ĞµÑĞ»Ğ¸:
            - Idle time > task.timeout
            - Delivery count > task.max_retries â†’ move to DLQ
            - Ğ˜Ğ½Ğ°Ñ‡Ğµ â†’ XCLAIM Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ°
        """
        if not self.redis:
            return
        
        streams = [self.HIGH_PRIORITY_STREAM, self.LOW_PRIORITY_STREAM]
        
        for stream in streams:
            try:
                # Get pending messages summary
                pending_info = await self.redis.xpending(
                    stream,
                    self.CONSUMER_GROUP
                )
                
                if not pending_info or pending_info[0] == 0:
                    continue
                
                # Get detailed pending list
                pending_messages = await self.redis.xpending_range(
                    stream,
                    self.CONSUMER_GROUP,
                    min="-",
                    max="+",
                    count=100
                )
                
                for msg in pending_messages:
                    message_id = msg["message_id"].decode()
                    consumer = msg["consumer"].decode()
                    idle_time = msg["time_since_delivered"]  # milliseconds
                    delivery_count = msg["times_delivered"]
                    
                    # Fetch full message data
                    message_data = await self.redis.xrange(
                        stream,
                        min=message_id,
                        max=message_id,
                        count=1
                    )
                    
                    if not message_data:
                        continue
                    
                    _, data = message_data[0]
                    task = Task.from_redis_dict(data)
                    
                    # Check if stalled (idle > timeout)
                    if idle_time > task.timeout * 1000:
                        if delivery_count >= task.max_retries:
                            # Max retries exceeded â†’ DLQ
                            await self.move_to_dlq(
                                task,
                                f"Max retries exceeded ({delivery_count})"
                            )
                            await self.acknowledge_task(stream, message_id)
                            logger.warning(
                                f"[Redis] Task {task.task_id} moved to DLQ after {delivery_count} attempts"
                            )
                        else:
                            # Claim for re-processing by another worker
                            await self.redis.xclaim(
                                stream,
                                self.CONSUMER_GROUP,
                                self.consumer_name,
                                min_idle_time=task.timeout * 1000,
                                message_ids=[message_id]
                            )
                            self.metrics["tasks_recovered"] += 1
                            logger.info(
                                f"[Redis] Claimed stalled task {task.task_id} "
                                f"(idle: {idle_time}ms, attempt: {delivery_count})"
                            )
                
            except Exception as e:
                logger.error(f"[Redis] Error recovering tasks from {stream}: {e}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # METRICS & MONITORING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def get_queue_depth(self, priority: Optional[TaskPriority] = None) -> Dict[str, int]:
        """
        Get current queue depth (number of pending tasks)
        
        Args:
            priority: Filter by priority (None = all queues)
        
        Returns:
            Dict with queue depths: {"high": 42, "low": 15}
        """
        if not self.redis:
            return {"high": 0, "low": 0}
        
        depths = {}
        
        streams = (
            [self.HIGH_PRIORITY_STREAM]
            if priority == TaskPriority.HIGH
            else [self.LOW_PRIORITY_STREAM]
            if priority == TaskPriority.LOW
            else [self.HIGH_PRIORITY_STREAM, self.LOW_PRIORITY_STREAM]
        )
        
        for stream in streams:
            try:
                pending_info = await self.redis.xpending(
                    stream,
                    self.CONSUMER_GROUP
                )
                count = pending_info[0] if pending_info else 0
                
                key = "high" if "high" in stream else "low"
                depths[key] = count
                
            except Exception as e:
                logger.error(f"[Redis] Error getting queue depth for {stream}: {e}")
                key = "high" if "high" in stream else "low"
                depths[key] = 0
        
        return depths
    
    async def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive queue metrics"""
        queue_depths = await self.get_queue_depth()
        
        return {
            "consumer_name": self.consumer_name,
            "queue_depth": queue_depths,
            "tasks_added": self.metrics["tasks_added"],
            "tasks_processed": self.metrics["tasks_processed"],
            "tasks_failed": self.metrics["tasks_failed"],
            "tasks_recovered": self.metrics["tasks_recovered"],
            "recovery_interval": self.recovery_interval,
            "timestamp": datetime.now().isoformat()
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# USAGE EXAMPLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    """Example usage of RedisStreamsQueueManager"""
    
    # Initialize manager
    manager = RedisStreamsQueueManager(
        redis_url="redis://localhost:6379",
        consumer_name="worker-example"
    )
    
    try:
        # Connect to Redis
        await manager.connect()
        
        # Start recovery monitor
        await manager.start_recovery_monitor()
        
        # Add some tasks
        high_priority_task = Task(
            task_id="task-123",
            type="reasoning",
            priority=TaskPriority.HIGH,
            payload={"prompt": "Analyze trading strategy"},
            created_at=time.time(),
            timeout=60
        )
        
        low_priority_task = Task(
            task_id="task-456",
            type="batch-optimization",
            priority=TaskPriority.LOW,
            payload={"strategy_ids": [1, 2, 3]},
            created_at=time.time(),
            timeout=300
        )
        
        msg_id_1 = await manager.add_task(high_priority_task)
        msg_id_2 = await manager.add_task(low_priority_task)
        
        print(f"âœ… Added tasks: {msg_id_1}, {msg_id_2}")
        
        # Read tasks (consumer)
        tasks = await manager.read_tasks(count=10, block=1000)
        print(f"ğŸ“¥ Read {len(tasks)} tasks")
        
        # Process and acknowledge
        for message_id, task in tasks:
            print(f"Processing task {task.task_id} (type: {task.type})")
            
            # Save checkpoint (for long-running workflows)
            await manager.save_checkpoint(
                task.task_id,
                step="reasoning",
                data={"progress": 50, "intermediate_result": "..."}
            )
            
            # Simulate processing
            await asyncio.sleep(0.1)
            
            # Acknowledge completion
            stream = (
                manager.HIGH_PRIORITY_STREAM
                if task.priority == TaskPriority.HIGH
                else manager.LOW_PRIORITY_STREAM
            )
            await manager.acknowledge_task(stream, message_id)
        
        # Get metrics
        metrics = await manager.get_metrics()
        print(f"ğŸ“Š Metrics: {metrics}")
        
        # Keep running for recovery monitor
        await asyncio.sleep(5)
        
    finally:
        # Cleanup
        await manager.stop_recovery_monitor()
        await manager.disconnect()
        print("âœ… Cleanup completed")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
