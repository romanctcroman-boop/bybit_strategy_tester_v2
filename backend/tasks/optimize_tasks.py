"""
Optimization Tasks

Celery –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (grid search, walk-forward, Bayesian).
"""

from datetime import UTC, datetime
from itertools import product
from typing import Any

from celery import Task
from loguru import logger

from backend.celery_app import celery_app
from backend.core.engine_adapter import get_engine
from backend.database import Optimization, SessionLocal
from backend.services.data_service import DataService


class OptimizationTask(Task):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""

    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫"""
        logger.error(f"‚ùå Optimization task {task_id} failed: {exc}")

        optimization_id = kwargs.get("optimization_id") or (args[0] if args else None)
        if optimization_id:
            try:
                db = SessionLocal()
                opt = db.query(Optimization).filter(Optimization.id == optimization_id).first()
                if opt:
                    opt.status = "failed"
                    opt.error_message = str(exc)
                    opt.updated_at = datetime.now(UTC)
                    db.commit()
                db.close()
            except Exception as e:
                logger.error(f"Failed to update optimization status: {e}")

    def on_success(self, retval, task_id, args, kwargs):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        logger.info(f"‚úÖ Optimization task {task_id} completed")


def _parse_dt(value: Any) -> datetime:
    """Parse ISO-like datetime strings to datetime; pass through if already datetime.

    Falls back to naive fromisoformat when possible.
    """
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        try:
            # Support 'YYYY-MM-DD' or full ISO strings
            if len(value) == 10:
                return datetime.fromisoformat(value)
            return datetime.fromisoformat(value)
        except Exception:
            # Last resort: try common format
            try:
                return datetime.strptime(value[:19], "%Y-%m-%dT%H:%M:%S")
            except Exception:
                pass
    raise ValueError(f"Invalid datetime value: {value!r}")


@celery_app.task(
    bind=True, base=OptimizationTask, name="backend.tasks.optimize_tasks.grid_search", max_retries=2
)
def grid_search_task(
    self,
    optimization_id: int,
    strategy_config: dict[str, Any],
    param_space: dict[str, list],
    symbol: str,
    interval: str,
    start_date: str,
    end_date: str,
    metric: str = "sharpe_ratio",
) -> dict[str, Any]:
    """
    Grid Search –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

    Args:
        optimization_id: ID –∑–∞–ø–∏—Å–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        strategy_config: –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        param_space: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ {param_name: [value1, value2, ...]}
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
        start_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞
        end_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (sharpe_ratio, total_return, profit_factor)

    Returns:
        –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    logger.info(f"üîç Starting grid search: optimization {optimization_id}")
    logger.info(f"   Param space: {param_space}")

    db = SessionLocal()
    data_service = DataService(db)

    try:
        # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å —á–µ—Ä–µ–∑ DataService
        opt = data_service.get_optimization(optimization_id)
        if not opt:
            raise ValueError(f"Optimization {optimization_id} not found")

        data_service.update_optimization(
            optimization_id, status="running", started_at=datetime.now(UTC)
        )

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        logger.info("üì• Loading market data...")
        data_service = DataService(db)
        start_dt = _parse_dt(start_date)
        end_dt = _parse_dt(end_date)
        candles = data_service.get_market_data(
            symbol=symbol,
            timeframe=interval,
            start_time=start_dt,
            end_time=end_dt,
        )
        if not candles or len(candles) == 0:
            raise ValueError(f"No data for {symbol} {interval}")

        logger.info(f"üìä Loaded {len(candles)} candles")

        # Normalize ORM objects to a list[dict] the engine can consume
        try:

            def _to_row(x):
                if isinstance(x, dict):
                    return x
                # hasattr path for ORM model attributes
                row = {}
                for key in ("timestamp", "open", "high", "low", "close", "volume", "quote_volume"):
                    if hasattr(x, key):
                        row[key] = getattr(x, key)
                return row

            norm_candles = [_to_row(c) for c in candles]
        except Exception:
            norm_candles = candles  # fallback

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_names = list(param_space.keys())
        param_values = list(param_space.values())
        combinations = list(product(*param_values))

        total_combinations = len(combinations)
        logger.info(f"üî¢ Testing {total_combinations} parameter combinations")

        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        self.update_state(state="PROGRESS", meta={"current": 0, "total": total_combinations})

        # –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        results = []
        best_score = float("-inf")
        best_params = None
        best_result = None

        engine = get_engine(
            None, initial_capital=10000.0, commission=0.0006, data_service=data_service
        )

        for idx, params in enumerate(combinations, 1):
            # –û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            test_config = strategy_config.copy()
            for param_name, param_value in zip(param_names, params, strict=True):
                test_config[param_name] = param_value

            # –ó–∞–ø—É—Å—Ç–∏—Ç—å –±—ç–∫—Ç–µ—Å—Ç
            try:
                result = engine.run(data=norm_candles, strategy_config=test_config)
                score = result.get(metric, 0)

                results.append(
                    {
                        "params": dict(zip(param_names, params, strict=True)),
                        "score": score,
                        "metrics": {
                            "total_return": result.get("total_return"),
                            "sharpe_ratio": result.get("sharpe_ratio"),
                            "max_drawdown": result.get("max_drawdown"),
                            "win_rate": result.get("win_rate"),
                            "total_trades": result.get("total_trades"),
                        },
                    }
                )

                # –û–±–Ω–æ–≤–∏—Ç—å –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if score > best_score:
                    best_score = score
                    best_params = dict(zip(param_names, params, strict=True))
                    best_result = result

                # –û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
                if idx % 10 == 0 or idx == total_combinations:
                    logger.info(
                        f"Progress: {idx}/{total_combinations} ({idx / total_combinations * 100:.1f}%)"
                    )
                    self.update_state(
                        state="PROGRESS",
                        meta={
                            "current": idx,
                            "total": total_combinations,
                            "best_score": best_score,
                            "best_params": best_params,
                        },
                    )

            except Exception as e:
                logger.warning(f"Backtest failed for params {params}: {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ç—Ä–∏–∫–µ
        results.sort(key=lambda x: x["score"], reverse=True)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("üíæ Saving optimization results...")
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ DataService
        data_service.update_optimization(
            optimization_id,
            status="completed",
            completed_at=datetime.now(UTC),
            best_params=best_params,
            best_score=best_score,
            results={
                "method": "grid_search",
                "metric": metric,
                "symbol": symbol,
                "interval": interval,
                "period": {"start": start_dt.isoformat(), "end": end_dt.isoformat()},
                "total_combinations": total_combinations,
                "best_params": best_params,
                "best_score": best_score,
                "best_metrics": best_result,
                "top_10": results[:10],
            },
        )

        logger.info("‚úÖ Grid search completed")
        logger.info(f"   Best {metric}: {best_score:.4f}")
        logger.info(f"   Best params: {best_params}")

        return {
            "optimization_id": optimization_id,
            "status": "completed",
            "best_params": best_params,
            "best_score": best_score,
            "total_combinations": total_combinations,
        }

    except Exception as e:
        logger.error(f"‚ùå Grid search failed: {e}")

        try:
            data_service.update_optimization(
                optimization_id,
                status="failed",
                error_message=str(e),
                completed_at=datetime.now(UTC),
            )
        except Exception as db_error:
            logger.error(f"Failed to update optimization status: {db_error}")

        if self.request.retries < self.max_retries:
            raise self.retry(exc=e)

        raise

    finally:
        db.close()


@celery_app.task(bind=True, base=OptimizationTask, name="backend.tasks.optimize_tasks.walk_forward")
def walk_forward_task(
    self,
    optimization_id: int,
    strategy_config: dict[str, Any],
    param_space: dict[str, list],
    symbol: str,
    interval: str,
    start_date: str,
    end_date: str,
    train_size: int = 120,  # Training days (IS window)
    test_size: int = 60,  # Testing days (OOS window)
    step_size: int = 30,  # Step size for rolling window
    metric: str = "sharpe_ratio",  # Optimization metric
) -> dict[str, Any]:
    """
    Walk-Forward –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

    –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø–µ—Ä–∏–æ–¥—ã train/test, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –Ω–∞ train,
    —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–∞ test, –∑–∞—Ç–µ–º —Å–¥–≤–∏–≥–∞–µ—Ç –æ–∫–Ω–æ.

    Args:
        optimization_id: ID –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        strategy_config: –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        param_space: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
        start_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞
        end_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        train_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –æ–±—É—á–µ–Ω–∏—è (–≤ –¥–Ω—è—Ö, IS window)
        test_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ –¥–Ω—è—Ö, OOS window)
        step_size: –®–∞–≥ —Å–¥–≤–∏–≥–∞ –æ–∫–Ω–∞ (–≤ –¥–Ω—è—Ö)
        metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã walk-forward
    """
    import asyncio
    from datetime import datetime

    from backend.core.walkforward import WalkForwardAnalyzer
    from backend.services.data_service import DataService

    logger.info(f"üö∂ Starting walk-forward optimization: {optimization_id}")
    logger.info(f"   Symbol: {symbol}, Interval: {interval}")
    logger.info(f"   Train: {train_size}d, Test: {test_size}d, Step: {step_size}d")
    logger.info(f"   Metric: {metric}")

    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "loading_data",
                "progress": 0,
            },
        )

        ds = DataService()
        start_dt = _parse_dt(start_date)
        end_dt = _parse_dt(end_date)
        data = ds.get_market_data(
            symbol=symbol,
            timeframe=interval,
            start_time=start_dt,
            end_time=end_dt,
        )
        if data is None or len(data) == 0:
            raise ValueError(f"No data available for {symbol} {interval}")
        logger.info(f"üìä Loaded {len(data)} candles")

        # 2. –°–æ–∑–¥–∞—ë–º Walk-Forward Analyzer
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "initializing",
                "progress": 10,
            },
        )

        analyzer = WalkForwardAnalyzer(
            data=data,
            initial_capital=strategy_config.get("initial_capital", 10000.0),
            commission=strategy_config.get("commission", 0.001),
            is_window_days=train_size,
            oos_window_days=test_size,
            step_days=step_size,
        )

        num_windows = len(analyzer.windows)
        logger.info(f"ü™ü Created {num_windows} windows for analysis")

        # 3. –ó–∞–ø—É—Å–∫–∞–µ–º Walk-Forward –∞–Ω–∞–ª–∏–∑
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "optimizing",
                "progress": 20,
                "num_windows": num_windows,
            },
        )

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –≤ event loop (py3.10+ and 3.14-safe)
        def _get_loop():
            try:
                return asyncio.get_running_loop()
            except RuntimeError:
                try:
                    loop_existing = asyncio.get_event_loop()
                except RuntimeError:
                    loop_existing = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop_existing)
                if loop_existing.is_closed():
                    loop_existing = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop_existing)
                return loop_existing

        loop = _get_loop()

        results = loop.run_until_complete(
            analyzer.run_async(
                strategy_config=strategy_config,
                param_space=param_space,
                metric=metric,
            )
        )

        # Persist results via DataService
        with DataService() as _ds:
            _ds.update_optimization(
                optimization_id,
                status="completed",
                completed_at=datetime.now(UTC),
                results={
                    "method": "walk_forward",
                    "metric": metric,
                    "symbol": symbol,
                    "interval": interval,
                    "period": {"start": start_dt.isoformat(), "end": end_dt.isoformat()},
                    "config": {
                        "train_size": train_size,
                        "test_size": test_size,
                        "step_size": step_size,
                    },
                    "results": results,
                },
            )

        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "completed",
                "progress": 100,
            },
        )

        logger.success(
            f"‚úÖ Walk-forward completed: {optimization_id}, "
            f"processed {len(results['windows'])}/{num_windows} windows"
        )

        return {
            "optimization_id": optimization_id,
            "method": "walk_forward",
            "symbol": symbol,
            "interval": interval,
            "start_date": start_date,
            "end_date": end_date,
            "config": {
                "train_size": train_size,
                "test_size": test_size,
                "step_size": step_size,
                "metric": metric,
            },
            "results": results,
            "status": "completed",
            "completed_at": datetime.now(UTC).isoformat(),
        }

    except Exception as e:
        logger.error(f"‚ùå Walk-forward failed: {optimization_id}, error: {e}")
        self.update_state(
            state="FAILURE",
            meta={
                "optimization_id": optimization_id,
                "error": str(e),
            },
        )
        raise


@celery_app.task(
    bind=True, base=OptimizationTask, name="backend.tasks.optimize_tasks.bayesian_optimization"
)
def bayesian_optimization_task(
    self,
    optimization_id: int,
    strategy_config: dict[str, Any],
    param_space: dict[str, dict[str, Any]],  # {param: {type, low, high}}
    symbol: str,
    interval: str,
    start_date: str,
    end_date: str,
    n_trials: int = 100,
    metric: str = "sharpe_ratio",
    direction: str = "maximize",
    n_jobs: int = 1,
    random_state: int | None = None,
) -> dict[str, Any]:
    """
    Bayesian Optimization –∏—Å–ø–æ–ª—å–∑—É—è Optuna

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Tree-structured Parzen Estimator (TPE) –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ Grid Search.

    Args:
        optimization_id: ID –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        strategy_config: –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        param_space: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            –§–æ—Ä–º–∞—Ç: {param_name: {type: 'int'|'float'|'categorical', low: X, high: Y}}
            –ü—Ä–∏–º–µ—Ä: {'period': {'type': 'int', 'low': 10, 'high': 100}}
        symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
        start_date: –î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞
        end_date: –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        n_trials: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        direction: 'maximize' –∏–ª–∏ 'minimize'
        n_jobs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Bayesian optimization
    """
    import asyncio
    from datetime import datetime

    from backend.core.bayesian import BayesianOptimizer
    from backend.services.data_service import DataService

    logger.info(f"üß† Starting Bayesian optimization: {optimization_id}")
    logger.info(f"   Symbol: {symbol}, Interval: {interval}")
    logger.info(f"   Trials: {n_trials}, Metric: {metric}, Direction: {direction}")

    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "loading_data",
                "progress": 0,
            },
        )

        ds = DataService()
        start_dt = _parse_dt(start_date)
        end_dt = _parse_dt(end_date)
        data = ds.get_market_data(
            symbol=symbol,
            timeframe=interval,
            start_time=start_dt,
            end_time=end_dt,
        )
        if data is None or len(data) == 0:
            raise ValueError(f"No data available for {symbol} {interval}")
        logger.info(f"üìä Loaded {len(data)} candles")

        # 2. –°–æ–∑–¥–∞—ë–º Bayesian Optimizer
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "initializing",
                "progress": 10,
            },
        )

        optimizer = BayesianOptimizer(
            data=data,
            initial_capital=strategy_config.get("initial_capital", 10000.0),
            commission=strategy_config.get("commission", 0.001),
            n_trials=n_trials,
            n_jobs=n_jobs,
            random_state=random_state,
        )

        logger.info(f"üéØ Initialized Bayesian Optimizer with {n_trials} trials")

        # 3. –ó–∞–ø—É—Å–∫–∞–µ–º Bayesian optimization
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "optimizing",
                "progress": 20,
                "n_trials": n_trials,
            },
        )

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –≤ event loop (py3.10+ and 3.14-safe)
        def _get_loop():
            try:
                return asyncio.get_running_loop()
            except RuntimeError:
                try:
                    loop_existing = asyncio.get_event_loop()
                except RuntimeError:
                    loop_existing = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop_existing)
                if loop_existing.is_closed():
                    loop_existing = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop_existing)
                return loop_existing

        loop = _get_loop()

        results = loop.run_until_complete(
            optimizer.optimize_async(
                strategy_config=strategy_config,
                param_space=param_space,
                metric=metric,
                direction=direction,
                show_progress=False,  # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º progress bar –≤ Celery
            )
        )

        # 4. –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        try:
            param_importance = optimizer.get_importance()
            results["param_importance"] = param_importance
        except Exception as e:
            logger.warning(f"Could not compute param importance: {e}")
            results["param_importance"] = {}

        # 5. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.update_state(
            state="PROGRESS",
            meta={
                "optimization_id": optimization_id,
                "status": "completed",
                "progress": 100,
            },
        )

        logger.success(
            f"‚úÖ Bayesian optimization completed: {optimization_id}, "
            f"best {metric}={results['best_value']:.4f}, "
            f"completed trials={results['statistics']['completed_trials']}/{n_trials}"
        )

        payload = {
            "optimization_id": optimization_id,
            "method": "bayesian",
            "symbol": symbol,
            "interval": interval,
            "start_date": start_date,
            "end_date": end_date,
            "config": {
                "n_trials": n_trials,
                "metric": metric,
                "direction": direction,
                "n_jobs": n_jobs,
            },
            "results": results,
            "status": "completed",
            "completed_at": datetime.now(UTC).isoformat(),
        }

        # Persist via DataService
        with DataService() as _ds:
            _ds.update_optimization(
                optimization_id,
                status="completed",
                completed_at=datetime.now(UTC),
                results={
                    "method": "bayesian",
                    "metric": metric,
                    "symbol": symbol,
                    "interval": interval,
                    "period": {"start": start_dt.isoformat(), "end": end_dt.isoformat()},
                    "config": payload["config"],
                    "results": results,
                },
            )

        return payload

    except Exception as e:
        logger.error(f"‚ùå Bayesian optimization failed: {optimization_id}, error: {e}")
        self.update_state(
            state="FAILURE",
            meta={
                "optimization_id": optimization_id,
                "error": str(e),
            },
        )
        raise
