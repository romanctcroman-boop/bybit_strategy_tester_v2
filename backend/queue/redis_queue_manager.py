"""
Redis Streams Queue Manager
–ó–∞–º–µ–Ω–∞ Celery –¥–ª—è –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á —Å SLA-–≥–∞—Ä–∞–Ω—Ç–∏—è–º–∏
"""

import asyncio
import json
import time
import uuid
from collections.abc import Callable
from dataclasses import asdict, dataclass
from enum import Enum
from typing import Any

try:
    import redis.asyncio as aioredis
except ImportError:
    # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π redis
    try:
        import aioredis
    except ImportError:
        raise ImportError("Install redis with: pip install redis>=5.0.0")

from loguru import logger


class TaskStatus(str, Enum):
    """–°—Ç–∞—Ç—É—Å—ã –∑–∞–¥–∞—á"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    RETRYING = "retrying"


class TaskPriority(int, Enum):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–¥–∞—á"""
    LOW = 1
    NORMAL = 5
    HIGH = 10
    CRITICAL = 20


@dataclass
class Task:
    """–ú–æ–¥–µ–ª—å –∑–∞–¥–∞—á–∏"""
    task_id: str
    task_type: str  # 'backtest', 'optimization', 'data_fetch'
    payload: dict[str, Any]
    priority: int = TaskPriority.NORMAL.value
    max_retries: int = 3
    retry_count: int = 0
    timeout_seconds: int = 3600  # 1 —á–∞—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    created_at: float = None
    started_at: float | None = None
    completed_at: float | None = None
    status: str = TaskStatus.PENDING.value
    error_message: str | None = None
    result: dict[str, Any] | None = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
    
    def to_dict(self) -> dict[str, Any]:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ dict –¥–ª—è Redis"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> 'Task':
        """–î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ dict"""
        return cls(**data)


class RedisQueueManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –æ—á–µ—Ä–µ–¥–µ–π –Ω–∞ Redis Streams
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - Consumer Groups –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry —Å exponential backoff
    - Dead Letter Queue –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–¥–∞—á
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ (Prometheus-ready)
    - Graceful shutdown
    """
    
    def __init__(
        self,
        redis_url: str = "redis://localhost:6379/0",
        stream_name: str = "bybit:tasks",
        consumer_group: str = "workers",
        consumer_name: str | None = None,
        max_pending_tasks: int = 1000,
        batch_size: int = 10,
    ):
        self.redis_url = redis_url
        self.stream_name = stream_name
        self.consumer_group = consumer_group
        self.consumer_name = consumer_name or f"worker-{uuid.uuid4().hex[:8]}"
        self.max_pending_tasks = max_pending_tasks
        self.batch_size = batch_size
        
        self._redis: aioredis.Redis | None = None
        self._handlers: dict[str, Callable] = {}
        self._running = False
        self._shutdown_event = asyncio.Event()
        
        # –ú–µ—Ç—Ä–∏–∫–∏ - —Ç–µ–ø–µ—Ä—å –≤ Redis Hash –¥–ª—è multi-process sync
        self.metrics_key = f"{stream_name}:metrics"
    
    async def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis"""
        try:
            self._redis = await aioredis.from_url(
                self.redis_url,
                encoding="utf-8",
                decode_responses=True
            )
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            await self._redis.ping()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Redis Hash metrics
            await self._init_metrics()
            
            # –°–æ–∑–¥–∞—Ç—å Consumer Group –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            try:
                await self._redis.xgroup_create(
                    name=self.stream_name,
                    groupname=self.consumer_group,
                    id="0",
                    mkstream=True
                )
                logger.info(f"‚úÖ Created consumer group: {self.consumer_group}")
            except Exception as e:
                if "BUSYGROUP" in str(e):
                    logger.info(f"Consumer group already exists: {self.consumer_group}")
                else:
                    raise
                    
            logger.info(f"‚úÖ Connected to Redis: {self.redis_url}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to Redis: {e}")
            raise
    
    async def disconnect(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç Redis"""
        if self._redis:
            await self._redis.close()
            self._redis = None
            logger.info("‚úÖ Disconnected from Redis")
    
    async def _init_metrics(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è metrics –≤ Redis Hash"""
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–π, –µ—Å–ª–∏ –Ω–µ—Ç - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å 0
        for metric in ["tasks_submitted", "tasks_completed", "tasks_failed", "tasks_timeout"]:
            exists = await self._redis.hexists(self.metrics_key, metric)
            if not exists:
                await self._redis.hset(self.metrics_key, metric, 0)
        logger.debug(f"‚úÖ Metrics initialized in Redis Hash: {self.metrics_key}")
    
    def register_handler(self, task_type: str, handler: Callable):
        """
        –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        
        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ ('backtest', 'optimization')
            handler: Async —Ñ—É–Ω–∫—Ü–∏—è-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        """
        self._handlers[task_type] = handler
        logger.info(f"üìù Registered handler for task_type: {task_type}")
    
    async def submit_task(
        self,
        task_type: str,
        payload: dict[str, Any],
        priority: int = TaskPriority.NORMAL.value,
        max_retries: int = 3,
        timeout_seconds: int = 3600,
    ) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å
        
        Returns:
            task_id: ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏
        """
        task = Task(
            task_id=str(uuid.uuid4()),
            task_type=task_type,
            payload=payload,
            priority=priority,
            max_retries=max_retries,
            timeout_seconds=timeout_seconds,
        )
        
        # –î–æ–±–∞–≤–∏—Ç—å –≤ Redis Stream
        await self._redis.xadd(
            name=self.stream_name,
            fields={"data": json.dumps(task.to_dict())},
            maxlen=self.max_pending_tasks,
        )
        
        # ATOMIC increment –≤ Redis Hash
        await self._redis.hincrby(self.metrics_key, "tasks_submitted", 1)
        logger.info(f"üì§ Submitted task {task.task_id} (type: {task_type}, priority: {priority})")
        
        return task.task_id
    
    async def start_worker(self):
        """
        –ó–∞–ø—É—Å–∫ worker –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á
        –ë–ª–æ–∫–∏—Ä—É—é—â–∏–π –≤—ã–∑–æ–≤ - –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ
        """
        self._running = True
        logger.info(f"üöÄ Worker {self.consumer_name} started")
        
        try:
            while self._running and not self._shutdown_event.is_set():
                try:
                    # –ß–∏—Ç–∞—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑ stream (blocking read)
                    messages = await self._redis.xreadgroup(
                        groupname=self.consumer_group,
                        consumername=self.consumer_name,
                        streams={self.stream_name: ">"},
                        count=self.batch_size,
                        block=5000,  # 5 —Å–µ–∫ timeout
                    )
                    
                    if not messages:
                        continue
                    
                    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å batch –∑–∞–¥–∞—á
                    for stream, msg_list in messages:
                        for msg_id, fields in msg_list:
                            await self._process_message(msg_id, fields)
                
                except asyncio.CancelledError:
                    logger.info("Worker received cancellation")
                    break
                except Exception as e:
                    # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ shutdown
                    if "Connection closed" in str(e) and not self._running:
                        break
                    logger.error(f"‚ùå Worker error: {e}", exc_info=True)
                    await asyncio.sleep(5)  # Backoff
        
        finally:
            logger.info(f"üõë Worker {self.consumer_name} stopped")
    
    async def _process_message(self, msg_id: str, fields: dict[str, str]):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ stream"""
        try:
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É
            task_data = json.loads(fields["data"])
            task = Task.from_dict(task_data)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å timeout
            if task.started_at:
                elapsed = time.time() - task.started_at
                if elapsed > task.timeout_seconds:
                    logger.warning(f"‚è∞ Task {task.task_id} timeout ({elapsed:.1f}s > {task.timeout_seconds}s)")
                    task.status = TaskStatus.TIMEOUT.value
                    await self._handle_failed_task(task, msg_id, "Task timeout")
                    return
            
            # –ù–∞–π—Ç–∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            handler = self._handlers.get(task.task_type)
            if not handler:
                logger.error(f"‚ùå No handler for task_type: {task.task_type}")
                await self._redis.xack(self.stream_name, self.consumer_group, msg_id)
                return
            
            # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å
            task.status = TaskStatus.RUNNING.value
            task.started_at = time.time()
            
            logger.info(f"‚ñ∂Ô∏è  Processing task {task.task_id} (type: {task.task_type})")
            
            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
            result = await handler(task.payload)
            
            # –£—Å–ø–µ—Ö
            task.status = TaskStatus.COMPLETED.value
            task.completed_at = time.time()
            task.result = result
            
            # ATOMIC increment –≤ Redis Hash
            await self._redis.hincrby(self.metrics_key, "tasks_completed", 1)
            
            logger.success(f"‚úÖ Task {task.task_id} completed in {task.completed_at - task.started_at:.2f}s")
            
            # ACK message
            await self._redis.xack(self.stream_name, self.consumer_group, msg_id)
            
            # –£–¥–∞–ª–∏—Ç—å –∏–∑ stream (cleanup)
            await self._redis.xdel(self.stream_name, msg_id)
        
        except Exception as e:
            logger.error(f"‚ùå Task processing error: {e}", exc_info=True)
            
            # Retry logic
            task.retry_count += 1
            if task.retry_count < task.max_retries:
                task.status = TaskStatus.RETRYING.value
                # Exponential backoff
                delay = 2 ** task.retry_count
                logger.warning(f"üîÑ Retrying task {task.task_id} in {delay}s (attempt {task.retry_count}/{task.max_retries})")
                await asyncio.sleep(delay)
                # Re-submit
                await self._redis.xadd(
                    name=self.stream_name,
                    fields={"data": json.dumps(task.to_dict())},
                )
            else:
                await self._handle_failed_task(task, msg_id, str(e))
    
    async def _handle_failed_task(self, task: Task, msg_id: str, error: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ (Dead Letter Queue)"""
        task.status = TaskStatus.FAILED.value
        task.error_message = error
        
        # ATOMIC increment –≤ Redis Hash
        await self._redis.hincrby(self.metrics_key, "tasks_failed", 1)
        
        # –ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤ DLQ
        dlq_stream = f"{self.stream_name}:dlq"
        await self._redis.xadd(
            name=dlq_stream,
            fields={"data": json.dumps(task.to_dict())},
        )
        
        # ACK –∏ —É–¥–∞–ª–∏—Ç—å –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ stream
        await self._redis.xack(self.stream_name, self.consumer_group, msg_id)
        await self._redis.xdel(self.stream_name, msg_id)
        
        logger.error(f"üíÄ Task {task.task_id} moved to DLQ: {error}")
    
    async def get_task_status(self, task_id: str) -> dict[str, Any] | None:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –ø–æ ID"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º hash
        return None
    
    async def shutdown(self, timeout: int = 30):
        """Graceful shutdown worker"""
        logger.info(f"üõë Shutting down worker {self.consumer_name}...")
        self._running = False
        self._shutdown_event.set()
        
        # –ñ–¥–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
        start = time.time()
        # –ü–æ–ª—É—á–∏—Ç—å active tasks –∏–∑ Redis Stream
        try:
            info = await self._redis.xinfo_stream(self.stream_name)
            active_tasks = info.get('length', 0)
        except:
            active_tasks = 0
        
        while active_tasks > 0 and (time.time() - start) < timeout:
            logger.info(f"‚è≥ Waiting for {active_tasks} active tasks...")
            await asyncio.sleep(1)
            try:
                info = await self._redis.xinfo_stream(self.stream_name)
                active_tasks = info.get('length', 0)
            except:
                break
        
        await self.disconnect()
        logger.info("‚úÖ Worker shutdown complete")
    
    def get_metrics(self) -> dict[str, int]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ Redis Hash (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏)
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Redis Hash –¥–ª—è atomic counters - —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ multi-process –æ–∫—Ä—É–∂–µ–Ω–∏–∏
        """
        try:
            if self._redis is None:
                return {
                    "tasks_submitted": 0,
                    "tasks_completed": 0,
                    "tasks_failed": 0,
                    "tasks_timeout": 0,
                    "active_tasks": 0,
                }
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Redis client –¥–ª—è sync –º–µ—Ç–æ–¥–∞
            import redis
            sync_redis = redis.from_url(self.redis_url, decode_responses=True)
            
            # –ß–∏—Ç–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ Redis Hash –ê–¢–û–ú–ê–†–ù–û
            metrics_data = sync_redis.hgetall(self.metrics_key)
            
            # –ü–æ–ª—É—á–∏—Ç—å pending tasks –∏–∑ stream
            try:
                info = sync_redis.xinfo_stream(self.stream_name)
                active_tasks = info.get('length', 0)
            except:
                active_tasks = 0
            
            sync_redis.close()
            
            return {
                "tasks_submitted": int(metrics_data.get("tasks_submitted", 0)),
                "tasks_completed": int(metrics_data.get("tasks_completed", 0)),
                "tasks_failed": int(metrics_data.get("tasks_failed", 0)),
                "tasks_timeout": int(metrics_data.get("tasks_timeout", 0)),
                "active_tasks": active_tasks,
            }
        except Exception as e:
            logger.warning(f"Failed to get metrics from Redis: {e}")
            return {
                "tasks_submitted": 0,
                "tasks_completed": 0,
                "tasks_failed": 0,
                "tasks_timeout": 0,
                "active_tasks": 0,
            }
