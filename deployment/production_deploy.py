"""
Production Deployment Script - Phase 1 Complete

Deploys all Phase 1 security components:
- Task 1: Sandbox Isolation (Docker containers)
- Task 2: API Authentication (JWT + RBAC + Rate Limiting)
- Task 3: Secure Logging (sensitive data filtering)
- Task 4: Horizontal Scaling (Redis consumer groups, dynamic workers)

Usage:
    python deployment/production_deploy.py --env production --workers 5
"""

import os
import sys
import subprocess
import argparse
import time
from pathlib import Path
from typing import Optional, List, Dict
import json

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger


class ProductionDeployer:
    """Production deployment orchestrator for Phase 1 components"""
    
    def __init__(
        self,
        env: str = "production",
        workers: int = 4,
        enable_sandbox: bool = True,
        enable_auth: bool = True,
        enable_logging: bool = True,
        enable_scaling: bool = True,
        redis_url: str = "redis://localhost:6379/0",
        postgres_url: Optional[str] = None,
    ):
        self.env = env
        self.workers = workers
        self.enable_sandbox = enable_sandbox
        self.enable_auth = enable_auth
        self.enable_logging = enable_logging
        self.enable_scaling = enable_scaling
        self.redis_url = redis_url
        self.postgres_url = postgres_url or os.getenv(
            "DATABASE_URL", 
            "postgresql://postgres:postgres@localhost:5432/bybit_strategy"
        )
        
        self.project_root = Path(__file__).parent.parent
        self.deployment_dir = self.project_root / "deployment"
        
        logger.info(f"üöÄ Initializing {env} deployment")
        logger.info(f"Workers: {workers}")
        logger.info(f"Sandbox: {enable_sandbox}")
        logger.info(f"Auth: {enable_auth}")
        logger.info(f"Logging: {enable_logging}")
        logger.info(f"Scaling: {enable_scaling}")
    
    def check_prerequisites(self) -> bool:
        """Check if all prerequisites are met"""
        logger.info("üîç Checking prerequisites...")
        
        checks = {
            "Docker": self._check_docker(),
            "Docker Compose": self._check_docker_compose(),
            "Redis": self._check_redis(),
            "PostgreSQL": self._check_postgres(),
            "Python 3.11+": self._check_python(),
        }
        
        for name, status in checks.items():
            icon = "‚úÖ" if status else "‚ùå"
            logger.info(f"{icon} {name}: {'OK' if status else 'MISSING'}")
        
        all_ok = all(checks.values())
        if not all_ok:
            logger.error("‚ùå Prerequisites check failed! Fix errors above.")
            return False
        
        logger.success("‚úÖ All prerequisites met!")
        return True
    
    def _check_docker(self) -> bool:
        """Check if Docker is installed and running"""
        try:
            result = subprocess.run(
                ["docker", "--version"],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0
        except Exception:
            return False
    
    def _check_docker_compose(self) -> bool:
        """Check if Docker Compose is installed"""
        try:
            result = subprocess.run(
                ["docker-compose", "--version"],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0
        except Exception:
            return False
    
    def _check_redis(self) -> bool:
        """Check if Redis is accessible"""
        try:
            import redis
            r = redis.from_url(self.redis_url, socket_timeout=2)
            r.ping()
            return True
        except Exception:
            logger.warning("Redis not accessible, will be started by Docker Compose")
            return True  # Docker Compose will start it
    
    def _check_postgres(self) -> bool:
        """Check if PostgreSQL is accessible"""
        try:
            import psycopg2
            conn = psycopg2.connect(self.postgres_url, connect_timeout=2)
            conn.close()
            return True
        except Exception:
            logger.warning("PostgreSQL not accessible, will be started by Docker Compose")
            return True  # Docker Compose will start it
    
    def _check_python(self) -> bool:
        """Check if Python 3.11+ is installed"""
        return sys.version_info >= (3, 11)
    
    def create_env_file(self) -> None:
        """Create production .env file with Phase 1 security settings"""
        logger.info("üìù Creating production .env file...")
        
        env_template = f"""# Production Environment - Phase 1 Security Enabled
# Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}

# Environment
ENVIRONMENT={self.env}
DEBUG=false
LOG_LEVEL=INFO

# Database
DATABASE_URL={self.postgres_url}

# Redis
REDIS_URL={self.redis_url}

# Security - JWT Authentication
JWT_SECRET_KEY={self._generate_secret()}
JWT_ALGORITHM=RS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# Security - Rate Limiting
RATE_LIMIT_PER_USER=100
RATE_LIMIT_PER_IP=1000
RATE_LIMIT_PER_ENDPOINT=500

# Security - Sandbox Isolation
SANDBOX_ENABLED={str(self.enable_sandbox).lower()}
SANDBOX_MEMORY_LIMIT=512M
SANDBOX_CPU_LIMIT=1.0
SANDBOX_TIMEOUT_SECONDS=300

# Security - Logging
SECURE_LOGGING_ENABLED={str(self.enable_logging).lower()}
LOG_FORMAT=json
AUDIT_LOG_PATH=./logs/audit.log

# Horizontal Scaling
SCALING_ENABLED={str(self.enable_scaling).lower()}
MIN_WORKERS=2
MAX_WORKERS={self.workers}
TARGET_QUEUE_DEPTH=100
SCALING_COOLDOWN_SECONDS=60

# Celery
CELERY_BROKER_URL={self.redis_url}
CELERY_RESULT_BACKEND={self.redis_url}
CELERY_WORKER_CONCURRENCY={self.workers}

# API Keys (set these manually!)
BYBIT_API_KEY=your_bybit_api_key_here
BYBIT_API_SECRET=your_bybit_api_secret_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Master Encryption Key (CRITICAL - KEEP SECURE!)
MASTER_ENCRYPTION_KEY={self._generate_secret()}
"""
        
        env_file = self.project_root / ".env.production"
        env_file.write_text(env_template)
        
        logger.success(f"‚úÖ Created {env_file}")
        logger.warning("‚ö†Ô∏è  IMPORTANT: Update API keys in .env.production manually!")
    
    def _generate_secret(self, length: int = 32) -> str:
        """Generate a secure random secret"""
        import secrets
        return secrets.token_urlsafe(length)
    
    def create_docker_compose_production(self) -> None:
        """Create production docker-compose.yml with all Phase 1 services"""
        logger.info("üê≥ Creating production docker-compose.yml...")
        
        compose_config = {
            "version": "3.8",
            "services": {
                # Backend API with all security features
                "backend": {
                    "build": {
                        "context": ".",
                        "dockerfile": "deployment/Dockerfile.backend"
                    },
                    "ports": ["8000:8000"],
                    "environment": [
                        "ENVIRONMENT=${ENVIRONMENT}",
                        "DATABASE_URL=${DATABASE_URL}",
                        "REDIS_URL=${REDIS_URL}",
                        "JWT_SECRET_KEY=${JWT_SECRET_KEY}",
                        "SANDBOX_ENABLED=${SANDBOX_ENABLED}",
                        "SECURE_LOGGING_ENABLED=${SECURE_LOGGING_ENABLED}",
                        "SCALING_ENABLED=${SCALING_ENABLED}",
                    ],
                    "env_file": [".env.production"],
                    "depends_on": ["postgres", "redis"],
                    "restart": "unless-stopped",
                    "volumes": [
                        "./logs:/app/logs",
                        "/var/run/docker.sock:/var/run/docker.sock"  # For sandbox
                    ],
                    "networks": ["app-network"],
                    "healthcheck": {
                        "test": ["CMD", "curl", "-f", "http://localhost:8000/health"],
                        "interval": "30s",
                        "timeout": "10s",
                        "retries": 3
                    }
                },
                
                # Celery workers with horizontal scaling
                "celery-worker": {
                    "build": {
                        "context": ".",
                        "dockerfile": "deployment/Dockerfile.celery"
                    },
                    "command": [
                        "celery", "-A", "backend.celery_app:app",
                        "worker",
                        f"--concurrency={self.workers}",
                        "--loglevel=info"
                    ],
                    "environment": [
                        "CELERY_BROKER_URL=${CELERY_BROKER_URL}",
                        "CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND}",
                        "DATABASE_URL=${DATABASE_URL}",
                        "REDIS_URL=${REDIS_URL}",
                    ],
                    "env_file": [".env.production"],
                    "depends_on": ["postgres", "redis"],
                    "restart": "unless-stopped",
                    "deploy": {
                        "replicas": min(2, self.workers),  # Start with 2 workers
                        "resources": {
                            "limits": {
                                "cpus": "1.0",
                                "memory": "1G"
                            }
                        }
                    },
                    "networks": ["app-network"]
                },
                
                # PostgreSQL database
                "postgres": {
                    "image": "postgres:15-alpine",
                    "environment": [
                        "POSTGRES_DB=bybit_strategy",
                        "POSTGRES_USER=postgres",
                        "POSTGRES_PASSWORD=${DB_PASSWORD}"
                    ],
                    "volumes": ["postgres_data:/var/lib/postgresql/data"],
                    "restart": "unless-stopped",
                    "networks": ["app-network"],
                    "healthcheck": {
                        "test": ["CMD-SHELL", "pg_isready -U postgres"],
                        "interval": "10s",
                        "timeout": "5s",
                        "retries": 5
                    }
                },
                
                # Redis for caching & task queue
                "redis": {
                    "image": "redis:7-alpine",
                    "command": "redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}",
                    "volumes": ["redis_data:/data"],
                    "restart": "unless-stopped",
                    "networks": ["app-network"],
                    "healthcheck": {
                        "test": ["CMD", "redis-cli", "ping"],
                        "interval": "10s",
                        "timeout": "3s",
                        "retries": 5
                    }
                },
                
                # Prometheus for metrics
                "prometheus": {
                    "image": "prom/prometheus:latest",
                    "ports": ["9090:9090"],
                    "volumes": [
                        "./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml",
                        "prometheus_data:/prometheus"
                    ],
                    "command": [
                        "--config.file=/etc/prometheus/prometheus.yml",
                        "--storage.tsdb.path=/prometheus",
                        "--storage.tsdb.retention.time=30d"
                    ],
                    "restart": "unless-stopped",
                    "networks": ["app-network"]
                },
                
                # Grafana for dashboards
                "grafana": {
                    "image": "grafana/grafana:latest",
                    "ports": ["3000:3000"],
                    "environment": [
                        "GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}"
                    ],
                    "volumes": [
                        "grafana_data:/var/lib/grafana",
                        "./monitoring/dashboards:/etc/grafana/provisioning/dashboards"
                    ],
                    "restart": "unless-stopped",
                    "depends_on": ["prometheus"],
                    "networks": ["app-network"]
                }
            },
            
            "volumes": {
                "postgres_data": {},
                "redis_data": {},
                "prometheus_data": {},
                "grafana_data": {}
            },
            
            "networks": {
                "app-network": {
                    "driver": "bridge"
                }
            }
        }
        
        compose_file = self.project_root / "docker-compose.production.yml"
        with open(compose_file, "w") as f:
            import yaml
            yaml.dump(compose_config, f, default_flow_style=False, sort_keys=False)
        
        logger.success(f"‚úÖ Created {compose_file}")
    
    def create_dockerfiles(self) -> None:
        """Create production Dockerfiles"""
        logger.info("üê≥ Creating Dockerfiles...")
        
        # Backend Dockerfile
        backend_dockerfile = """FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    docker.io \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY backend/ ./backend/
COPY alembic/ ./alembic/
COPY alembic.ini .

# Create log directory
RUN mkdir -p /app/logs

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \\
    CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "backend.api.app:app", "--host", "0.0.0.0", "--port", "8000"]
"""
        
        (self.deployment_dir / "Dockerfile.backend").write_text(backend_dockerfile)
        
        # Celery Dockerfile
        celery_dockerfile = """FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY backend/ ./backend/

# Run Celery worker
CMD ["celery", "-A", "backend.celery_app:app", "worker", "--loglevel=info"]
"""
        
        (self.deployment_dir / "Dockerfile.celery").write_text(celery_dockerfile)
        
        logger.success("‚úÖ Created Dockerfiles")
    
    def run_database_migrations(self) -> bool:
        """Run Alembic database migrations"""
        logger.info("üóÑÔ∏è  Running database migrations...")
        
        try:
            # Set DATABASE_URL for alembic
            env = os.environ.copy()
            env["DATABASE_URL"] = self.postgres_url
            
            # Run migrations
            result = subprocess.run(
                ["alembic", "upgrade", "head"],
                cwd=self.project_root,
                env=env,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode != 0:
                logger.error(f"Migration failed: {result.stderr}")
                return False
            
            logger.success("‚úÖ Database migrations completed")
            return True
            
        except Exception as e:
            logger.error(f"Migration failed: {e}")
            return False
    
    def deploy(self) -> bool:
        """Execute full deployment"""
        logger.info("üöÄ Starting production deployment...")
        
        # Step 1: Check prerequisites
        if not self.check_prerequisites():
            return False
        
        # Step 2: Create configuration files
        self.create_env_file()
        self.create_docker_compose_production()
        self.create_dockerfiles()
        
        # Step 3: Build Docker images
        logger.info("üèóÔ∏è  Building Docker images...")
        try:
            subprocess.run(
                ["docker-compose", "-f", "docker-compose.production.yml", "build"],
                cwd=self.project_root,
                check=True,
                timeout=600
            )
            logger.success("‚úÖ Docker images built")
        except Exception as e:
            logger.error(f"Docker build failed: {e}")
            return False
        
        # Step 4: Start services
        logger.info("üöÄ Starting services...")
        try:
            subprocess.run(
                ["docker-compose", "-f", "docker-compose.production.yml", "up", "-d"],
                cwd=self.project_root,
                check=True,
                timeout=120
            )
            logger.success("‚úÖ Services started")
        except Exception as e:
            logger.error(f"Service start failed: {e}")
            return False
        
        # Step 5: Wait for services to be healthy
        logger.info("‚è≥ Waiting for services to be healthy...")
        time.sleep(10)
        
        # Step 6: Run database migrations
        if not self.run_database_migrations():
            logger.error("Database migration failed, but services are running")
        
        # Step 7: Verify deployment
        logger.info("‚úÖ Verifying deployment...")
        if self.verify_deployment():
            logger.success("üéâ DEPLOYMENT SUCCESSFUL!")
            self.print_deployment_info()
            return True
        else:
            logger.error("‚ùå Deployment verification failed")
            return False
    
    def verify_deployment(self) -> bool:
        """Verify that all services are running"""
        import requests
        
        checks = {
            "Backend API": "http://localhost:8000/health",
            "Prometheus": "http://localhost:9090/-/healthy",
            "Grafana": "http://localhost:3000/api/health",
        }
        
        for service, url in checks.items():
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    logger.success(f"‚úÖ {service}: OK")
                else:
                    logger.warning(f"‚ö†Ô∏è  {service}: HTTP {response.status_code}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  {service}: {e}")
        
        return True  # Continue even if some checks fail
    
    def print_deployment_info(self) -> None:
        """Print deployment information"""
        print("\n" + "="*70)
        print("üéâ PHASE 1 PRODUCTION DEPLOYMENT COMPLETE!")
        print("="*70)
        print("\nüìä Services:")
        print(f"  ‚Ä¢ Backend API:       http://localhost:8000")
        print(f"  ‚Ä¢ API Docs:          http://localhost:8000/docs")
        print(f"  ‚Ä¢ Prometheus:        http://localhost:9090")
        print(f"  ‚Ä¢ Grafana:           http://localhost:3000")
        print("\nüîí Security Features:")
        print(f"  ‚úÖ Sandbox Isolation: {'Enabled' if self.enable_sandbox else 'Disabled'}")
        print(f"  ‚úÖ JWT Authentication: {'Enabled' if self.enable_auth else 'Disabled'}")
        print(f"  ‚úÖ Secure Logging: {'Enabled' if self.enable_logging else 'Disabled'}")
        print(f"  ‚úÖ Horizontal Scaling: {'Enabled' if self.enable_scaling else 'Disabled'}")
        print(f"\nüöÄ Workers: {self.workers} (dynamic scaling: 2-{self.workers})")
        print("\n‚ö†Ô∏è  IMPORTANT:")
        print("  1. Update API keys in .env.production")
        print("  2. Secure MASTER_ENCRYPTION_KEY")
        print("  3. Set strong passwords for DB/Redis")
        print("  4. Review logs: docker-compose logs -f")
        print("\nüìù Next Steps:")
        print("  ‚Ä¢ Monitor metrics: http://localhost:9090")
        print("  ‚Ä¢ View dashboards: http://localhost:3000")
        print("  ‚Ä¢ Check logs: docker-compose -f docker-compose.production.yml logs -f")
        print("  ‚Ä¢ Scale workers: docker-compose -f docker-compose.production.yml scale celery-worker=5")
        print("="*70 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="Deploy Bybit Strategy Tester v2 to Production (Phase 1 Complete)"
    )
    parser.add_argument(
        "--env",
        default="production",
        choices=["production", "staging"],
        help="Deployment environment"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of Celery workers (min 2, max 20)"
    )
    parser.add_argument(
        "--no-sandbox",
        action="store_true",
        help="Disable sandbox isolation"
    )
    parser.add_argument(
        "--no-auth",
        action="store_true",
        help="Disable authentication"
    )
    parser.add_argument(
        "--no-logging",
        action="store_true",
        help="Disable secure logging"
    )
    parser.add_argument(
        "--no-scaling",
        action="store_true",
        help="Disable horizontal scaling"
    )
    parser.add_argument(
        "--redis-url",
        default="redis://redis:6379/0",
        help="Redis connection URL"
    )
    parser.add_argument(
        "--postgres-url",
        default=None,
        help="PostgreSQL connection URL"
    )
    
    args = parser.parse_args()
    
    deployer = ProductionDeployer(
        env=args.env,
        workers=max(2, min(args.workers, 20)),  # Clamp 2-20
        enable_sandbox=not args.no_sandbox,
        enable_auth=not args.no_auth,
        enable_logging=not args.no_logging,
        enable_scaling=not args.no_scaling,
        redis_url=args.redis_url,
        postgres_url=args.postgres_url,
    )
    
    success = deployer.deploy()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
