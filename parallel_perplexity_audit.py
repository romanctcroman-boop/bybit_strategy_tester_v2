"""
ğŸ”¥ Parallel Perplexity Audit Engine
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 8 API ĞºĞ»ÑÑ‡ĞµĞ¹ Perplexity Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ comprehensive Ğ¢Ğ— Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

Features:
- 8x parallel execution (ThreadPoolExecutor)
- ĞĞ½Ğ°Ğ»Ğ¸Ğ· ai_audit_results ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ° (700+ JSON files)
- ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ²ÑĞµÑ… Python Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
- AI-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ comprehensive Ğ¢Ğ—
- Rate limiting & error recovery
"""

import json
import os
import asyncio
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import List, Dict, Optional
import time
from collections import defaultdict

# Add project root to PYTHONPATH
import sys
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

from backend.agents.unified_agent_interface import (
    UnifiedAgentInterface, 
    AgentType,
    AgentRequest,
    AgentChannel
)
from loguru import logger

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROJECT_ROOT = Path(__file__).parent
AI_AUDIT_DIR = PROJECT_ROOT / "ai_audit_results"
OUTPUT_DIR = PROJECT_ROOT / "parallel_audit_results"
OUTPUT_DIR.mkdir(exist_ok=True)

MAX_WORKERS = 8  # Match with 8 Perplexity API keys
RATE_LIMIT_DELAY = 1.0  # Delay between requests per worker (seconds)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUDIT TASKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AUDIT_TASKS = [
    {
        "name": "analyze_existing_audits",
        "prompt": """ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ²ÑĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ñ‚Ñ‹ Ğ² ai_audit_results:
        
{audit_summaries}

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹ comprehensive summary:
1. ĞšĞ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ñ‡Ğ°Ñ‰Ğµ Ğ²ÑĞµĞ³Ğ¾?
2. ĞšĞ°ĞºĞ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ issues?
3. ĞšĞ°ĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ?
4. ĞšĞ°ĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² background_analysis JSON files?
5. Ğ§Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ DeepSeek audit reports?

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°: JSON Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹."""
    },
    {
        "name": "backend_architecture_audit",
        "prompt": """ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ backend Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°:

Files analyzed: {file_count}
Key services: data_service.py, backtest_tasks.py, optimize_tasks.py

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ:
1. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ´Ğ° (type hints, docstrings, error handling)
2. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ (separation of concerns, SOLID)
3. ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (database queries, async/await usage)
4. Security (API keys, input validation)
5. Testing coverage

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: JSON Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ 1-10 Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¿ÑƒĞ½ĞºÑ‚Ñƒ + ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ recommendations."""
    },
    {
        "name": "frontend_quality_audit",
        "prompt": """ĞÑƒĞ´Ğ¸Ñ‚ frontend ĞºĞ¾Ğ´Ğ° (React + TypeScript):

Components: {component_count}
Pages: {page_count}

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ:
1. TypeScript usage quality
2. React best practices (hooks, memoization)
3. State management (Redux/Context)
4. UI/UX consistency
5. Performance optimization

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: JSON Ñ issues Ğ¸ priority (HIGH/MEDIUM/LOW)."""
    },
    {
        "name": "ai_agents_integration_audit",
        "prompt": """ĞÑƒĞ´Ğ¸Ñ‚ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (DeepSeek + Perplexity):

Current status:
- 8 DeepSeek API keys loaded
- 8 Perplexity API keys loaded
- UnifiedAgentInterface active
- MCP Server configured

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ:
1. API key management quality
2. Error handling & fallback logic
3. Rate limiting implementation
4. Caching strategy
5. Agent communication patterns

Recommendations Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."""
    },
    {
        "name": "database_schema_audit",
        "prompt": """ĞÑƒĞ´Ğ¸Ñ‚ database schema (SQLAlchemy):

Tables: backtest_results, strategies, optimizations, etc.

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ:
1. Schema design (normalization, indexes)
2. Migration strategy (Alembic)
3. Query performance
4. Data integrity (constraints, foreign keys)
5. Backup & recovery

Concrete issues Ğ¸ fixes."""
    },
    {
        "name": "testing_coverage_audit",
        "prompt": """ĞÑƒĞ´Ğ¸Ñ‚ Ñ‚ĞµÑÑ‚Ğ¾Ğ² (pytest):

Test files: {test_count}
Coverage: {coverage}%

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ:
1. Unit test coverage
2. Integration tests completeness
3. Mocking strategy
4. Test performance
5. CI/CD integration

Priority areas requiring more tests."""
    },
    {
        "name": "security_audit",
        "prompt": """Security Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ²ÑĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹:

Components:
- API keys encryption (KeyManager)
- Database access
- External API calls
- File operations
- User authentication

ĞĞ°Ğ¹Ğ´Ğ¸:
1. Security vulnerabilities
2. Potential data leaks
3. Unsafe dependencies
4. Missing validations
5. OWASP Top 10 issues

Critical fixes required."""
    },
    {
        "name": "performance_bottlenecks_audit",
        "prompt": """Performance Ğ°ÑƒĞ´Ğ¸Ñ‚:

Services:
- Backtest engine
- Optimization tasks (Celery)
- Database queries
- Frontend rendering
- API response times

ĞĞ°Ğ¹Ğ´Ğ¸:
1. Slow operations
2. Memory leaks
3. N+1 queries
4. Blocking operations
5. Inefficient algorithms

Optimization priorities."""
    }
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERPLEXITY AUDIT ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ParallelPerplexityAuditEngine:
    """8x parallel Perplexity audit engine"""
    
    def __init__(self):
        self.unified_interface = UnifiedAgentInterface()
        self.results: Dict[str, Dict] = {}
        self.errors: List[Dict] = []
        
        logger.info(f"ğŸš€ Initialized with {len(self.unified_interface.key_manager.perplexity_keys)} Perplexity keys")
    
    def analyze_existing_audits(self) -> Dict:
        """Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ğ²"""
        logger.info("ğŸ“Š Analyzing existing audit results...")
        
        audit_files = list(AI_AUDIT_DIR.glob("*.json"))
        logger.info(f"Found {len(audit_files)} JSON audit files")
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼
        audits_by_type = defaultdict(list)
        
        for file_path in audit_files[:50]:  # First 50 for speed
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if "background_analysis" in file_path.name:
                    audits_by_type["background"].append(data)
                elif "deepseek" in file_path.name.lower():
                    audits_by_type["deepseek"].append(data)
                elif "perplexity" in file_path.name.lower():
                    audits_by_type["perplexity"].append(data)
                elif "test_watcher" in file_path.name:
                    audits_by_type["test_watcher"].append(data)
            except Exception as e:
                logger.warning(f"Failed to parse {file_path.name}: {e}")
        
        return {
            "total_files": len(audit_files),
            "by_type": {k: len(v) for k, v in audits_by_type.items()},
            "sample_data": {
                "background": audits_by_type["background"][:3] if audits_by_type["background"] else [],
                "deepseek": audits_by_type["deepseek"][:2] if audits_by_type["deepseek"] else []
            }
        }
    
    def scan_project_structure(self) -> Dict:
        """Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"""
        logger.info("ğŸ“‚ Scanning project structure...")
        
        python_files = list(PROJECT_ROOT.glob("**/*.py"))
        backend_files = list((PROJECT_ROOT / "backend").glob("**/*.py"))
        test_files = list((PROJECT_ROOT / "tests").glob("**/*.py"))
        
        # Frontend (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
        frontend_dir = PROJECT_ROOT / "frontend"
        frontend_components = []
        if frontend_dir.exists():
            frontend_components = list(frontend_dir.glob("**/*.tsx")) + list(frontend_dir.glob("**/*.ts"))
        
        return {
            "total_python_files": len(python_files),
            "backend_files": len(backend_files),
            "test_files": len(test_files),
            "frontend_components": len(frontend_components),
            "key_services": [
                "backend/services/data_service.py",
                "backend/tasks/backtest_tasks.py",
                "backend/tasks/optimize_tasks.py"
            ]
        }
    
    async def execute_audit_task(self, task: Dict, context: Dict) -> Dict:
        """Execute single audit task via Perplexity"""
        task_name = task["name"]
        logger.info(f"ğŸ” Executing: {task_name}")
        
        try:
            # Format prompt with context
            prompt = task["prompt"].format(**context)
            
            # Call Perplexity via AgentRequest
            request = AgentRequest(
                agent_type=AgentType.PERPLEXITY,
                task_type="analyze",  # Analysis task
                prompt=prompt,
                context={"max_tokens": 2000, "temperature": 0.7}
            )
            
            response = await self.unified_interface.send_request(
                request=request,
                preferred_channel=AgentChannel.DIRECT_API
            )
            
            if not response.success:
                raise Exception(f"API call failed: {response.error}")
            
            result = response.content
            logger.success(f"âœ… Completed: {task_name}")
            
            return {
                "task": task_name,
                "status": "success",
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed: {task_name} - {e}")
            self.errors.append({
                "task": task_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "task": task_name,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def run_parallel_audit(self):
        """Run all audit tasks in parallel"""
        logger.info(f"ğŸš€ Starting parallel audit with {MAX_WORKERS} workers...")
        
        # Step 1: Gather context
        logger.info("ğŸ“Š Step 1/3: Gathering context...")
        audit_summary = self.analyze_existing_audits()
        project_structure = self.scan_project_structure()
        
        context = {
            "audit_summaries": json.dumps(audit_summary, indent=2),
            "file_count": project_structure["backend_files"],
            "component_count": project_structure["frontend_components"],
            "page_count": 10,  # Placeholder
            "test_count": project_structure["test_files"],
            "coverage": 75  # Placeholder
        }
        
        # Step 2: Execute audits in parallel
        logger.info(f"ğŸ” Step 2/3: Running {len(AUDIT_TASKS)} audit tasks in parallel...")
        
        # Run all tasks concurrently
        tasks = [self.execute_audit_task(audit_task, context) for audit_task in AUDIT_TASKS]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Store results
        for result in results:
            if isinstance(result, dict) and result.get("status") == "success":
                self.results[result["task"]] = result
        
        # Step 3: Generate comprehensive TZ
        logger.info("ğŸ“ Step 3/3: Generating comprehensive Ğ¢Ğ—...")
        await self.generate_comprehensive_tz()
        
        # Save results
        self.save_results()
        
        logger.success(f"ğŸ‰ Audit complete! Results: {len(self.results)}, Errors: {len(self.errors)}")
    
    async def generate_comprehensive_tz(self):
        """Generate comprehensive technical specification based on audit results"""
        logger.info("ğŸ¤– Generating comprehensive Ğ¢Ğ— via Perplexity...")
        
        # Prepare audit summary for TZ generation
        audit_summary = {
            "tasks_completed": len(self.results),
            "tasks_failed": len(self.errors),
            "key_findings": []
        }
        
        for task_name, result in self.results.items():
            audit_summary["key_findings"].append({
                "area": task_name,
                "result": result.get("result", "")[:500]  # First 500 chars
            })
        
        tz_prompt = f"""ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ comprehensive Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° bybit_strategy_tester_v2, ÑĞ¾Ğ·Ğ´Ğ°Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ.

Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ°:
{json.dumps(audit_summary, indent=2, ensure_ascii=False)}

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹ Ğ¢Ğ— ÑĞ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ°Ğ¼Ğ¸:

# 1. Executive Summary
- Ğ¢ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° (Ğ¾Ñ†ĞµĞ½ĞºĞ° 1-10)
- ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ (TOP-5)
- Recommended priorities

# 2. Critical Issues (Ğ¢Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ)
- Issue 1: [Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ]
  - Description
  - Impact (HIGH/MEDIUM/LOW)
  - Effort (hours)
  - Priority

# 3. Architecture Improvements
- Backend refactoring tasks
- Frontend optimization tasks
- Database schema improvements

# 4. AI Agents Enhancement
- DeepSeek integration improvements
- Perplexity usage optimization
- MCP Server fixes

# 5. Testing & Quality
- Test coverage gaps
- Code quality improvements
- CI/CD enhancements

# 6. Performance Optimization
- Bottlenecks to fix
- Caching strategies
- Query optimization

# 7. Security Hardening
- Vulnerabilities to patch
- Security best practices
- Compliance requirements

# 8. Implementation Roadmap
- Phase 1 (1-2 weeks): Critical fixes
- Phase 2 (2-4 weeks): Major improvements
- Phase 3 (1-2 months): Long-term enhancements

# 9. Success Metrics
- KPIs to track
- Testing criteria
- Performance targets

Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Markdown document, comprehensive and actionable."""
        
        try:
            tz_result = await self.unified_interface.send_request(
                request=AgentRequest(
                    agent_type=AgentType.PERPLEXITY,
                    task_type="generate",
                    prompt=tz_prompt,
                    context={"max_tokens": 4000, "temperature": 0.7}
                ),
                preferred_channel=AgentChannel.DIRECT_API
            )
            
            if not tz_result.success:
                raise Exception(f"TZ generation failed: {tz_result.error}")
            
            result_content = tz_result.content
            
            if not tz_result.success:
                raise Exception(f"TZ generation failed: {tz_result.error}")
            
            result_content = tz_result.content
            
            self.results["comprehensive_tz"] = {
                "task": "generate_comprehensive_tz",
                "status": "success",
                "result": result_content,
                "timestamp": datetime.now().isoformat()
            }
            
            # Save TZ as separate markdown file
            tz_path = OUTPUT_DIR / f"COMPREHENSIVE_TZ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(tz_path, 'w', encoding='utf-8') as f:
                f.write(result_content)
            
            logger.success(f"ğŸ“„ Ğ¢Ğ— saved to: {tz_path}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate Ğ¢Ğ—: {e}")
            self.errors.append({
                "task": "generate_comprehensive_tz",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
    
    def save_results(self):
        """Save audit results to JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Full results
        results_path = OUTPUT_DIR / f"parallel_audit_results_{timestamp}.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "results": self.results,
                "errors": self.errors,
                "stats": {
                    "total_tasks": len(AUDIT_TASKS),
                    "successful": len(self.results),
                    "failed": len(self.errors),
                    "success_rate": len(self.results) / len(AUDIT_TASKS) * 100
                }
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ Results saved to: {results_path}")
        
        # Summary report
        summary_path = OUTPUT_DIR / f"audit_summary_{timestamp}.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"""# Parallel Perplexity Audit Summary

**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Statistics

- **Total Tasks:** {len(AUDIT_TASKS)}
- **Successful:** {len(self.results)}
- **Failed:** {len(self.errors)}
- **Success Rate:** {len(self.results) / len(AUDIT_TASKS) * 100:.1f}%

## Completed Tasks

""")
            for task_name in self.results.keys():
                f.write(f"- âœ… {task_name}\n")
            
            if self.errors:
                f.write("\n## Failed Tasks\n\n")
                for error in self.errors:
                    f.write(f"- âŒ {error['task']}: {error['error']}\n")
        
        logger.info(f"ğŸ“„ Summary saved to: {summary_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    """Main entry point"""
    logger.info("="*80)
    logger.info("ğŸ”¥ Parallel Perplexity Audit Engine Starting...")
    logger.info("="*80)
    
    engine = ParallelPerplexityAuditEngine()
    
    start_time = time.time()
    
    try:
        await engine.run_parallel_audit()
        
        elapsed = time.time() - start_time
        logger.success(f"âœ… Audit complete in {elapsed:.1f} seconds")
        
    except Exception as e:
        logger.error(f"âŒ Audit failed: {e}")
        raise
    
    logger.info("="*80)
    logger.info("ğŸ‰ Done! Check parallel_audit_results/ folder for outputs")
    logger.info("="*80)


if __name__ == "__main__":
    asyncio.run(main())
