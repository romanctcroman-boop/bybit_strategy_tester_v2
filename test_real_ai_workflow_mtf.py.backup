"""
ðŸš€ Ð ÐÐ¡Ð¨Ð˜Ð Ð•ÐÐÐ«Ð™ Ð Ð•ÐÐ›Ð¬ÐÐ«Ð™ AI WORKFLOW Ð¡ MULTI-TIMEFRAME Ð¢Ð•Ð¡Ð¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð•Ðœ

Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ:
1. âœ… Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐŸÐžÐ›ÐÐ«Ð¥ 3 Ð¼ÐµÑÑÑ†ÐµÐ² Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð½Ðµ 1000 ÑÐ²ÐµÑ‡ÐµÐ¹)
2. âœ… Multi-Timeframe: 5m, 15m, 30m Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾
3. âœ… ÐœÐ½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ñ†Ð¸ÐºÐ»Ñ‹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
4. âœ… Grid Search Ð¿Ð¾ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼ Ð¾Ñ‚ Perplexity
5. âœ… Walk-Forward Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
6. âœ… Ð ÐµÐ°Ð»ÑŒÐ½Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· backfill

Ð›ÐžÐ—Ð£ÐÐ“: MCP ÑÐµÑ€Ð²ÐµÑ€Ð° Copilot â†” Perplexity - Ð½Ð°ÑˆÐµ Ð±ÑƒÐ´ÑƒÑ‰ÐµÐµ Ð¸ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐµ!
"""

import os
from dotenv import load_dotenv

load_dotenv()  # Load environment variables from .env file


import asyncio
import json
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, Any, List
import httpx
import pandas as pd

# Add paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'mcp-server'))
sys.path.insert(0, os.path.dirname(__file__))

# Perplexity API config
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

if not PERPLEXITY_API_KEY:
    raise ValueError(
        "âš ï¸ SECURITY: PERPLEXITY_API_KEY not configured.
"
        "Please add PERPLEXITY_API_KEY to .env file"
    )
PERPLEXITY_API_URL = "https://api.perplexity.ai/chat/completions"


class MTFAIWorkflowOrchestrator:
    """Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ñ Multi-Timeframe Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹."""
    
    def __init__(self):
        self.dialogue_history = []
        self.test_results = {}
        self.current_date = datetime(2025, 10, 29)
        self.test_period_start = self.current_date - timedelta(days=90)  # 3 Ð¼ÐµÑÑÑ†Ð°
        self.timeframes = ['5', '15', '30']  # MTF: 5m, 15m, 30m
        self.central_tf = '15'  # Ð¦ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼
        
    async def call_perplexity(
        self, 
        query: str, 
        model: str = "sonar-pro",
        context: str = ""
    ) -> Dict[str, Any]:
        """ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð²Ñ‹Ð·Ð¾Ð² Perplexity API."""
        
        full_query = query
        if context:
            full_query = f"{context}\n\n{query}"
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    PERPLEXITY_API_URL,
                    headers={
                        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are a cryptocurrency trading expert and backtesting specialist with deep knowledge of multi-timeframe analysis."
                            },
                            {
                                "role": "user",
                                "content": full_query
                            }
                        ],
                        "temperature": 0.2,
                        "top_p": 0.9,
                        "return_citations": True,
                        "search_recency_filter": "month",
                        "stream": False
                    }
                )
                
                if response.status_code != 200:
                    return {
                        "success": False,
                        "error": f"API error: {response.status_code}",
                        "answer": ""
                    }
                
                data = response.json()
                answer = data["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "answer": answer,
                    "model": model,
                    "usage": data.get("usage", {}),
                    "citations": data.get("citations", [])
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "answer": ""
            }
    
    def log_dialogue(self, agent: str, message: str, response: str):
        """Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ."""
        self.dialogue_history.append({
            "timestamp": datetime.now().isoformat(),
            "agent": agent,
            "message": message,
            "response": response[:500] + "..." if len(response) > 500 else response
        })
    
    async def phase1_mtf_strategy_design(self):
        """Ð¤ÐÐ—Ð 1: Ð”Ð¸Ð·Ð°Ð¹Ð½ Multi-Timeframe ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Perplexity."""
        print("\n" + "="*80)
        print("ðŸ“‹ Ð¤ÐÐ—Ð 1: Ð”Ð˜Ð—ÐÐ™Ð MTF Ð¡Ð¢Ð ÐÐ¢Ð•Ð“Ð˜Ð˜ Ð§Ð•Ð Ð•Ð— PERPLEXITY")
        print("="*80)
        
        copilot_query = f"""
        TASK: Design a Multi-Timeframe (MTF) trading strategy for crypto backtesting
        
        CONTEXT:
        - Timeframes: 5m (LTF), 15m (Central), 30m (HTF)
        - Asset: BTCUSDT
        - Period: {self.test_period_start.strftime('%Y-%m-%d')} to {self.current_date.strftime('%Y-%m-%d')} (90 days)
        - Total data needed: ~8,640 5m candles, ~2,880 15m candles, ~1,440 30m candles
        
        REQUIREMENTS:
        1. How to use 5m for entry precision?
        2. How to use 15m for main signals (EMA crossover)?
        3. How to use 30m for trend filter?
        4. Should we align timeframes or use independent analysis?
        5. What are the best HTF (Higher TimeFrame) filters?
        
        Provide specific MTF strategy design with roles for each timeframe.
        """
        
        print("\nðŸ¤– Copilot â†’ Perplexity:")
        print(f"   Query: Designing MTF strategy...")
        
        result = await self.call_perplexity(copilot_query, model="sonar-pro")
        
        if result["success"]:
            print(f"\nðŸ”® Perplexity MTF Strategy Design:")
            print(f"   {result['answer'][:500]}...")
            print(f"   [Total: {len(result['answer'])} chars, {result['usage'].get('total_tokens', 'N/A')} tokens]")
            
            self.log_dialogue("Copilot", copilot_query, result["answer"])
            self.test_results["phase1_mtf_design"] = result["answer"]
            return result["answer"]
        else:
            print(f"   âŒ Error: {result['error']}")
            return None
    
    async def phase2_data_requirements(self):
        """Ð¤ÐÐ—Ð 2: Ð Ð°ÑÑ‡Ñ‘Ñ‚ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ðº Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ñ‡ÐµÑ€ÐµÐ· Perplexity."""
        print("\n" + "="*80)
        print("ðŸ“Š Ð¤ÐÐ—Ð 2: Ð ÐÐ¡Ð§ÐÐ¢ Ð¢Ð Ð•Ð‘ÐžÐ’ÐÐÐ˜Ð™ Ðš Ð”ÐÐÐÐ«Ðœ")
        print("="*80)
        
        copilot_query = f"""
        CALCULATION TASK: Data requirements for 3-month MTF backtest
        
        GIVEN:
        - Period: 90 days ({self.test_period_start.strftime('%Y-%m-%d')} to {self.current_date.strftime('%Y-%m-%d')})
        - Timeframes: 5m, 15m, 30m
        - 24/7 crypto market
        
        CALCULATE:
        1. How many 5-minute candles in 90 days? (Formula: days * 24 * 60 / 5)
        2. How many 15-minute candles in 90 days?
        3. How many 30-minute candles in 90 days?
        4. Bybit API limit is 1000 candles per request - how many requests needed per timeframe?
        5. What's the optimal pagination strategy?
        
        Provide exact numbers and data loading strategy.
        """
        
        print("\nðŸ¤– Copilot â†’ Perplexity:")
        print(f"   Calculating data requirements...")
        
        result = await self.call_perplexity(copilot_query, model="sonar-pro")
        
        if result["success"]:
            answer = result["answer"]
            print(f"\nðŸ”® Perplexity Data Requirements:")
            print(f"   {answer}")
            
            self.log_dialogue("Copilot", copilot_query, answer)
            self.test_results["phase2_data_requirements"] = answer
            
            # Copilot calculates
            candles_5m = 90 * 24 * 12  # 25,920
            candles_15m = 90 * 24 * 4  # 8,640
            candles_30m = 90 * 24 * 2  # 4,320
            
            print(f"\nðŸŽ¯ Copilot Calculated:")
            print(f"   5m candles needed: {candles_5m:,}")
            print(f"   15m candles needed: {candles_15m:,}")
            print(f"   30m candles needed: {candles_30m:,}")
            print(f"   API requests (5m): {candles_5m // 1000 + 1}")
            print(f"   API requests (15m): {candles_15m // 1000 + 1}")
            print(f"   API requests (30m): {candles_30m // 1000 + 1}")
            
            self.test_results["data_requirements"] = {
                "candles_5m": candles_5m,
                "candles_15m": candles_15m,
                "candles_30m": candles_30m,
                "requests_5m": candles_5m // 1000 + 1,
                "requests_15m": candles_15m // 1000 + 1,
                "requests_30m": candles_30m // 1000 + 1,
            }
            
            return answer
        else:
            print(f"   âŒ Error: {result['error']}")
            return None
    
    async def phase3_load_full_mtf_data(self, symbol: str):
        """Ð¤ÐÐ—Ð 3: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐŸÐžÐ›ÐÐ«Ð¥ 3 Ð¼ÐµÑÑÑ†ÐµÐ² Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð²ÑÐµÑ… Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð¾Ð²."""
        print("\n" + "="*80)
        print("ðŸ“¥ Ð¤ÐÐ—Ð 3: Ð—ÐÐ“Ð Ð£Ð—ÐšÐ ÐŸÐžÐ›ÐÐ«Ð¥ MTF Ð”ÐÐÐÐ«Ð¥ (3 ÐœÐ•Ð¡Ð¯Ð¦Ð)")
        print("="*80)
        
        try:
            from backend.services.backfill_service import BackfillService
            from backend.database.connection import get_session
            
            print(f"\nðŸ”§ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ BackfillService...")
            
            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÐµÑ€Ð²Ð¸Ñ
            with get_session() as session:
                service = BackfillService(session)
                
                mtf_data = {}
                
                for tf in self.timeframes:
                    print(f"\nðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {tf}m Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ {symbol}...")
                    print(f"   ÐŸÐµÑ€Ð¸Ð¾Ð´: {self.test_period_start.strftime('%Y-%m-%d')} â†’ {self.current_date.strftime('%Y-%m-%d')}")
                    
                    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ backfill Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
                    result = service.backfill_range(
                        symbol=symbol,
                        interval=tf,
                        start_at=self.test_period_start,
                        end_at=self.current_date,
                        page_limit=1000
                    )
                    
                    print(f"   âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾: {result['upserts']} ÑÐ²ÐµÑ‡ÐµÐ¹")
                    print(f"   API Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²: {result['pages']}")
                    print(f"   Ð’Ñ€ÐµÐ¼Ñ: {result['elapsed_sec']:.2f}s")
                    
                    # Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð¸Ð· Ð‘Ð”
                    from backend.database.models import BybitKlineAudit
                    
                    klines = session.query(BybitKlineAudit).filter(
                        BybitKlineAudit.symbol == symbol,
                        BybitKlineAudit.interval == tf,
                        BybitKlineAudit.open_time >= self.test_period_start,
                        BybitKlineAudit.open_time <= self.current_date
                    ).order_by(BybitKlineAudit.open_time).all()
                    
                    # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² DataFrame
                    df = pd.DataFrame([{
                        'timestamp': k.open_time,
                        'open': float(k.open_price) if k.open_price else 0,
                        'high': float(k.high_price) if k.high_price else 0,
                        'low': float(k.low_price) if k.low_price else 0,
                        'close': float(k.close_price) if k.close_price else 0,
                        'volume': float(k.volume) if k.volume else 0,
                    } for k in klines])
                    
                    if not df.empty:
                        df = df.sort_values('timestamp').reset_index(drop=True)
                        mtf_data[tf] = df
                        
                        print(f"   ðŸ“ˆ DataFrame ÑÐ¾Ð·Ð´Ð°Ð½: {len(df)} ÑÑ‚Ñ€Ð¾Ðº")
                        print(f"      ÐŸÐµÑ€Ð¸Ð¾Ð´: {df['timestamp'].iloc[0]} â†’ {df['timestamp'].iloc[-1]}")
                        print(f"      Ð¦ÐµÐ½Ð°: ${df['close'].iloc[0]:.2f} â†’ ${df['close'].iloc[-1]:.2f}")
                    else:
                        print(f"   âš ï¸ ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð‘Ð” Ð´Ð»Ñ {tf}m")
                
                self.test_results["mtf_data_loaded"] = {
                    tf: {
                        "candles": len(df),
                        "start": df['timestamp'].iloc[0].isoformat() if not df.empty else None,
                        "end": df['timestamp'].iloc[-1].isoformat() if not df.empty else None,
                    } for tf, df in mtf_data.items()
                }
                
                return mtf_data
                
        except Exception as e:
            print(f"\nâŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
            import traceback
            traceback.print_exc()
            
            # Fallback: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´
            print(f"\nâš ï¸ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback Ð¼ÐµÑ‚Ð¾Ð´ (Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¾ 1000 ÑÐ²ÐµÑ‡ÐµÐ¹)...")
            return await self.phase3_fallback_load_data(symbol)
    
    async def phase3_fallback_load_data(self, symbol: str):
        """Fallback Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐµÑÐ»Ð¸ backfill Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚."""
        from backend.services.adapters.bybit import BybitAdapter
        
        adapter = BybitAdapter()
        mtf_data = {}
        
        for tf in self.timeframes:
            print(f"\nðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {tf}m (fallback - Ð¼Ð°ÐºÑ 1000 ÑÐ²ÐµÑ‡ÐµÐ¹)...")
            
            raw_data = adapter.get_klines(symbol=symbol, interval=tf, limit=1000)
            
            if raw_data:
                df = pd.DataFrame(raw_data)
                df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')
                
                column_mapping = {
                    'open_price': 'open',
                    'high_price': 'high',
                    'low_price': 'low',
                    'close_price': 'close',
                }
                
                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns and new_col not in df.columns:
                        df[new_col] = df[old_col]
                
                df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].sort_values('timestamp').reset_index(drop=True)
                mtf_data[tf] = df
                
                print(f"   âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(df)} ÑÐ²ÐµÑ‡ÐµÐ¹")
                print(f"   ÐŸÐµÑ€Ð¸Ð¾Ð´: {df['timestamp'].iloc[0]} â†’ {df['timestamp'].iloc[-1]}")
        
        return mtf_data
    
    async def phase4_grid_optimization(self, mtf_data: Dict[str, pd.DataFrame], parameter_matrix: List[Dict]):
        """Ð¤ÐÐ—Ð 4: Grid Search Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼ Ð¾Ñ‚ Perplexity."""
        print("\n" + "="*80)
        print("âš™ï¸ Ð¤ÐÐ—Ð 4: GRID SEARCH ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯")
        print("="*80)
        
        from backend.core.backtest_engine import BacktestEngine
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð³Ð¾ Ð±ÑÐºÑ‚ÐµÑÑ‚Ð°
        central_data = mtf_data.get(self.central_tf)
        
        if central_data is None or central_data.empty:
            print("âŒ ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð°")
            return None
        
        print(f"\nðŸŽ¯ Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ {len(parameter_matrix)} ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²...")
        print(f"   Ð”Ð°Ð½Ð½Ñ‹Ðµ: {len(central_data)} ÑÐ²ÐµÑ‡ÐµÐ¹ Ð½Ð° {self.central_tf}m")
        
        optimization_results = []
        
        for i, params in enumerate(parameter_matrix, 1):
            print(f"\n   ðŸ”„ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ {i}/{len(parameter_matrix)}: {params}")
            
            engine = BacktestEngine(
                initial_capital=10_000.0,
                commission=0.055 / 100,
                slippage_pct=0.05
            )
            
            strategy_config = {
                'type': 'ema_crossover',
                'fast_ema': params['fast_ema'],
                'slow_ema': params['slow_ema'],
                'rsi_period': params.get('rsi_period', 14),
                'take_profit_pct': 3.0,
                'stop_loss_pct': 1.5,
                'risk_per_trade_pct': 2.0,
                'max_positions': 1,
            }
            
            try:
                results = engine.run(central_data, strategy_config)
                
                optimization_results.append({
                    "params": params,
                    "total_return": results['total_return'],
                    "sharpe_ratio": results['sharpe_ratio'],
                    "max_drawdown": results['max_drawdown'],
                    "total_trades": results['total_trades'],
                    "win_rate": results['win_rate'],
                    "profit_factor": results['profit_factor'],
                    "final_capital": results['final_capital']
                })
                
                print(f"      Return: {results['total_return']*100:.2f}%, Sharpe: {results['sharpe_ratio']:.2f}, "
                      f"Trades: {results['total_trades']}, WinRate: {results['win_rate']*100:.1f}%")
                
            except Exception as e:
                print(f"      âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
                optimization_results.append({
                    "params": params,
                    "error": str(e)
                })
        
        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Sharpe Ratio
        valid_results = [r for r in optimization_results if 'error' not in r]
        if valid_results:
            valid_results.sort(key=lambda x: x['sharpe_ratio'], reverse=True)
            
            print(f"\nðŸ† Ð¢ÐžÐŸ-3 Ð›Ð£Ð§Ð¨Ð˜Ð¥ ÐšÐžÐœÐ‘Ð˜ÐÐÐ¦Ð˜Ð™ (Ð¿Ð¾ Sharpe):")
            for i, result in enumerate(valid_results[:3], 1):
                print(f"\n   {i}. {result['params']}")
                print(f"      Return: {result['total_return']*100:.2f}%")
                print(f"      Sharpe: {result['sharpe_ratio']:.2f}")
                print(f"      Max DD: {result['max_drawdown']*100:.2f}%")
                print(f"      Trades: {result['total_trades']}")
                print(f"      Win Rate: {result['win_rate']*100:.1f}%")
        
        self.test_results["grid_optimization"] = optimization_results
        
        return optimization_results
    
    async def phase5_ai_analysis_optimization(self, optimization_results: List[Dict]):
        """Ð¤ÐÐ—Ð 5: AI Ð°Ð½Ð°Ð»Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸."""
        print("\n" + "="*80)
        print("ðŸ”¬ Ð¤ÐÐ—Ð 5: AI ÐÐÐÐ›Ð˜Ð— Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢ÐžÐ’ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð˜")
        print("="*80)
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ summary Ð´Ð»Ñ Perplexity
        valid_results = [r for r in optimization_results if 'error' not in r]
        
        if not valid_results:
            print("âŒ ÐÐµÑ‚ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
            return None
        
        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ñ€Ð°Ð·Ð½Ñ‹Ð¼ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸ÑÐ¼
        by_return = sorted(valid_results, key=lambda x: x['total_return'], reverse=True)[:3]
        by_sharpe = sorted(valid_results, key=lambda x: x['sharpe_ratio'], reverse=True)[:3]
        by_winrate = sorted(valid_results, key=lambda x: x['win_rate'], reverse=True)[:3]
        
        copilot_query = f"""
        OPTIMIZATION RESULTS ANALYSIS
        
        Tested {len(valid_results)} parameter combinations for EMA Crossover strategy on BTCUSDT 15m.
        
        TOP 3 BY TOTAL RETURN:
        {json.dumps(by_return, indent=2)}
        
        TOP 3 BY SHARPE RATIO:
        {json.dumps(by_sharpe, indent=2)}
        
        TOP 3 BY WIN RATE:
        {json.dumps(by_winrate, indent=2)}
        
        QUESTIONS:
        1. Which parameters show the best overall performance?
        2. Is there overfitting risk (too few trades, too high win rate)?
        3. Which metric should we prioritize: Return, Sharpe, or Win Rate?
        4. Are there parameter patterns (e.g., faster EMAs always better)?
        5. What's the recommended final parameter set?
        
        Provide expert analysis and final recommendation.
        """
        
        print("\nðŸ¤– Copilot â†’ Perplexity:")
        print(f"   Sending optimization results for analysis...")
        
        result = await self.call_perplexity(copilot_query, model="sonar-pro")
        
        if result["success"]:
            answer = result["answer"]
            print(f"\nðŸ”® Perplexity Expert Analysis:")
            print(f"{answer}")
            
            self.log_dialogue("Copilot", copilot_query, answer)
            self.test_results["phase5_optimization_analysis"] = answer
            
            return answer
        else:
            print(f"   âŒ Error: {result['error']}")
            return None
    
    async def run_full_mtf_workflow(self, symbol: str = "BTCUSDT"):
        """Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ MTF workflow."""
        print("\n" + "ðŸŒŸ"*40)
        print("ðŸš€ Ð ÐÐ¡Ð¨Ð˜Ð Ð•ÐÐÐ«Ð™ AI WORKFLOW: MTF + ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯")
        print("ðŸŒŸ"*40)
        print(f"\nÐ¡Ð¸Ð¼Ð²Ð¾Ð»: {symbol}")
        print(f"ÐŸÐµÑ€Ð¸Ð¾Ð´: {self.test_period_start.strftime('%Y-%m-%d')} - {self.current_date.strftime('%Y-%m-%d')} (90 Ð´Ð½ÐµÐ¹)")
        print(f"Ð¢Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ñ‹: {', '.join([f'{tf}m' for tf in self.timeframes])}")
        print(f"Ð¦ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ TF: {self.central_tf}m")
        print(f"\nÐ›ÐžÐ—Ð£ÐÐ“: MCP ÑÐµÑ€Ð²ÐµÑ€Ð° Copilot â†” Perplexity - Ð½Ð°ÑˆÐµ Ð±ÑƒÐ´ÑƒÑ‰ÐµÐµ Ð¸ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐµ!")
        
        try:
            # Ð¤Ð°Ð·Ð° 1: MTF Strategy Design
            await self.phase1_mtf_strategy_design()
            await asyncio.sleep(2)
            
            # Ð¤Ð°Ð·Ð° 2: Data Requirements
            await self.phase2_data_requirements()
            await asyncio.sleep(2)
            
            # Ð¤Ð°Ð·Ð° 3: Load FULL MTF Data
            mtf_data = await self.phase3_load_full_mtf_data(symbol)
            await asyncio.sleep(2)
            
            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² (Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ³Ð¾ Ñ‚ÐµÑÑ‚Ð°)
            parameter_matrix = [
                {"fast_ema": 9, "slow_ema": 21, "rsi_period": 14},
                {"fast_ema": 12, "slow_ema": 26, "rsi_period": 14},
                {"fast_ema": 20, "slow_ema": 50, "rsi_period": 9},
                {"fast_ema": 10, "slow_ema": 30, "rsi_period": 14},
                {"fast_ema": 15, "slow_ema": 45, "rsi_period": 21},
            ]
            
            # Ð¤Ð°Ð·Ð° 4: Grid Optimization
            optimization_results = await self.phase4_grid_optimization(mtf_data, parameter_matrix)
            await asyncio.sleep(2)
            
            # Ð¤Ð°Ð·Ð° 5: AI Analysis
            if optimization_results:
                await self.phase5_ai_analysis_optimization(optimization_results)
            
            # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚
            await self.generate_final_report()
            
        except Exception as e:
            print(f"\nâŒ ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
            import traceback
            traceback.print_exc()
    
    async def generate_final_report(self):
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð°."""
        print("\n" + "="*80)
        print("ðŸ“„ Ð¤Ð˜ÐÐÐ›Ð¬ÐÐ«Ð™ MTF ÐžÐ¢Ð§ÐÐ¢")
        print("="*80)
        
        report_path = "REAL_AI_MTF_WORKFLOW_REPORT.json"
        
        report_data = {
            "test_date": datetime.now().isoformat(),
            "test_period": {
                "start": self.test_period_start.isoformat(),
                "end": self.current_date.isoformat(),
                "days": 90
            },
            "timeframes": self.timeframes,
            "central_timeframe": self.central_tf,
            "dialogue_turns": len(self.dialogue_history),
            "results": self.test_results,
            "dialogue_history": self.dialogue_history
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… MTF Ð¾Ñ‚Ñ‡Ñ‘Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½: {report_path}")
        print(f"\nðŸ“Š Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ:")
        print(f"   ÐžÐ±Ð¾Ñ€Ð¾Ñ‚Ñ‹ Copilot â†” Perplexity: {len(self.dialogue_history)}")
        print(f"   Ð¢Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ñ‹ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: {len(self.timeframes)}")
        
        if "mtf_data_loaded" in self.test_results:
            print(f"\nðŸ“¥ Ð—ÐÐ“Ð Ð£Ð–Ð•ÐÐÐ«Ð• Ð”ÐÐÐÐ«Ð•:")
            for tf, info in self.test_results["mtf_data_loaded"].items():
                print(f"   {tf}m: {info['candles']} ÑÐ²ÐµÑ‡ÐµÐ¹")
        
        if "grid_optimization" in self.test_results:
            valid = len([r for r in self.test_results["grid_optimization"] if 'error' not in r])
            print(f"\nâš™ï¸ ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯:")
            print(f"   ÐŸÑ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ð¹: {valid}")


async def main():
    """Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ."""
    orchestrator = MTFAIWorkflowOrchestrator()
    await orchestrator.run_full_mtf_workflow(symbol="BTCUSDT")


if __name__ == "__main__":
    print("\n" + "ðŸŒŸ"*40)
    print("Ð—ÐÐŸÐ£Ð¡Ðš Ð ÐÐ¡Ð¨Ð˜Ð Ð•ÐÐÐžÐ“Ðž MTF AI WORKFLOW")
    print("Multi-Timeframe + Grid Optimization + Real Data")
    print("ðŸŒŸ"*40 + "\n")
    
    asyncio.run(main())
