"""
üöÄ DeepSeek AI Robot - Advanced Enterprise Architecture
=======================================================

–ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
1. Multi-API Keys Pool (4-8 –∫–ª—é—á–µ–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã)
2. –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å + –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
3. Intelligent Context Storage —Å ML
4. Fast & Reliable Caching System
5. Workflow: DeepSeek ‚Üí Perplexity ‚Üí DeepSeek ‚Üí Copilot

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
- –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ 4-8 –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ DeepSeek API
- ML-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–º–Ω–æ–≥–æ –∫—ç—à–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –°–∫–æ—Ä–æ—Å—Ç—å: +400-800% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å sequential
"""

import asyncio
import hashlib
import heapq  # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: heap –¥–ª—è O(log n) eviction
import json
import pickle
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import deque
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading
import logging
import psutil  # üöÄ Wave 2 Priority 4: Memory monitoring
import gc  # Garbage collection
import weakref  # Weak references

# ML imports –¥–ª—è —É–º–Ω–æ–≥–æ –∫—ç—à–∞
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("‚ö†Ô∏è  sklearn not available, ML features disabled")

# Real API clients
from automation.deepseek_robot.api_clients import (
    DeepSeekClient,
    PerplexityClient,
    DeepSeekAPIError,
    PerplexityAPIError
)

logger = logging.getLogger(__name__)


class WeakRefWrapper:
    """
    üöÄ Wave 2 Priority 4: Wrapper for weak references to large objects
    
    Numpy arrays can't use weakref directly, so we wrap them in a class.
    This allows garbage collection to free memory when the array is no longer needed.
    """
    def __init__(self, obj: Any):
        self.obj = obj
    
    def get(self) -> Any:
        """Get the wrapped object"""
        return self.obj
    
    def clear(self):
        """Clear the reference for manual cleanup"""
        self.obj = None


class MemoryMonitor:
    """
    üöÄ Wave 2 Priority 4: Memory Leak Detection & Monitoring
    
    Features:
    - Real-time memory usage tracking
    - Memory leak detection
    - Automatic cleanup triggers
    - Performance statistics
    """
    
    def __init__(self, warning_threshold_mb: int = 500, critical_threshold_mb: int = 1000):
        """
        Args:
            warning_threshold_mb: Warning threshold in MB
            critical_threshold_mb: Critical threshold in MB (trigger cleanup)
        """
        self.process = psutil.Process()
        self.warning_threshold = warning_threshold_mb * 1024 * 1024  # Convert to bytes
        self.critical_threshold = critical_threshold_mb * 1024 * 1024
        
        # Tracking
        self.memory_samples = deque(maxlen=100)  # Last 100 samples
        self.baseline_memory = None
        self.peak_memory = 0
        self.cleanup_count = 0
        self.warnings_count = 0
        
        # Record initial baseline
        self._record_baseline()
    
    def _record_baseline(self):
        """Record baseline memory usage"""
        mem_info = self.process.memory_info()
        self.baseline_memory = mem_info.rss
        self.peak_memory = mem_info.rss
    
    def check_memory(self) -> Dict[str, Any]:
        """
        Check current memory usage
        
        Returns:
            {
                "current_mb": float,
                "baseline_mb": float,
                "peak_mb": float,
                "growth_mb": float,
                "growth_percent": float,
                "status": "ok"/"warning"/"critical",
                "needs_cleanup": bool
            }
        """
        mem_info = self.process.memory_info()
        current = mem_info.rss
        
        # Update peak
        if current > self.peak_memory:
            self.peak_memory = current
        
        # Record sample
        self.memory_samples.append({
            "timestamp": datetime.now(),
            "memory": current
        })
        
        # Calculate metrics
        growth = current - self.baseline_memory
        growth_percent = (growth / self.baseline_memory * 100) if self.baseline_memory > 0 else 0
        
        # Determine status
        status = "ok"
        needs_cleanup = False
        
        if current >= self.critical_threshold:
            status = "critical"
            needs_cleanup = True
            self.warnings_count += 1
        elif current >= self.warning_threshold:
            status = "warning"
            self.warnings_count += 1
        
        return {
            "current_mb": current / 1024 / 1024,
            "baseline_mb": self.baseline_memory / 1024 / 1024,
            "peak_mb": self.peak_memory / 1024 / 1024,
            "growth_mb": growth / 1024 / 1024,
            "growth_percent": growth_percent,
            "status": status,
            "needs_cleanup": needs_cleanup,
            "warnings_count": self.warnings_count
        }
    
    def cleanup(self) -> Dict[str, Any]:
        """
        Trigger memory cleanup
        
        Returns:
            Cleanup statistics
        """
        before = self.process.memory_info().rss
        
        # Force garbage collection
        collected = gc.collect()
        
        after = self.process.memory_info().rss
        freed = before - after
        
        self.cleanup_count += 1
        
        return {
            "freed_mb": freed / 1024 / 1024,
            "objects_collected": collected,
            "cleanup_count": self.cleanup_count
        }
    
    def get_trend(self) -> str:
        """
        Analyze memory trend
        
        Returns:
            "stable", "growing", "shrinking"
        """
        if len(self.memory_samples) < 10:
            return "insufficient_data"
        
        # Compare recent samples
        recent = [s["memory"] for s in list(self.memory_samples)[-10:]]
        older = [s["memory"] for s in list(self.memory_samples)[-20:-10]] if len(self.memory_samples) >= 20 else recent
        
        avg_recent = sum(recent) / len(recent)
        avg_older = sum(older) / len(older)
        
        diff_percent = ((avg_recent - avg_older) / avg_older * 100) if avg_older > 0 else 0
        
        if diff_percent > 10:
            return "growing"
        elif diff_percent < -10:
            return "shrinking"
        else:
            return "stable"
    
    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive memory statistics"""
        current_check = self.check_memory()
        trend = self.get_trend()
        
        return {
            **current_check,
            "trend": trend,
            "cleanup_count": self.cleanup_count,
            "samples_count": len(self.memory_samples)
        }


@dataclass
class CacheEntry:
    """
    –ó–∞–ø–∏—Å—å –≤ –∫—ç—à–µ
    
    üöÄ Wave 2 Priority 4: Optimized for memory efficiency
    Large embeddings can be garbage collected when memory pressure is high
    """
    key: str
    value: Any
    timestamp: datetime
    access_count: int = 0
    last_access: datetime = None
    embedding: Optional[np.ndarray] = None  # ML embedding –¥–ª—è –ø–æ–∏—Å–∫–∞
    
    def __post_init__(self):
        if self.last_access is None:
            self.last_access = self.timestamp


@dataclass
class ContextSnapshot:
    """
    –°–Ω–∏–º–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è DeepSeek Agent
    
    üöÄ Wave 2 Priority 4: Optimized for memory efficiency
    """
    timestamp: datetime
    conversation_history: List[Dict[str, Any]]
    learned_patterns: Dict[str, Any]
    quality_metrics: Dict[str, float]
    project_state: Dict[str, Any]
    embedding: Optional[np.ndarray] = None


class APIKeyPool:
    """
    –ü—É–ª API –∫–ª—é—á–µ–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
    
    Features:
    - Smart load balancing (latency-based)
    - Health monitoring per key
    - Automatic failover
    - Rate limiting per key
    
    üöÄ Wave 2 Priority 3: Load Balancing —Å health monitoring
    """
    
    def __init__(self, keys: List[str], max_requests_per_minute: int = 60):
        """
        Args:
            keys: –°–ø–∏—Å–æ–∫ API –∫–ª—é—á–µ–π (4-8)
            max_requests_per_minute: –õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É –Ω–∞ –∫–ª—é—á
        """
        self.keys = keys
        self.max_rpm = max_requests_per_minute
        self.current_index = 0
        self.lock = threading.Lock()
        
        # üöÄ ENHANCED: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ + health monitoring
        self.key_stats = {
            key: {
                "requests": deque(maxlen=100),  # –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
                "errors": 0,
                "total_requests": 0,
                "successful_requests": 0,
                "avg_response_time": 0.0,
                "last_used": None,
                # üöÄ NEW: Health metrics
                "latency_samples": deque(maxlen=20),  # Last 20 latencies
                "error_rate": 0.0,  # Percentage
                "health_score": 100.0,  # 0-100
                "is_healthy": True
            }
            for key in keys
        }
    
    def get_available_key(self) -> Optional[str]:
        """
        üöÄ SMART LOAD BALANCING: –ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à–∏–π API –∫–ª—é—á
        
        –í—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        1. Health score (latency + error rate)
        2. Rate limiting
        3. Load balancing
        
        Returns:
            API –∫–ª—é—á –∏–ª–∏ None –µ—Å–ª–∏ –≤—Å–µ –∑–∞–Ω—è—Ç—ã
        """
        with self.lock:
            now = datetime.now()
            
            # –°–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â–∏—Ö rate limit)
            candidates = []
            
            for key in self.keys:
                stats = self.key_stats[key]
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—Å—Ç–∞—Ä—à–µ 1 –º–∏–Ω—É—Ç—ã)
                while stats["requests"] and (now - stats["requests"][0]) > timedelta(minutes=1):
                    stats["requests"].popleft()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç + health
                if len(stats["requests"]) < self.max_rpm and stats["is_healthy"]:
                    candidates.append((key, stats))
            
            if not candidates:
                return None  # –í—Å–µ –∫–ª—é—á–∏ –∑–∞–Ω—è—Ç—ã –∏–ª–∏ unhealthy
            
            # üöÄ –í—ã–±–∏—Ä–∞–µ–º key —Å –ª—É—á—à–∏–º health score
            best_key = max(candidates, key=lambda x: x[1]["health_score"])[0]
            
            # Update stats
            stats = self.key_stats[best_key]
            stats["requests"].append(now)
            stats["total_requests"] += 1
            stats["last_used"] = now
            
            return best_key
    
    def report_success(self, key: str, latency: float):
        """
        üöÄ NEW: Report successful request with latency
        
        Args:
            key: API key
            latency: Response time in seconds
        """
        if key in self.key_stats:
            stats = self.key_stats[key]
            stats["successful_requests"] += 1
            stats["latency_samples"].append(latency)
            
            # Update avg response time
            if stats["latency_samples"]:
                stats["avg_response_time"] = sum(stats["latency_samples"]) / len(stats["latency_samples"])
            
            # Update health score
            self._update_health_score(key)
    
    def report_error(self, key: str):
        """–û—Ç–º–µ—Ç–∏—Ç—å –æ—à–∏–±–∫—É –¥–ª—è –∫–ª—é—á–∞"""
        if key in self.key_stats:
            stats = self.key_stats[key]
            stats["errors"] += 1
            
            # Update health score
            self._update_health_score(key)
    
    def _update_health_score(self, key: str):
        """
        üöÄ NEW: Calculate health score –¥–ª—è key
        
        Health score (0-100):
        - Error rate weight: 50%
        - Latency weight: 50%
        """
        stats = self.key_stats[key]
        
        # Error rate (0-100, lower is better)
        total = stats["total_requests"]
        if total > 0:
            stats["error_rate"] = (stats["errors"] / total) * 100
            error_score = max(0, 100 - stats["error_rate"] * 10)  # Penalty: 10x error rate
        else:
            error_score = 100
        
        # Latency score (0-100, lower is better)
        if stats["latency_samples"]:
            avg_latency = stats["avg_response_time"]
            # Normalize: 1s = 100, 10s = 0
            latency_score = max(0, 100 - (avg_latency * 10))
        else:
            latency_score = 100
        
        # Combined health score
        stats["health_score"] = (error_score * 0.5 + latency_score * 0.5)
        
        # Mark as unhealthy if score too low
        stats["is_healthy"] = stats["health_score"] > 30  # Threshold: 30
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return {
            "total_keys": len(self.keys),
            "key_stats": self.key_stats,
            "total_requests": sum(s["total_requests"] for s in self.key_stats.values()),
            "total_errors": sum(s["errors"] for s in self.key_stats.values())
        }


class MLContextManager:
    """
    ML-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∫—ç—à–µ–º
    
    Features:
    - Semantic search –≤ –∫—ç—à–µ (–Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω—É–∂–Ω–æ—Å—Ç–∏ –∫—ç—à–∞
    - –£–º–Ω–∞—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è
    """
    
    def __init__(self, cache_dir: Path):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        if ML_AVAILABLE:
            self.vectorizer = TfidfVectorizer(max_features=500)
            self.is_fitted = False
            self.documents = []  # –î–ª—è –æ–±—É—á–µ–Ω–∏—è
            self.embeddings = []  # –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        else:
            self.vectorizer = None
    
    def fit_on_history(self, texts: List[str]):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ (–∑–∞–ø—Ä–æ—Å—ã, –æ—Ç–≤–µ—Ç—ã)
        """
        if not ML_AVAILABLE or not texts:
            return
        
        try:
            self.documents.extend(texts)
            
            # –û–±—É—á–∞–µ–º vectorizer –Ω–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            self.vectorizer.fit(self.documents)
            self.is_fitted = True
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
            self.embeddings = self.vectorizer.transform(self.documents).toarray()
            
            print(f"‚úÖ ML Context Manager trained on {len(self.documents)} documents")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  ML training failed: {e}")
    
    def find_similar(self, query: str, top_k: int = 3, threshold: float = 0.5) -> List[Tuple[int, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ similarity
        
        Returns:
            [(index, similarity_score), ...]
        """
        if not ML_AVAILABLE or not self.is_fitted:
            return []
        
        try:
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            query_vec = self.vectorizer.transform([query]).toarray()
            
            # –í—ã—á–∏—Å–ª—è–µ–º similarity —Å–æ –≤—Å–µ–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            similarities = cosine_similarity(query_vec, self.embeddings)[0]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É
            results = [
                (int(idx), float(similarities[idx]))
                for idx in top_indices
                if similarities[idx] >= threshold
            ]
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Similarity search failed: {e}")
            return []
    
    def predict_cache_utility(self, entry: CacheEntry) -> float:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –∫—ç—à-–∑–∞–ø–∏—Å–∏ (0-1)
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
        - –ß–∞—Å—Ç–æ—Ç—É –¥–æ—Å—Ç—É–ø–∞
        - –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        - –í–æ–∑—Ä–∞—Å—Ç –∑–∞–ø–∏—Å–∏
        
        Returns:
            Utility score (0-1)
        """
        now = datetime.now()
        
        # –í–æ–∑—Ä–∞—Å—Ç –∑–∞–ø–∏—Å–∏
        age_hours = (now - entry.timestamp).total_seconds() / 3600
        age_score = max(0, 1 - age_hours / 168)  # –°–Ω–∏–∂–∞–µ—Ç—Å—è –∑–∞ –Ω–µ–¥–µ–ª—é
        
        # –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        last_access_hours = (now - entry.last_access).total_seconds() / 3600
        recency_score = max(0, 1 - last_access_hours / 24)  # –°–Ω–∏–∂–∞–µ—Ç—Å—è –∑–∞ —Å—É—Ç–∫–∏
        
        # –ß–∞—Å—Ç–æ—Ç–∞ –¥–æ—Å—Ç—É–ø–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º)
        frequency_score = min(1.0, entry.access_count / 10)
        
        # Weighted average
        utility = (
            age_score * 0.2 +
            recency_score * 0.3 +
            frequency_score * 0.5
        )
        
        return utility
    
    def save_context_snapshot(self, snapshot: ContextSnapshot):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–Ω–∏–º–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫"""
        filename = self.cache_dir / f"context_{snapshot.timestamp.isoformat().replace(':', '-')}.pkl"
        
        with open(filename, 'wb') as f:
            pickle.dump(snapshot, f)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–Ω–∏–º–∫–∏ (—Ö—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10)
        snapshots = sorted(self.cache_dir.glob("context_*.pkl"))
        if len(snapshots) > 10:
            for old_snapshot in snapshots[:-10]:
                old_snapshot.unlink()
    
    def load_latest_context(self) -> Optional[ContextSnapshot]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–Ω–∏–º–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        snapshots = sorted(self.cache_dir.glob("context_*.pkl"))
        
        if not snapshots:
            return None
        
        with open(snapshots[-1], 'rb') as f:
            return pickle.load(f)


class IntelligentCache:
    """
    –£–º–Ω—ã–π –∫—ç—à —Å ML-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    
    Features:
    - Semantic search (–Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã)
    - LRU + ML-based eviction (üöÄ O(log n) —Å heap)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è
    - Persistence –Ω–∞ –¥–∏—Å–∫
    """
    
    def __init__(
        self,
        max_size: int = 1000,
        ttl_seconds: int = 3600,
        cache_dir: Path = None
    ):
        self.max_size = max_size
        self.ttl = timedelta(seconds=ttl_seconds)
        self.cache: Dict[str, CacheEntry] = {}
        self.lock = threading.Lock()
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Min-heap –¥–ª—è O(log n) eviction
        self.utility_heap: List[Tuple[float, str]] = []  # (utility, key)
        
        self.ml_manager = MLContextManager(
            cache_dir or Path("d:/bybit_strategy_tester_v2/.cache")
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.hits = 0
        self.misses = 0
        self.evictions = 0
    
    def _compute_key(self, data: Dict[str, Any]) -> str:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
        
        üöÄ QUICK WIN 3: Query normalization –¥–ª—è +3% cache hit rate
        """
        # Normalize query if present
        if "query" in data:
            # Lowercase + strip + collapse whitespace
            original_query = data["query"]
            normalized_query = " ".join(original_query.lower().strip().split())
            data = {**data, "query": normalized_query}
        
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        stable_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(stable_str.encode()).hexdigest()[:16]
    
    @staticmethod
    def normalize_query(query: str) -> str:
        """
        üöÄ QUICK WIN 3: Normalize query –¥–ª—è –ª—É—á—à–µ–≥–æ cache hit rate
        
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: lowercase + strip + collapse whitespace
        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –∫—ç—à –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–∑–Ω–æ–π –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π
        
        Args:
            query: Raw query string
            
        Returns:
            Normalized query
            
        Example:
            "  Analyze THIS  File  " ‚Üí "analyze this file"
        """
        return " ".join(query.lower().strip().split())
    
    def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞"""
        with self.lock:
            entry = self.cache.get(key)
            
            if entry is None:
                self.misses += 1
                return None
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ TTL
            if datetime.now() - entry.timestamp > self.ttl:
                del self.cache[key]
                self.misses += 1
                return None
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            entry.access_count += 1
            entry.last_access = datetime.now()
            self.hits += 1
            
            return entry.value
    
    def set(self, key: str, value: Any, text_for_ml: str = ""):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à"""
        with self.lock:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            if len(self.cache) >= self.max_size:
                self._evict()
            
            # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å
            entry = CacheEntry(
                key=key,
                value=value,
                timestamp=datetime.now()
            )
            
            # ML embedding –¥–ª—è semantic search
            if ML_AVAILABLE and text_for_ml:
                try:
                    if self.ml_manager.is_fitted:
                        vec = self.ml_manager.vectorizer.transform([text_for_ml]).toarray()[0]
                        entry.embedding = vec
                except:
                    pass
            
            self.cache[key] = entry
            
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –î–æ–±–∞–≤–ª—è–µ–º –≤ heap O(log n)
            utility = self.ml_manager.predict_cache_utility(entry)
            heapq.heappush(self.utility_heap, (utility, key))
    
    def find_similar(self, query: str, threshold: float = 0.8) -> List[Tuple[str, Any, float]]:
        """
        Semantic search –≤ –∫—ç—à–µ
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ similarity (0.8 –¥–ª—è better precision)
        
        Returns:
            [(key, value, similarity), ...]
        """
        if not ML_AVAILABLE or not self.ml_manager.is_fitted:
            return []
        
        results = []
        
        try:
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            query_vec = self.ml_manager.vectorizer.transform([query]).toarray()[0]
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤ –∫—ç—à–µ
            with self.lock:
                for key, entry in self.cache.items():
                    if entry.embedding is not None:
                        similarity = cosine_similarity(
                            [query_vec],
                            [entry.embedding]
                        )[0][0]
                        
                        if similarity >= threshold:
                            results.append((key, entry.value, float(similarity)))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
            results.sort(key=lambda x: x[2], reverse=True)
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Semantic search failed: {e}")
        
        return results
    
    def _evict(self):
        """
        üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Heap-based eviction O(log n)
        
        –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π utility –∏—Å–ø–æ–ª—å–∑—É—è min-heap.
        Complexity: O(k * log n) –≥–¥–µ k = eviction_count
        """
        with self.lock:
            if not self.cache:
                return
            
            # –£–¥–∞–ª—è–µ–º 10% —Å –Ω–∞–∏–º–µ–Ω—å—à–µ–π utility
            to_evict = max(1, int(len(self.cache) * 0.1))
            
            evicted_count = 0
            
            # Pop from heap (lowest utility first) O(log n)
            while evicted_count < to_evict and self.utility_heap:
                _, key = heapq.heappop(self.utility_heap)
                
                # Check if key still in cache (may have been already evicted)
                if key in self.cache:
                    del self.cache[key]
                    self.evictions += 1
                    evicted_count += 1
            
            # Rebuild heap if too many stale entries (lazy cleanup)
            if len(self.utility_heap) > len(self.cache) * 2:
                self._rebuild_heap()
    
    def _rebuild_heap(self):
        """Rebuild heap from current cache entries"""
        self.utility_heap = []
        for key, entry in self.cache.items():
            utility = self.ml_manager.predict_cache_utility(entry)
            heapq.heappush(self.utility_heap, (utility, key))
    
    def cleanup_expired(self) -> int:
        """
        üöÄ Wave 2 Priority 4: Periodic cleanup of expired entries
        
        Removes entries that exceeded TTL.
        Should be called periodically or when memory pressure detected.
        
        Returns:
            Number of expired entries removed
        """
        with self.lock:
            now = datetime.now()
            expired_keys = []
            
            for key, entry in self.cache.items():
                if now - entry.timestamp > self.ttl:
                    expired_keys.append(key)
            
            # Remove expired entries
            for key in expired_keys:
                del self.cache[key]
                self.evictions += 1
            
            # Rebuild heap if needed (lazy cleanup)
            if expired_keys and len(self.utility_heap) > len(self.cache) * 2:
                self._rebuild_heap()
            
            return len(expired_keys)
    
    def cleanup_low_utility(self, threshold: float = 0.3, max_removal_percent: float = 0.2) -> int:
        """
        üöÄ Wave 2 Priority 4: Remove low-utility cache entries
        
        Removes entries with utility score below threshold.
        Useful for memory pressure situations.
        
        Args:
            threshold: Utility score threshold (default 0.3)
            max_removal_percent: Max % of cache to remove (default 20%)
            
        Returns:
            Number of entries removed
        """
        with self.lock:
            if not self.cache:
                return 0
            
            # Calculate utility for all entries
            entries_with_utility = []
            for key, entry in self.cache.items():
                utility = self.ml_manager.predict_cache_utility(entry)
                entries_with_utility.append((utility, key))
            
            # Sort by utility (ascending)
            entries_with_utility.sort()
            
            # Find entries below threshold
            low_utility_keys = [key for utility, key in entries_with_utility if utility < threshold]
            
            # Limit removal to max_removal_percent
            max_removal = max(1, int(len(self.cache) * max_removal_percent))
            keys_to_remove = low_utility_keys[:max_removal]
            
            # Remove entries
            for key in keys_to_remove:
                del self.cache[key]
                self.evictions += 1
            
            # Rebuild heap
            if keys_to_remove:
                self._rebuild_heap()
            
            return len(keys_to_remove)
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0
        
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": f"{hit_rate:.1%}",
            "evictions": self.evictions,
            "ml_enabled": ML_AVAILABLE
        }


class ParallelDeepSeekExecutor:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π executor –¥–ª—è DeepSeek API —Å –ø—É–ª–æ–º –∫–ª—é—á–µ–π
    
    Features:
    - –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ 4-8 –∑–∞–ø—Ä–æ—Å–æ–≤
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π load balancing
    - Retry —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏
    - –£–º–Ω—ã–π –∫—ç—à
    """
    
    def __init__(
        self,
        api_keys: List[str],
        cache: IntelligentCache,
        max_workers: int = None,
        enable_memory_monitoring: bool = True
    ):
        """
        Args:
            api_keys: –°–ø–∏—Å–æ–∫ API –∫–ª—é—á–µ–π (4-8)
            cache: –£–º–Ω—ã–π –∫—ç—à
            max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (default: len(api_keys))
            enable_memory_monitoring: Enable memory leak detection (Wave 2 Priority 4)
        """
        self.key_pool = APIKeyPool(api_keys)
        self.cache = cache
        self.max_workers = max_workers or len(api_keys)
        
        # üöÄ Wave 2 Priority 4: Memory monitoring
        self.memory_monitor = MemoryMonitor(
            warning_threshold_mb=500,
            critical_threshold_mb=1000
        ) if enable_memory_monitoring else None
        self.operations_count = 0
        self.memory_check_interval = 50  # Check every 50 operations
        
        # Thread pool –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        
        print(f"üöÄ Parallel DeepSeek Executor initialized:")
        print(f"   ‚Ä¢ API Keys: {len(api_keys)}")
        print(f"   ‚Ä¢ Max Workers: {self.max_workers}")
        print(f"   ‚Ä¢ Memory Monitoring: {'‚úÖ Enabled' if enable_memory_monitoring else '‚ùå Disabled'}")
        print(f"   ‚Ä¢ Expected speedup: {self.max_workers}x")
    
    async def execute_batch(
        self,
        requests: List[Dict[str, Any]],
        use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ batch –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Args:
            requests: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ)
        """
        # üöÄ Wave 2 Priority 4: Memory monitoring
        self._check_memory_periodic()
        
        results = []
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏
        tasks = []
        for i, request in enumerate(requests):
            task = self._execute_single(i, request, use_cache)
            tasks.append(task)
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        completed = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
        sorted_results = sorted(completed, key=lambda x: x[0] if isinstance(x, tuple) else 999)
        
        return [result[1] for result in sorted_results if isinstance(result, tuple)]
    
    def _check_memory_periodic(self):
        """
        üöÄ Wave 2 Priority 4: Periodic memory check
        
        Checks memory every N operations and triggers cleanup if needed.
        """
        if not self.memory_monitor:
            return
        
        self.operations_count += 1
        
        # Check every N operations
        if self.operations_count % self.memory_check_interval == 0:
            mem_stats = self.memory_monitor.check_memory()
            
            if mem_stats["status"] == "warning":
                logger.warning(f"‚ö†Ô∏è  Memory warning: {mem_stats['current_mb']:.1f}MB (growth: {mem_stats['growth_percent']:.1f}%)")
                
                # Cleanup expired entries
                expired_count = self.cache.cleanup_expired()
                logger.info(f"üßπ Cleaned up {expired_count} expired cache entries")
            
            elif mem_stats["status"] == "critical":
                logger.error(f"üö® Memory critical: {mem_stats['current_mb']:.1f}MB (growth: {mem_stats['growth_percent']:.1f}%)")
                
                # Aggressive cleanup
                expired_count = self.cache.cleanup_expired()
                low_utility_count = self.cache.cleanup_low_utility(threshold=0.3, max_removal_percent=0.2)
                
                # Force garbage collection
                cleanup_stats = self.memory_monitor.cleanup()
                
                logger.info(f"üßπ Emergency cleanup:")
                logger.info(f"   ‚Ä¢ Expired entries: {expired_count}")
                logger.info(f"   ‚Ä¢ Low utility entries: {low_utility_count}")
                logger.info(f"   ‚Ä¢ Memory freed: {cleanup_stats['freed_mb']:.1f}MB")
                logger.info(f"   ‚Ä¢ Objects collected: {cleanup_stats['objects_collected']}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """
        üöÄ Wave 2 Priority 4: Get memory statistics
        
        Returns:
            Memory usage and trend statistics
        """
        if not self.memory_monitor:
            return {"enabled": False}
        
        return self.memory_monitor.get_stats()
    
    async def _execute_single(
        self,
        index: int,
        request: Dict[str, Any],
        use_cache: bool
    ) -> Tuple[int, Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–µ–º
        
        Returns:
            (index, result)
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if use_cache:
            cache_key = self.cache._compute_key(request)
            cached = self.cache.get(cache_key)
            
            if cached is not None:
                return (index, {
                    **cached,
                    "cached": True,
                    "index": index
                })
            
            # Semantic search
            query_text = request.get("query", "")
            similar = self.cache.find_similar(query_text, threshold=0.85)
            
            if similar:
                _, value, similarity = similar[0]
                print(f"üîç Found similar cached result (similarity: {similarity:.2%})")
                return (index, {
                    **value,
                    "cached": True,
                    "semantic_match": True,
                    "similarity": similarity,
                    "index": index
                })
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        api_key = self.key_pool.get_available_key()
        
        if api_key is None:
            # –í—Å–µ –∫–ª—é—á–∏ –∑–∞–Ω—è—Ç—ã, –∂–¥—ë–º
            await asyncio.sleep(1)
            return await self._execute_single(index, request, use_cache)
        
        try:
            # –†–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ API
            # (–∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥ –≤—ã–∑–æ–≤–∞ DeepSeek API)
            result = await self._call_deepseek_api(api_key, request)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if use_cache:
                self.cache.set(
                    cache_key,
                    result,
                    text_for_ml=request.get("query", "")
                )
            
            return (index, {
                **result,
                "cached": False,
                "index": index
            })
            
        except Exception as e:
            self.key_pool.report_error(api_key)
            print(f"‚ö†Ô∏è  Request {index} failed with key {api_key[:8]}...: {e}")
            
            # Retry —Å –¥—Ä—É–≥–∏–º –∫–ª—é—á–æ–º
            return await self._execute_single(index, request, use_cache)
    
    async def _call_deepseek_api(
        self,
        api_key: str,
        request: Dict[str, Any],
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        Real DeepSeek API call with retry logic
        
        üöÄ Wave 2 Priority 3: Retry —Å failover –Ω–∞ –¥—Ä—É–≥–æ–π key
        
        Args:
            api_key: DeepSeek API key
            request: Request with 'query', 'model', etc.
            max_retries: Max retry attempts with different keys
        
        Returns:
            Response dict with success, response, usage
        """
        import time
        
        for attempt in range(max_retries + 1):
            try:
                start_time = time.time()
                
                # Create client
                client = DeepSeekClient(api_key=api_key, timeout=60.0)
                
                # Prepare messages
                query = request.get("query", "")
                messages = [{"role": "user", "content": query}]
                
                # Get model and parameters from request
                model = request.get("model", "deepseek-coder")
                temperature = request.get("temperature", 0.1)
                max_tokens = request.get("max_tokens", 4000)
                
                # Call API
                result = await client.chat_completion(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                latency = time.time() - start_time
                
                # üöÄ Report success to key pool
                if result.get("success"):
                    self.key_pool.report_success(api_key, latency)
                
                # Add metadata
                result["api_key_used"] = api_key[:8] + "..."
                result["request_query"] = query[:100]
                result["latency"] = latency
                result["attempt"] = attempt + 1
                
                return result
                
            except DeepSeekAPIError as e:
                logger.error(f"‚ùå DeepSeek API error (attempt {attempt + 1}/{max_retries + 1}): {e}")
                
                # üöÄ Report error to key pool
                self.key_pool.report_error(api_key)
                
                # üöÄ Try failover to another key
                if attempt < max_retries:
                    new_key = self.key_pool.get_available_key()
                    if new_key and new_key != api_key:
                        logger.info(f"‚ôªÔ∏è Failover: trying different key...")
                        api_key = new_key
                        await asyncio.sleep(1)  # Brief delay
                        continue
                
                # Final attempt failed
                return {
                    "success": False,
                    "error": str(e),
                    "api_key_used": api_key[:8] + "...",
                    "response": "",
                    "attempt": attempt + 1
                }
            
            except Exception as e:
                logger.error(f"‚ùå Unexpected error: {e}")
                self.key_pool.report_error(api_key)
                
                # Try failover
                if attempt < max_retries:
                    new_key = self.key_pool.get_available_key()
                    if new_key and new_key != api_key:
                        api_key = new_key
                        await asyncio.sleep(1)
                        continue
                
                return {
                    "success": False,
                    "error": f"Unexpected: {str(e)}",
                    "api_key_used": api_key[:8] + "...",
                    "response": "",
                    "attempt": attempt + 1
                }


class AdvancedWorkflowOrchestrator:
    """
    –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä workflow: DeepSeek ‚Üí Perplexity ‚Üí DeepSeek ‚Üí Copilot
    
    Features:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ
    - –£–º–Ω—ã–π –∫—ç—à —Å ML
    - Context management
    - Automatic retry and failover
    """
    
    def __init__(
        self,
        deepseek_keys: List[str],
        perplexity_key: str,
        cache_dir: Path = None
    ):
        """
        Args:
            deepseek_keys: 4-8 API –∫–ª—é—á–µ–π DeepSeek
            perplexity_key: API –∫–ª—é—á Perplexity
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞
        """
        cache_path = cache_dir or Path("d:/bybit_strategy_tester_v2/.cache")
        cache_path.mkdir(parents=True, exist_ok=True)
        
        # –£–º–Ω—ã–π –∫—ç—à
        self.cache = IntelligentCache(
            max_size=1000,
            ttl_seconds=3600,
            cache_dir=cache_path
        )
        
        # Parallel executor –¥–ª—è DeepSeek
        self.deepseek_executor = ParallelDeepSeekExecutor(
            api_keys=deepseek_keys,
            cache=self.cache,
            max_workers=len(deepseek_keys)
        )
        
        # Perplexity client
        self.perplexity_key = perplexity_key
        self.perplexity_client = PerplexityClient(api_key=perplexity_key) if perplexity_key else None
        
        # Context management
        self.ml_manager = self.cache.ml_manager
        self.context_history: List[ContextSnapshot] = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        latest_context = self.ml_manager.load_latest_context()
        if latest_context:
            self.context_history.append(latest_context)
            print(f"‚úÖ Loaded context from {latest_context.timestamp}")
        
        print(f"\nüéØ Advanced Workflow Orchestrator initialized:")
        print(f"   ‚Ä¢ DeepSeek Keys: {len(deepseek_keys)}")
        print(f"   ‚Ä¢ Cache: Intelligent with ML")
        print(f"   ‚Ä¢ Context: {len(self.context_history)} snapshots loaded")
    
    async def execute_workflow(
        self,
        tasks: List[Dict[str, Any]],
        save_context: bool = True
    ) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ workflow
        
        Workflow:
        1. DeepSeek (initial analysis) - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        2. Perplexity (research) - –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        3. DeepSeek (refinement) - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        4. Copilot (validation) - –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        
        Args:
            tasks: –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            save_context: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã workflow
        """
        start_time = datetime.now()
        results = {
            "workflow_id": hashlib.md5(str(start_time).encode()).hexdigest()[:8],
            "start_time": start_time.isoformat(),
            "stages": {}
        }
        
        print(f"\n{'='*80}")
        print(f"üöÄ Starting Advanced Workflow")
        print(f"{'='*80}")
        print(f"Tasks: {len(tasks)}")
        print(f"Expected speedup: {len(self.deepseek_executor.key_pool.keys)}x")
        
        # Stage 1: DeepSeek Initial Analysis (Parallel)
        print(f"\n1Ô∏è‚É£ Stage 1: DeepSeek Initial Analysis...")
        stage1_start = datetime.now()
        
        stage1_results = await self.deepseek_executor.execute_batch(
            requests=tasks,
            use_cache=True
        )
        
        stage1_duration = (datetime.now() - stage1_start).total_seconds()
        print(f"‚úÖ Stage 1 completed in {stage1_duration:.2f}s")
        print(f"   ‚Ä¢ Results: {len(stage1_results)}")
        print(f"   ‚Ä¢ Cached: {sum(1 for r in stage1_results if r.get('cached'))}")
        
        results["stages"]["stage1_deepseek"] = {
            "duration": stage1_duration,
            "results": stage1_results,
            "cached_count": sum(1 for r in stage1_results if r.get("cached"))
        }
        
        # Stage 2: Perplexity Research (if needed)
        needs_research = any(r.get("needs_research", False) for r in stage1_results)
        
        if needs_research and self.perplexity_client:
            print(f"\n2Ô∏è‚É£ Stage 2: Perplexity Research...")
            stage2_start = datetime.now()
            
            # Collect queries that need research
            research_queries = [
                r.get("response", "")[:500]  # First 500 chars as query
                for r in stage1_results
                if r.get("needs_research", False)
            ]
            
            # Execute Perplexity research
            research_results = []
            for query in research_queries:
                try:
                    result = await self.perplexity_client.search(query, model="sonar-pro")
                    research_results.append(result)
                    print(f"   ‚úÖ Research completed: {len(result.get('sources', []))} sources")
                except PerplexityAPIError as e:
                    logger.error(f"‚ùå Perplexity error: {e}")
                    research_results.append({
                        "success": False,
                        "error": str(e),
                        "response": ""
                    })
            
            stage2_duration = (datetime.now() - stage2_start).total_seconds()
            print(f"‚úÖ Stage 2 completed in {stage2_duration:.2f}s")
            
            results["stages"]["stage2_perplexity"] = {
                "duration": stage2_duration,
                "results": research_results,
                "queries_count": len(research_queries)
            }
        else:
            if not self.perplexity_client:
                print(f"\n2Ô∏è‚É£ Stage 2: Perplexity Research...")
                print("   ‚è≠Ô∏è  Skipped (Perplexity client not configured)")
            else:
                print(f"\n2Ô∏è‚É£ Stage 2: Perplexity Research...")
                print("   ‚è≠Ô∏è  Skipped (no research needed)")
        
        # Stage 3: DeepSeek Refinement (Parallel)
        print(f"\n3Ô∏è‚É£ Stage 3: DeepSeek Refinement...")
        stage3_start = datetime.now()
        
        # –°–æ–∑–¥–∞—ë–º refined –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ stage1
        refined_requests = [
            {
                "query": f"Refine: {r.get('response', '')}",
                "context": r
            }
            for r in stage1_results
        ]
        
        stage3_results = await self.deepseek_executor.execute_batch(
            requests=refined_requests,
            use_cache=True
        )
        
        stage3_duration = (datetime.now() - stage3_start).total_seconds()
        print(f"‚úÖ Stage 3 completed in {stage3_duration:.2f}s")
        
        results["stages"]["stage3_deepseek_refine"] = {
            "duration": stage3_duration,
            "results": stage3_results
        }
        
        # Stage 4: Copilot Validation (if needed)
        print(f"\n4Ô∏è‚É£ Stage 4: Copilot Validation...")
        print("   ‚è≠Ô∏è  Skipped (file-based integration)")
        
        # Finalize
        total_duration = (datetime.now() - start_time).total_seconds()
        results["total_duration"] = total_duration
        results["end_time"] = datetime.now().isoformat()
        
        print(f"\n{'='*80}")
        print(f"‚úÖ Workflow Completed!")
        print(f"{'='*80}")
        print(f"Total duration: {total_duration:.2f}s")
        print(f"Cache stats: {self.cache.get_stats()}")
        print(f"API Key pool stats: {self.deepseek_executor.key_pool.get_stats()}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if save_context:
            snapshot = ContextSnapshot(
                timestamp=datetime.now(),
                conversation_history=[r for stage in results["stages"].values() for r in stage.get("results", [])],
                learned_patterns={},  # TODO: Extract patterns
                quality_metrics={},  # TODO: Calculate metrics
                project_state={}  # TODO: Project state
            )
            
            self.context_history.append(snapshot)
            self.ml_manager.save_context_snapshot(snapshot)
            print(f"üíæ Context saved")
        
        return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def demo_advanced_architecture():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è advanced architecture
    """
    print("\n" + "="*80)
    print("üéØ DEMO: Advanced Enterprise Architecture")
    print("="*80)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–µ–π (4-8)
    deepseek_keys = [
        "key1_mock_for_demo",
        "key2_mock_for_demo",
        "key3_mock_for_demo",
        "key4_mock_for_demo",
    ]
    
    perplexity_key = "perplexity_key_mock"
    
    # –°–æ–∑–¥–∞—ë–º orchestrator
    orchestrator = AdvancedWorkflowOrchestrator(
        deepseek_keys=deepseek_keys,
        perplexity_key=perplexity_key
    )
    
    # –°–æ–∑–¥–∞—ë–º batch –∑–∞–¥–∞—á (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10 —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
    tasks = [
        {"query": f"Analyze file_{i}.py for bugs", "file": f"file_{i}.py"}
        for i in range(10)
    ]
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
    results = await orchestrator.execute_workflow(tasks)
    
    print(f"\nüìä Results:")
    print(json.dumps(results, indent=2, default=str))


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ
    asyncio.run(demo_advanced_architecture())
