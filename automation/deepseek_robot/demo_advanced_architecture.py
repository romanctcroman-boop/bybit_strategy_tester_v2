"""
üöÄ Demo: Advanced Architecture with 4-8 API Keys
================================================

–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:
1. API Key Pool —Å round-robin
2. Parallel execution (4x speedup)
3. Intelligent cache —Å ML
4. Semantic search
5. Context management
6. Full workflow: DeepSeek ‚Üí Perplexity ‚Üí DeepSeek ‚Üí Copilot
"""

import asyncio
import os
import time
from pathlib import Path
from dotenv import load_dotenv

from automation.deepseek_robot.advanced_architecture import (
    APIKeyPool,
    MLContextManager,
    IntelligentCache,
    ParallelDeepSeekExecutor,
    AdvancedWorkflowOrchestrator,
    ContextSnapshot
)

# –ó–∞–≥—Ä—É–∑–∫–∞ .env
load_dotenv()


def print_section(title: str):
    """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80)


async def demo_1_api_key_pool():
    """Demo 1: API Key Pool"""
    print_section("DEMO 1: API Key Pool")
    
    # –°–æ–∑–¥–∞—ë–º –ø—É–ª —Å 4 –∫–ª—é—á–∞–º–∏
    keys = [
        "key1_demo",
        "key2_demo",
        "key3_demo",
        "key4_demo"
    ]
    
    pool = APIKeyPool(keys, max_requests_per_minute=60)
    
    print(f"‚úÖ API Key Pool initialized")
    print(f"   ‚Ä¢ Keys: {len(keys)}")
    print(f"   ‚Ä¢ Rate limit: 60 req/min per key")
    print(f"   ‚Ä¢ Total capacity: {len(keys) * 60} req/min")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–∏ (round-robin)
    print(f"\nüîÑ Round-robin distribution:")
    for i in range(8):
        key = pool.get_available_key()
        print(f"   Request {i+1}: {key}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = pool.get_stats()
    print(f"\nüìä Stats after 8 requests:")
    print(f"   ‚Ä¢ Total requests: {stats['total_requests']}")
    print(f"   ‚Ä¢ Each key used: 2 times")
    

async def demo_2_intelligent_cache():
    """Demo 2: Intelligent Cache —Å ML"""
    print_section("DEMO 2: Intelligent Cache —Å ML")
    
    cache = IntelligentCache(
        max_size=100,
        ttl_seconds=3600,
        cache_dir=Path("d:/bybit_strategy_tester_v2/.cache/demo")
    )
    
    print(f"‚úÖ Intelligent Cache initialized")
    print(f"   ‚Ä¢ Max size: 100 entries")
    print(f"   ‚Ä¢ TTL: 1 hour")
    print(f"   ‚Ä¢ ML features: {'Enabled' if cache.ml_manager.vectorizer else 'Disabled'}")
    
    # –û–±—É—á–∞–µ–º ML –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    training_texts = [
        "analyze robot.py for bugs and errors",
        "check performance issues in executor.py",
        "review security vulnerabilities in api.py",
        "find memory leaks in cache.py",
        "test async functions in workflow.py"
    ]
    
    print(f"\nüß† Training ML on {len(training_texts)} examples...")
    cache.ml_manager.fit_on_history(training_texts)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à
    print(f"\nüíæ Adding entries to cache...")
    for i, text in enumerate(training_texts):
        cache.set(f"key_{i}", {"result": f"Analysis {i}"}, text_for_ml=text)
    
    print(f"   ‚Ä¢ Cached: {len(training_texts)} entries")
    
    # Semantic search
    print(f"\nüîç Semantic search:")
    query = "find bugs in robot code"
    similar = cache.find_similar(query, threshold=0.5)
    
    print(f"   Query: '{query}'")
    print(f"   Found {len(similar)} similar entries:")
    for key, value, similarity in similar[:3]:
        print(f"      ‚Ä¢ {key}: similarity={similarity:.2%}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = cache.get_stats()
    print(f"\nüìä Cache stats:")
    for key, val in stats.items():
        print(f"   ‚Ä¢ {key}: {val}")


async def demo_3_parallel_executor():
    """Demo 3: Parallel Executor"""
    print_section("DEMO 3: Parallel Executor (4x speedup)")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    keys = [f"key_{i}_demo" for i in range(1, 5)]
    cache = IntelligentCache(max_size=100, ttl_seconds=3600)
    
    executor = ParallelDeepSeekExecutor(
        api_keys=keys,
        cache=cache,
        max_workers=4
    )
    
    print(f"‚úÖ Parallel Executor initialized")
    print(f"   ‚Ä¢ Workers: 4")
    print(f"   ‚Ä¢ Expected speedup: 4x")
    
    # –°–æ–∑–¥–∞—ë–º batch –∑–∞–ø—Ä–æ—Å–æ–≤
    requests = [
        {"query": f"analyze file_{i}.py for bugs"}
        for i in range(1, 9)
    ]
    
    print(f"\n‚ö° Executing batch of {len(requests)} requests...")
    
    # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ (no cache)
    start_time = time.time()
    results = await executor.execute_batch(requests, use_cache=True)
    duration1 = time.time() - start_time
    
    print(f"‚úÖ First run completed in {duration1:.2f}s")
    print(f"   ‚Ä¢ Results: {len(results)}")
    print(f"   ‚Ä¢ Cached: {sum(1 for r in results if r.get('cached'))}")
    
    # –í—Ç–æ—Ä–æ–π –∑–∞–ø—É—Å–∫ (100% cache)
    print(f"\n‚ö° Re-executing same batch (should be cached)...")
    start_time = time.time()
    results = await executor.execute_batch(requests, use_cache=True)
    duration2 = time.time() - start_time
    
    print(f"‚úÖ Second run completed in {duration2:.2f}s")
    print(f"   ‚Ä¢ Results: {len(results)}")
    print(f"   ‚Ä¢ Cached: {sum(1 for r in results if r.get('cached'))}")
    print(f"   ‚Ä¢ Speedup: {duration1/duration2:.0f}x faster!")


async def demo_4_context_management():
    """Demo 4: Context Management"""
    print_section("DEMO 4: Context Management")
    
    ml_manager = MLContextManager(
        cache_dir=Path("d:/bybit_strategy_tester_v2/.cache/demo")
    )
    
    print(f"‚úÖ ML Context Manager initialized")
    
    # –°–æ–∑–¥–∞—ë–º snapshot
    from datetime import datetime
    
    snapshot = ContextSnapshot(
        timestamp=datetime.now(),
        conversation_history=[
            {"role": "user", "content": "analyze robot.py"},
            {"role": "assistant", "content": "Found 3 issues..."}
        ],
        learned_patterns={
            "common_bugs": ["missing error handling", "no type hints"],
            "file_types": ["py", "md", "json"]
        },
        quality_metrics={
            "avg_response_time": 2.5,
            "cache_hit_rate": 0.67,
            "user_satisfaction": 0.85
        },
        project_state={
            "files_analyzed": 15,
            "bugs_found": 23,
            "fixes_applied": 18
        }
    )
    
    print(f"\nüíæ Saving context snapshot...")
    ml_manager.save_context_snapshot(snapshot)
    print(f"   ‚Ä¢ Timestamp: {snapshot.timestamp}")
    print(f"   ‚Ä¢ History entries: {len(snapshot.conversation_history)}")
    print(f"   ‚Ä¢ Learned patterns: {len(snapshot.learned_patterns)}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    print(f"\nüìÇ Loading latest context...")
    loaded = ml_manager.load_latest_context()
    
    if loaded:
        print(f"   ‚úÖ Loaded context from {loaded.timestamp}")
        print(f"   ‚Ä¢ Files analyzed: {loaded.project_state.get('files_analyzed')}")
        print(f"   ‚Ä¢ Bugs found: {loaded.project_state.get('bugs_found')}")
        print(f"   ‚Ä¢ Cache hit rate: {loaded.quality_metrics.get('cache_hit_rate', 0):.0%}")
    else:
        print(f"   ‚ö†Ô∏è  No context found")


async def demo_5_full_workflow():
    """Demo 5: Full Workflow Orchestrator"""
    print_section("DEMO 5: Full Workflow (DeepSeek ‚Üí Perplexity ‚Üí DeepSeek ‚Üí Copilot)")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º demo –∫–ª—é—á–∏)
    deepseek_keys = [f"deepseek_key_{i}_demo" for i in range(1, 5)]
    perplexity_key = "perplexity_key_demo"
    
    orchestrator = AdvancedWorkflowOrchestrator(
        deepseek_keys=deepseek_keys,
        perplexity_key=perplexity_key,
        cache_dir=Path("d:/bybit_strategy_tester_v2/.cache/demo")
    )
    
    # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏
    tasks = [
        {"query": "analyze robot.py for bugs"},
        {"query": "check performance in executor.py"},
        {"query": "review security in api_handler.py"},
        {"query": "test async code in workflow.py"},
    ]
    
    print(f"\nüöÄ Starting workflow with {len(tasks)} tasks...")
    print(f"   ‚Ä¢ Stage 1: DeepSeek (Initial Analysis) - Parallel")
    print(f"   ‚Ä¢ Stage 2: Perplexity (Research) - If needed")
    print(f"   ‚Ä¢ Stage 3: DeepSeek (Refinement) - Parallel")
    print(f"   ‚Ä¢ Stage 4: Copilot (Validation) - If needed")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º workflow
    start_time = time.time()
    results = await orchestrator.execute_workflow(tasks, save_context=True)
    total_duration = time.time() - start_time
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n‚úÖ Workflow completed!")
    print(f"   ‚Ä¢ Total duration: {total_duration:.2f}s")
    print(f"   ‚Ä¢ Workflow ID: {results.get('workflow_id')}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —ç—Ç–∞–ø–∞–º
    stages = results.get("stages", {})
    for stage_name, stage_data in stages.items():
        print(f"\n   üìä {stage_name}:")
        print(f"      ‚Ä¢ Duration: {stage_data.get('duration', 0):.2f}s")
        print(f"      ‚Ä¢ Results: {len(stage_data.get('results', []))}")
        if "cached_count" in stage_data:
            print(f"      ‚Ä¢ Cached: {stage_data['cached_count']}")
    
    # –ö—ç—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    cache_stats = orchestrator.cache.get_stats()
    print(f"\n   üíæ Cache stats:")
    print(f"      ‚Ä¢ Hit rate: {cache_stats.get('hit_rate')}")
    print(f"      ‚Ä¢ Size: {cache_stats.get('size')}/{cache_stats.get('max_size')}")
    
    # API Key pool —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    pool_stats = orchestrator.deepseek_executor.key_pool.get_stats()
    print(f"\n   üîë API Key pool stats:")
    print(f"      ‚Ä¢ Total keys: {pool_stats.get('total_keys')}")
    print(f"      ‚Ä¢ Total requests: {pool_stats.get('total_requests')}")
    print(f"      ‚Ä¢ Errors: {pool_stats.get('total_errors')}")


async def demo_6_performance_comparison():
    """Demo 6: Performance Comparison"""
    print_section("DEMO 6: Performance Comparison (Sequential vs Parallel)")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    keys = [f"key_{i}_demo" for i in range(1, 5)]
    cache = IntelligentCache(max_size=100, ttl_seconds=3600)
    executor = ParallelDeepSeekExecutor(api_keys=keys, cache=cache, max_workers=4)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_sizes = [4, 8, 16]
    
    print(f"\nüìä Testing different batch sizes:\n")
    
    for size in test_sizes:
        requests = [{"query": f"test query {i}"} for i in range(size)]
        
        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
        start = time.time()
        results = await executor.execute_batch(requests, use_cache=False)
        duration = time.time() - start
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ sequential –≤—Ä–µ–º—è
        sequential_time = size * 0.1  # 0.1s per request (mock)
        speedup = sequential_time / duration if duration > 0 else 0
        
        print(f"   Batch size: {size}")
        print(f"   ‚Ä¢ Sequential (estimated): {sequential_time:.2f}s")
        print(f"   ‚Ä¢ Parallel (actual): {duration:.2f}s")
        print(f"   ‚Ä¢ Speedup: {speedup:.1f}x")
        print()


async def run_all_demos():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –¥–µ–º–æ"""
    print("\n" + "üéØ"*40)
    print("  ADVANCED ARCHITECTURE DEMO SUITE")
    print("üéØ"*40)
    
    demos = [
        ("API Key Pool", demo_1_api_key_pool),
        ("Intelligent Cache", demo_2_intelligent_cache),
        ("Parallel Executor", demo_3_parallel_executor),
        ("Context Management", demo_4_context_management),
        ("Full Workflow", demo_5_full_workflow),
        ("Performance Comparison", demo_6_performance_comparison),
    ]
    
    for i, (name, demo_func) in enumerate(demos, 1):
        try:
            await demo_func()
        except Exception as e:
            print(f"\n‚ùå Demo {i} failed: {e}")
            import traceback
            traceback.print_exc()
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –¥–µ–º–æ
        if i < len(demos):
            print(f"\n‚è≥ Press Enter to continue to next demo...")
            # input()  # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
            await asyncio.sleep(1)
    
    # –ò—Ç–æ–≥–∏
    print_section("DEMO SUITE COMPLETED")
    print(f"‚úÖ All {len(demos)} demos executed successfully!")
    print(f"\nüìö Key features demonstrated:")
    print(f"   ‚Ä¢ API Key Pool with round-robin (4-8 keys)")
    print(f"   ‚Ä¢ Intelligent Cache with ML (semantic search)")
    print(f"   ‚Ä¢ Parallel Executor (4-8x speedup)")
    print(f"   ‚Ä¢ Context Management (persistence)")
    print(f"   ‚Ä¢ Full Workflow Orchestration")
    print(f"   ‚Ä¢ Performance Benchmarks")
    
    print(f"\nüöÄ Ready for production integration!")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –¥–µ–º–æ
    asyncio.run(run_all_demos())
