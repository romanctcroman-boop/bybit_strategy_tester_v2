"""
Test Watcher - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ DeepSeek AI (–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø)

–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- ‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ API –∫–ª—é—á–∞–º–∏
- üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏
- üíæ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø—Ä–æ—Å–æ–≤
- üìà 20-30x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    File Changes ‚Üí Test Runner ‚Üí Parallel DeepSeek Client ‚Üí AI Analysis
                                     ‚Üì
                               (4 API keys, 10 concurrent)
                                     ‚Üì
                               Results + Statistics
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Set
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import pytest
import coverage
from loguru import logger
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
project_root = Path(__file__).parent.parent.parent
env_path = project_root / '.env'
load_dotenv(env_path)

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å KeyManager –∏ ParallelDeepSeekClient
sys.path.insert(0, str(project_root / "automation" / "task2_key_manager"))
sys.path.insert(0, str(project_root))
from key_manager import KeyManager
from backend.api.parallel_deepseek_client import (
    ParallelDeepSeekClient,
    DeepSeekTask,
    TaskPriority
)


class ParallelTestWatcher:
    """
    –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —á–µ—Ä–µ–∑ AI.
    
    –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 20-30x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    """
    
    def __init__(self, watch_path: str = ".", debounce_seconds: int = 20):
        self.watch_path = Path(watch_path).resolve()
        self.debounce_seconds = debounce_seconds
        self.observer = Observer()
        self.changed_files: Set[Path] = set()
        self.last_change_time = 0
        self.processing = False
        self.loop = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è KeyManager
        self.key_manager = KeyManager()
        self._init_parallel_client()
        
        # Results directory
        self.results_dir = project_root / "ai_audit_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # Configure logging
        logs_dir = project_root / "logs"
        logs_dir.mkdir(exist_ok=True)
        log_file = logs_dir / "test_watcher_parallel.log"
        logger.add(str(log_file), rotation="10 MB", level="INFO")
    
    def _init_parallel_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ DeepSeek –∫–ª–∏–µ–Ω—Ç–∞"""
        encryption_key = os.getenv("ENCRYPTION_KEY")
        
        if encryption_key and self.key_manager.initialize_encryption(encryption_key):
            secrets_file = project_root / "encrypted_secrets.json"
            if secrets_file.exists() and self.key_manager.load_keys(str(secrets_file)):
                # –ü–æ–ª—É—á–∞–µ–º –í–°–ï DeepSeek –∫–ª—é—á–∏
                api_keys = self.key_manager.get_all_keys("DEEPSEEK_API_KEY")
                
                if api_keys:
                    logger.info(f"[OK] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(api_keys)} DeepSeek API –∫–ª—é—á–µ–π")
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
                    self.parallel_client = ParallelDeepSeekClient(
                        api_keys=api_keys,
                        max_concurrent=min(10, len(api_keys) * 3),  # 3 –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∫–ª—é—á
                        enable_cache=True,
                        cache_ttl=3600  # 1 —á–∞—Å
                    )
                    
                    logger.info(
                        f"[OK] ParallelDeepSeekClient –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: "
                        f"{len(api_keys)} keys, max_concurrent={self.parallel_client.max_concurrent}"
                    )
                    return
        
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –∫–ª—é—á –∏–∑ .env
        logger.warning("[WARN] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Ä–µ–∂–∏–º —Å –æ–¥–Ω–∏–º API –∫–ª—é—á–æ–º")
        single_key = os.getenv("DEEPSEEK_API_KEY")
        
        if single_key:
            self.parallel_client = ParallelDeepSeekClient(
                api_keys=[single_key],
                max_concurrent=3,
                enable_cache=True
            )
            logger.info("[WARN] ParallelDeepSeekClient —Ä–∞–±–æ—Ç–∞–µ—Ç —Å 1 –∫–ª—é—á–æ–º (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
        else:
            logger.error("[ERROR] DEEPSEEK_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
            self.parallel_client = None
    
    class ChangeHandler(FileSystemEventHandler):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–±—ã—Ç–∏–π –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤"""
        
        def __init__(self, watcher):
            self.watcher = watcher
            
        def on_modified(self, event):
            if not event.is_directory:
                file_path = Path(event.src_path)
                if file_path.suffix in ['.py', '.pyx', '.pyi']:
                    self.watcher.handle_file_change(file_path)
                    
        def on_created(self, event):
            if not event.is_directory:
                file_path = Path(event.src_path)
                if file_path.suffix in ['.py', '.pyx', '.pyi']:
                    self.watcher.handle_file_change(file_path)
    
    def handle_file_change(self, file_path: Path):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞"""
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –∏–∑ .venv –∏ __pycache__
        if '.venv' in file_path.parts or '__pycache__' in file_path.parts:
            return
        
        self.changed_files.add(file_path)
        self.last_change_time = time.time()
        
        logger.info(f"File changed: {file_path.name}")
        
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ debounce –ø–µ—Ä–∏–æ–¥
        if self.loop and not self.processing:
            asyncio.run_coroutine_threadsafe(
                self.schedule_processing(),
                self.loop
            )
    
    async def schedule_processing(self):
        """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å debounce"""
        while True:
            await asyncio.sleep(1)
            
            if (not self.processing and 
                self.changed_files and 
                (time.time() - self.last_change_time) >= self.debounce_seconds):
                
                await self.process_changes()
                break
    
    async def process_changes(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º"""
        if self.processing or not self.changed_files:
            return
        
        self.processing = True
        changed_files_snapshot = list(self.changed_files)
        self.changed_files.clear()
        
        try:
            logger.info(f"\n{'='*80}")
            logger.info(f"Processing {len(changed_files_snapshot)} changed files...")
            logger.info(f"{'='*80}\n")
            
            # 1. –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
            test_results = self.run_tests_with_coverage(changed_files_snapshot)
            
            # 2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ DeepSeek
            analysis_results = await self.parallel_analyze(
                test_results, 
                changed_files_snapshot
            )
            
            # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await self.save_results(
                test_results, 
                analysis_results, 
                changed_files_snapshot
            )
            
            # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self._print_statistics()
            
        except Exception as e:
            logger.error(f"Error processing changes: {e}", exc_info=True)
        finally:
            self.processing = False
    
    def run_tests_with_coverage(self, changed_files: List[Path]) -> Dict:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å –ø–æ–∫—Ä—ã—Ç–∏–µ–º –∫–æ–¥–∞"""
        logger.info("Running tests with coverage...")
        
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è coverage
            cov = coverage.Coverage(source=['backend'])
            cov.start()
            
            # –ó–∞–ø—É—Å–∫ pytest
            exit_code = pytest.main([
                '--tb=short',
                '--disable-warnings',
                '-v',
                str(project_root)
            ])
            
            cov.stop()
            cov.save()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è
            total_coverage = cov.report(show_missing=False)
            
            # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Ñ–∞–π–ª–∞–º
            coverage_by_file = {}
            for filename, data in cov.get_data().measured_files():
                coverage_by_file[filename] = {
                    "lines": len(data.lines),
                    "executed": len(data.arcs) if data.arcs else 0
                }
            
            logger.info(f"Tests completed with exit code: {exit_code}")
            logger.info(f"Total coverage: {total_coverage:.2f}%")
            
            return {
                "pytest_exit_code": exit_code,
                "coverage_total": total_coverage,
                "coverage_by_file": coverage_by_file,
                "timestamp": time.time(),
                "success": exit_code == 0
            }
            
        except Exception as e:
            logger.error(f"Error running tests: {e}")
            return {
                "pytest_exit_code": -1,
                "coverage_total": 0,
                "coverage_by_file": {},
                "timestamp": time.time(),
                "success": False,
                "error": str(e)
            }
    
    async def parallel_analyze(
        self, 
        test_results: Dict, 
        changed_files: List[Path]
    ) -> Dict:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ DeepSeek.
        
        –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:
        1. –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤
        2. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –∫–æ–¥–∞
        3. –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        """
        if not self.parallel_client:
            logger.warning("Parallel client not available, skipping analysis")
            return {"analysis_skipped": True, "reason": "No API keys configured"}
        
        logger.info("Starting parallel AI analysis...")
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        tasks = []
        
        # –ó–∞–¥–∞—á–∞ 1: –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤
        tasks.append(DeepSeekTask(
            task_id="test_quality",
            prompt=self._build_test_quality_prompt(test_results),
            priority=TaskPriority.HIGH,
            temperature=0.3,
            max_tokens=1500
        ))
        
        # –ó–∞–¥–∞—á–∞ 2: –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è
        tasks.append(DeepSeekTask(
            task_id="coverage_analysis",
            prompt=self._build_coverage_prompt(test_results),
            priority=TaskPriority.HIGH,
            temperature=0.3,
            max_tokens=1500
        ))
        
        # –ó–∞–¥–∞—á–∞ 3: –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–º–∞–∫—Å. 5 —Ñ–∞–π–ª–æ–≤)
        for i, file_path in enumerate(changed_files[:5]):
            tasks.append(DeepSeekTask(
                task_id=f"file_analysis_{i}",
                prompt=self._build_file_analysis_prompt(file_path, test_results),
                priority=TaskPriority.MEDIUM,
                temperature=0.3,
                max_tokens=1000,
                metadata={"file": str(file_path)}
            ))
        
        # –ó–∞–¥–∞—á–∞ 4: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        tasks.append(DeepSeekTask(
            task_id="recommendations",
            prompt=self._build_recommendations_prompt(test_results, changed_files),
            priority=TaskPriority.LOW,
            temperature=0.4,
            max_tokens=2000
        ))
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∑–∞–¥–∞—á
        results = await self.parallel_client.process_batch(tasks, show_progress=True)
        
        processing_time = time.time() - start_time
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis_output = {
            "processing_time": f"{processing_time:.2f}s",
            "tasks_completed": len([r for r in results if r.success]),
            "tasks_failed": len([r for r in results if not r.success]),
            "analyses": {}
        }
        
        for result in results:
            if result.success:
                analysis_output["analyses"][result.task_id] = {
                    "response": result.response,
                    "tokens": result.tokens_used,
                    "time": f"{result.processing_time:.2f}s",
                    "model": result.model
                }
            else:
                analysis_output["analyses"][result.task_id] = {
                    "error": result.error
                }
        
        logger.info(
            f"Parallel analysis completed: {len(tasks)} tasks in {processing_time:.2f}s "
            f"(Success: {analysis_output['tasks_completed']}, Failed: {analysis_output['tasks_failed']})"
        )
        
        return analysis_output
    
    def _build_test_quality_prompt(self, test_results: Dict) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤"""
        return f"""
Analyze the quality of this Python test execution:

TEST RESULTS:
- Exit Code: {test_results.get('pytest_exit_code')}
- Success: {test_results.get('success')}
- Total Coverage: {test_results.get('coverage_total', 0):.2f}%

Provide a brief assessment (3-4 sentences):
1. Overall test health
2. Coverage quality
3. Key concerns (if any)
"""
    
    def _build_coverage_prompt(self, test_results: Dict) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è"""
        coverage_data = test_results.get('coverage_by_file', {})
        return f"""
Analyze code coverage gaps:

COVERAGE DATA:
{json.dumps(coverage_data, indent=2)[:1000]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

Provide (3-4 sentences):
1. Files with low coverage
2. Priority areas for improvement
3. Coverage improvement strategy
"""
    
    def _build_file_analysis_prompt(self, file_path: Path, test_results: Dict) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        return f"""
Analyze changes in file: {file_path.name}

TEST CONTEXT:
- Overall Success: {test_results.get('success')}
- Coverage: {test_results.get('coverage_total', 0):.2f}%

Provide brief analysis (2-3 sentences):
1. Potential impact of this file's changes
2. Test coverage concerns
"""
    
    def _build_recommendations_prompt(self, test_results: Dict, changed_files: List[Path]) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        files_str = "\n".join([f"- {f.name}" for f in changed_files[:10]])
        return f"""
Provide actionable recommendations for this test run:

CONTEXT:
- {len(changed_files)} files changed
- Test Success: {test_results.get('success')}
- Coverage: {test_results.get('coverage_total', 0):.2f}%

FILES:
{files_str}

Provide 3-5 specific, actionable recommendations to improve:
1. Test coverage
2. Test reliability
3. Code quality
"""
    
    async def save_results(
        self, 
        test_results: Dict, 
        analysis_results: Dict, 
        changed_files: List[Path]
    ):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        timestamp = int(time.time())
        filename = self.results_dir / f"parallel_test_watcher_{timestamp}.json"
        
        results_data = {
            "timestamp": timestamp,
            "changed_files": [str(f) for f in changed_files],
            "test_results": test_results,
            "parallel_analysis": analysis_results,
            "performance": self.parallel_client.get_statistics() if self.parallel_client else {},
            "metadata": {
                "watch_path": str(self.watch_path),
                "debounce_seconds": self.debounce_seconds,
                "parallel_mode": True
            }
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Results saved to: {filename}")
            
        except Exception as e:
            logger.error(f"Error saving results: {e}")
    
    def _print_statistics(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.parallel_client:
            return
        
        stats = self.parallel_client.get_statistics()
        
        logger.info(f"\n{'='*80}")
        logger.info("PERFORMANCE STATISTICS")
        logger.info(f"{'='*80}")
        logger.info(f"Total Requests:      {stats['total_requests']}")
        logger.info(f"Success Rate:        {stats['success_rate']}")
        logger.info(f"Cache Hit Rate:      {stats['cache_hit_rate']}")
        logger.info(f"Total Tokens:        {stats['total_tokens']}")
        logger.info(f"Avg Tokens/Request:  {stats['avg_tokens_per_request']:.0f}")
        logger.info(f"Total Time:          {stats['total_processing_time']}")
        logger.info(f"Avg Time/Request:    {stats['avg_processing_time']}")
        logger.info(f"API Keys Used:       {stats['api_keys_count']}")
        logger.info(f"Max Concurrent:      {stats['max_concurrent']}")
        logger.info(f"{'='*80}\n")
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ file watcher"""
        self.loop = asyncio.get_running_loop()
        
        logger.info(f"\n{'='*80}")
        logger.info("  PARALLEL TEST WATCHER STARTED")
        logger.info(f"{'='*80}")
        logger.info(f"Watch Path:        {self.watch_path}")
        logger.info(f"Debounce Period:   {self.debounce_seconds}s")
        
        if self.parallel_client:
            stats = self.parallel_client.get_statistics()
            logger.info(f"API Keys:          {stats['api_keys_count']}")
            logger.info(f"Max Concurrent:    {stats['max_concurrent']}")
            logger.info(f"Cache Enabled:     Yes (TTL: 1h)")
        else:
            logger.warning("‚ö†Ô∏è  Parallel client not available")
        
        logger.info(f"{'='*80}\n")
        
        event_handler = self.ChangeHandler(self)
        self.observer.schedule(event_handler, str(self.watch_path), recursive=True)
        self.observer.start()
        
        try:
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("\nShutting down...")
            self.observer.stop()
        
        self.observer.join()


async def main():
    """Main entry point"""
    watcher = ParallelTestWatcher(watch_path=str(project_root))
    await watcher.start()


if __name__ == "__main__":
    asyncio.run(main())
